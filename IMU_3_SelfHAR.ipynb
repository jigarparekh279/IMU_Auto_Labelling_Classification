{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMU Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 1098209\n",
      "Train records: 898365\n",
      "Test records: 199844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2811883/2003044245.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[['X','Y','Z']] = (train_data[['X','Y','Z']] - mean_vals) / std_vals\n",
      "/tmp/ipykernel_2811883/2003044245.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data[['X','Y','Z']]  = (test_data[['X','Y','Z']]  - mean_vals) / std_vals\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labeled windows: 3891\n",
      "Train unlabeled windows: 597\n",
      "Test windows: 860\n",
      "Unique activity classes: ['Downstairs' 'Jogging' 'Sitting' 'Standing' 'Upstairs' 'Walking']\n",
      "Mapping: {'Downstairs': 0, 'Jogging': 1, 'Sitting': 2, 'Standing': 3, 'Upstairs': 4, 'Walking': 5}\n"
     ]
    }
   ],
   "source": [
    "# --- Parameters ---\n",
    "WINDOW_SIZE = 400\n",
    "STEP_SIZE = 200\n",
    "\n",
    "# --- Load All Data and Split Based on ID ---\n",
    "all_files = [\"data/IMU_case_dataset_part1_.csv\",\n",
    "             \"data/IMU_case_dataset_part2_.csv\",\n",
    "             \"data/IMU_case_dataset_part3_.csv\",\n",
    "             \"data/IMU_case_dataset_part4_.csv\"]\n",
    "\n",
    "dfs = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    # Keep the necessary columns: 'X', 'Y', 'Z', 'activity', and 'ID'\n",
    "    df = df[['X','Y','Z','activity','ID']]\n",
    "    dfs.append(df)\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Ensure that the 'ID' column is numeric\n",
    "data['ID'] = pd.to_numeric(data['ID'], errors='coerce')\n",
    "\n",
    "# Split the dataset:\n",
    "# Training data: IDs from 1 to 30\n",
    "# Testing data: IDs from 31 to 36\n",
    "train_data = data[data['ID'].between(1, 30)]\n",
    "test_data  = data[data['ID'].between(31, 36)]\n",
    "\n",
    "print(\"Total records:\", data.shape[0])\n",
    "print(\"Train records:\", train_data.shape[0])\n",
    "print(\"Test records:\", test_data.shape[0])\n",
    "\n",
    "# --- Normalization ---\n",
    "# Compute normalization parameters (mean and std) on the training set:\n",
    "mean_vals = train_data[['X','Y','Z']].mean()\n",
    "std_vals = train_data[['X','Y','Z']].std()\n",
    "\n",
    "# Apply normalization to both training and test data:\n",
    "train_data[['X','Y','Z']] = (train_data[['X','Y','Z']] - mean_vals) / std_vals\n",
    "test_data[['X','Y','Z']]  = (test_data[['X','Y','Z']]  - mean_vals) / std_vals\n",
    "\n",
    "# --- Windowing Function ---\n",
    "def generate_windows(df):\n",
    "    X_windows = []\n",
    "    y_labels = []\n",
    "    # Assuming the dataframe is sorted by time\n",
    "    for start in range(0, len(df) - WINDOW_SIZE + 1, STEP_SIZE):\n",
    "        window = df.iloc[start : start + WINDOW_SIZE]\n",
    "        labels = window[\"activity\"]\n",
    "        if labels.isna().all():\n",
    "            # Completely unlabeled window\n",
    "            X_windows.append(window[[\"X\", \"Y\", \"Z\"]].values)\n",
    "            y_labels.append(None)\n",
    "        else:\n",
    "            unique_labels = labels.dropna().unique()\n",
    "            if len(unique_labels) == 1:\n",
    "                X_windows.append(window[[\"X\", \"Y\", \"Z\"]].values)\n",
    "                y_labels.append(unique_labels[0])\n",
    "            else:\n",
    "                # Mixed or transitional window is treated as unlabeled\n",
    "                X_windows.append(window[[\"X\", \"Y\", \"Z\"]].values)\n",
    "                y_labels.append(None)\n",
    "    return X_windows, y_labels\n",
    "\n",
    "# --- Generate Windows from Training and Test Data ---\n",
    "# For training data:\n",
    "train_X_all, train_y_all = generate_windows(train_data)\n",
    "# Separate into labeled and unlabeled windows:\n",
    "train_labeled_X = [x for x, y in zip(train_X_all, train_y_all) if y is not None]\n",
    "train_labeled_y = [y for y in train_y_all if y is not None]\n",
    "train_unlabeled_X = [x for x, y in zip(train_X_all, train_y_all) if y is None]\n",
    "\n",
    "# Convert to numpy arrays:\n",
    "train_labeled_X = np.array(train_labeled_X)   # shape: (N_labeled, 400, 3)\n",
    "train_labeled_y = np.array(train_labeled_y)     # shape: (N_labeled,)\n",
    "train_unlabeled_X = np.array(train_unlabeled_X) # shape: (N_unlabeled, 400, 3)\n",
    "\n",
    "# Remove windows with any NaN features (if any)\n",
    "not_nan_idx = np.isnan(train_labeled_X).sum(axis=-1).sum(axis=-1) == 0\n",
    "train_labeled_X = train_labeled_X[not_nan_idx]\n",
    "train_labeled_y = train_labeled_y[not_nan_idx]\n",
    "\n",
    "not_nan_idx = np.isnan(train_unlabeled_X).sum(axis=-1).sum(axis=-1) == 0\n",
    "train_unlabeled_X = train_unlabeled_X[not_nan_idx]\n",
    "\n",
    "# For test data (only use windows with defined labels)\n",
    "test_X, test_y = generate_windows(test_data)\n",
    "test_X = np.array([x for x, y in zip(test_X, test_y) if y is not None])\n",
    "test_y = np.array([y for y in test_y if y is not None])\n",
    "\n",
    "print(\"Train labeled windows:\", len(train_labeled_y))\n",
    "print(\"Train unlabeled windows:\", train_unlabeled_X.shape[0])\n",
    "print(\"Test windows:\", len(test_y))\n",
    "\n",
    "# --- Label Encoding ---\n",
    "# Convert string labels to integer indices\n",
    "unique_classes = np.unique(train_labeled_y)\n",
    "print(\"Unique activity classes:\", unique_classes)\n",
    "class_to_idx = {label: idx for idx, label in enumerate(unique_classes)}\n",
    "print(\"Mapping:\", class_to_idx)\n",
    "\n",
    "train_labeled_y_idx = np.array([class_to_idx[label] for label in train_labeled_y])\n",
    "test_y_idx = np.array([class_to_idx[label] for label in test_y])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architecture: Teacher-Student TPN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPNModel(\n",
      "  (conv1): Conv1d(3, 32, kernel_size=(24,), stride=(1,))\n",
      "  (conv2): Conv1d(32, 64, kernel_size=(16,), stride=(1,))\n",
      "  (conv3): Conv1d(64, 96, kernel_size=(8,), stride=(1,))\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc_har1): Linear(in_features=96, out_features=1024, bias=True)\n",
      "  (fc_har2): Linear(in_features=1024, out_features=6, bias=True)\n",
      ")\n",
      "TPNModel(\n",
      "  (conv1): Conv1d(3, 32, kernel_size=(24,), stride=(1,))\n",
      "  (conv2): Conv1d(32, 64, kernel_size=(16,), stride=(1,))\n",
      "  (conv3): Conv1d(64, 96, kernel_size=(8,), stride=(1,))\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc_har1): Linear(in_features=96, out_features=1024, bias=True)\n",
      "  (fc_har2): Linear(in_features=1024, out_features=6, bias=True)\n",
      "  (transform_heads): ModuleList(\n",
      "    (0-7): 8 x Sequential(\n",
      "      (0): Linear(in_features=96, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TPNModel(nn.Module):\n",
    "    def __init__(self, num_classes, num_transforms=0):\n",
    "        super(TPNModel, self).__init__()\n",
    "        # Convolutional core: three conv layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=32, kernel_size=24, stride=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=16, stride=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=96, kernel_size=8, stride=1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        if num_classes > 0:\n",
    "            self.fc_har1 = nn.Linear(96, 1024)\n",
    "            self.fc_har2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "        self.num_transforms = num_transforms\n",
    "        if num_transforms > 0:\n",
    "            self.transform_heads = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(96, 256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, 1)\n",
    "                )\n",
    "                for _ in range(num_transforms)\n",
    "            ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, 400, 3) -> transpose to (batch, 3, 400)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        # Global max pooling over time dimension\n",
    "        x = F.max_pool1d(x, kernel_size=x.shape[2])\n",
    "        features = x.squeeze(2)  # shape: (batch, 96)\n",
    "\n",
    "        # Activity classification head\n",
    "        class_logits = None\n",
    "        if self.num_classes > 0:\n",
    "            h = self.relu(self.fc_har1(features))\n",
    "            class_logits = self.fc_har2(h)  # raw scores for each class\n",
    "\n",
    "        # Transformation discrimination heads\n",
    "        transform_logits = None\n",
    "        if self.num_transforms > 0:\n",
    "            logits_list = []\n",
    "            for head in self.transform_heads:\n",
    "                logit = head(features)   # shape: (batch, 1)\n",
    "                logits_list.append(logit.squeeze(-1))  # shape: (batch,)\n",
    "            transform_logits = torch.stack(logits_list, dim=1)  # shape: (batch, num_transforms)\n",
    "        return class_logits, transform_logits\n",
    "\n",
    "# Define the number of activity classes (from unique_classes) and transformations (8)\n",
    "num_classes = len(unique_classes)\n",
    "num_transforms = 8\n",
    "\n",
    "# Teacher: only classification head\n",
    "teacher_model = TPNModel(num_classes=num_classes, num_transforms=0)\n",
    "# Student: both classification and transformation heads\n",
    "student_model = TPNModel(num_classes=num_classes, num_transforms=num_transforms)\n",
    "\n",
    "print(teacher_model)\n",
    "print(student_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Teacher Model Training (Supervised Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1: Train Loss=1.4038, BalAcc=0.1910, F1_wtd=0.3176 | Val Loss=1.3022, BalAcc=0.1973, F1_wtd=0.3797\n",
      "Epoch 2: Train Loss=1.1682, BalAcc=0.2644, F1_wtd=0.5063 | Val Loss=1.2456, BalAcc=0.2633, F1_wtd=0.5402\n",
      "Epoch 3: Train Loss=0.9608, BalAcc=0.2986, F1_wtd=0.5944 | Val Loss=0.9632, BalAcc=0.3094, F1_wtd=0.6239\n",
      "Epoch 4: Train Loss=0.7476, BalAcc=0.3235, F1_wtd=0.6426 | Val Loss=0.7943, BalAcc=0.3707, F1_wtd=0.6846\n",
      "Epoch 5: Train Loss=0.6365, BalAcc=0.3392, F1_wtd=0.6610 | Val Loss=0.7133, BalAcc=0.4691, F1_wtd=0.7185\n",
      "Epoch 6: Train Loss=0.5708, BalAcc=0.3535, F1_wtd=0.6709 | Val Loss=0.6552, BalAcc=0.5938, F1_wtd=0.7685\n",
      "Epoch 7: Train Loss=0.5083, BalAcc=0.5406, F1_wtd=0.7550 | Val Loss=0.5700, BalAcc=0.6533, F1_wtd=0.7883\n",
      "Epoch 8: Train Loss=0.4457, BalAcc=0.6597, F1_wtd=0.8016 | Val Loss=0.5059, BalAcc=0.6570, F1_wtd=0.7886\n",
      "Epoch 9: Train Loss=0.4070, BalAcc=0.6664, F1_wtd=0.8035 | Val Loss=0.4683, BalAcc=0.7409, F1_wtd=0.8363\n",
      "Epoch 10: Train Loss=0.3697, BalAcc=0.6928, F1_wtd=0.8264 | Val Loss=0.4171, BalAcc=0.6996, F1_wtd=0.8158\n",
      "Epoch 11: Train Loss=0.3378, BalAcc=0.7211, F1_wtd=0.8463 | Val Loss=0.3575, BalAcc=0.7445, F1_wtd=0.8595\n",
      "Epoch 12: Train Loss=0.3126, BalAcc=0.7297, F1_wtd=0.8536 | Val Loss=0.3672, BalAcc=0.7836, F1_wtd=0.8598\n",
      "Epoch 13: Train Loss=0.2884, BalAcc=0.7653, F1_wtd=0.8751 | Val Loss=0.3057, BalAcc=0.7834, F1_wtd=0.8747\n",
      "Epoch 14: Train Loss=0.2606, BalAcc=0.7836, F1_wtd=0.8885 | Val Loss=0.3179, BalAcc=0.8226, F1_wtd=0.9000\n",
      "Epoch 15: Train Loss=0.2457, BalAcc=0.7831, F1_wtd=0.8888 | Val Loss=0.2665, BalAcc=0.8155, F1_wtd=0.9011\n",
      "Epoch 16: Train Loss=0.2342, BalAcc=0.8061, F1_wtd=0.9024 | Val Loss=0.2892, BalAcc=0.8475, F1_wtd=0.9150\n",
      "Epoch 17: Train Loss=0.2330, BalAcc=0.8070, F1_wtd=0.9023 | Val Loss=0.2415, BalAcc=0.8374, F1_wtd=0.9186\n",
      "Epoch 18: Train Loss=0.2214, BalAcc=0.8224, F1_wtd=0.9117 | Val Loss=0.2667, BalAcc=0.8722, F1_wtd=0.9315\n",
      "Epoch 19: Train Loss=0.2213, BalAcc=0.8292, F1_wtd=0.9138 | Val Loss=0.2327, BalAcc=0.8203, F1_wtd=0.9088\n",
      "Epoch 20: Train Loss=0.2112, BalAcc=0.8370, F1_wtd=0.9192 | Val Loss=0.2458, BalAcc=0.8792, F1_wtd=0.9375\n",
      "Epoch 21: Train Loss=0.1900, BalAcc=0.8534, F1_wtd=0.9270 | Val Loss=0.2251, BalAcc=0.8553, F1_wtd=0.9279\n",
      "Epoch 22: Train Loss=0.1901, BalAcc=0.8627, F1_wtd=0.9342 | Val Loss=0.2321, BalAcc=0.8811, F1_wtd=0.9379\n",
      "Epoch 23: Train Loss=0.1786, BalAcc=0.8635, F1_wtd=0.9336 | Val Loss=0.2057, BalAcc=0.9218, F1_wtd=0.9533\n",
      "Epoch 24: Train Loss=0.1712, BalAcc=0.8698, F1_wtd=0.9358 | Val Loss=0.2238, BalAcc=0.9198, F1_wtd=0.9489\n",
      "Epoch 25: Train Loss=0.1691, BalAcc=0.8691, F1_wtd=0.9360 | Val Loss=0.2140, BalAcc=0.9208, F1_wtd=0.9510\n",
      "Epoch 26: Train Loss=0.1577, BalAcc=0.8829, F1_wtd=0.9432 | Val Loss=0.2293, BalAcc=0.8841, F1_wtd=0.9326\n",
      "Epoch 27: Train Loss=0.1497, BalAcc=0.8835, F1_wtd=0.9431 | Val Loss=0.2103, BalAcc=0.9129, F1_wtd=0.9509\n",
      "Epoch 28: Train Loss=0.1514, BalAcc=0.8955, F1_wtd=0.9482 | Val Loss=0.2026, BalAcc=0.9227, F1_wtd=0.9513\n",
      "Epoch 29: Train Loss=0.1446, BalAcc=0.8997, F1_wtd=0.9493 | Val Loss=0.2069, BalAcc=0.9179, F1_wtd=0.9493\n",
      "Epoch 30: Train Loss=0.1492, BalAcc=0.9020, F1_wtd=0.9499 | Val Loss=0.2095, BalAcc=0.9258, F1_wtd=0.9495\n",
      "Epoch 31: Train Loss=0.1469, BalAcc=0.9066, F1_wtd=0.9508 | Val Loss=0.1885, BalAcc=0.9079, F1_wtd=0.9480\n",
      "Epoch 32: Train Loss=0.1308, BalAcc=0.9105, F1_wtd=0.9550 | Val Loss=0.1728, BalAcc=0.9268, F1_wtd=0.9563\n",
      "Epoch 33: Train Loss=0.1309, BalAcc=0.9121, F1_wtd=0.9565 | Val Loss=0.2145, BalAcc=0.9109, F1_wtd=0.9422\n",
      "Epoch 34: Train Loss=0.1231, BalAcc=0.9097, F1_wtd=0.9545 | Val Loss=0.1686, BalAcc=0.9177, F1_wtd=0.9531\n",
      "Epoch 35: Train Loss=0.1215, BalAcc=0.9161, F1_wtd=0.9575 | Val Loss=0.1975, BalAcc=0.9145, F1_wtd=0.9466\n",
      "Epoch 36: Train Loss=0.1121, BalAcc=0.9215, F1_wtd=0.9614 | Val Loss=0.1938, BalAcc=0.9092, F1_wtd=0.9492\n",
      "Epoch 37: Train Loss=0.1052, BalAcc=0.9212, F1_wtd=0.9608 | Val Loss=0.2020, BalAcc=0.9156, F1_wtd=0.9452\n",
      "Epoch 38: Train Loss=0.1043, BalAcc=0.9345, F1_wtd=0.9650 | Val Loss=0.1610, BalAcc=0.9218, F1_wtd=0.9538\n",
      "Epoch 39: Train Loss=0.0991, BalAcc=0.9379, F1_wtd=0.9667 | Val Loss=0.1709, BalAcc=0.9312, F1_wtd=0.9546\n",
      "Epoch 40: Train Loss=0.0946, BalAcc=0.9395, F1_wtd=0.9677 | Val Loss=0.1556, BalAcc=0.9206, F1_wtd=0.9563\n",
      "Epoch 41: Train Loss=0.0864, BalAcc=0.9457, F1_wtd=0.9725 | Val Loss=0.1987, BalAcc=0.9168, F1_wtd=0.9431\n",
      "Epoch 42: Train Loss=0.0966, BalAcc=0.9432, F1_wtd=0.9694 | Val Loss=0.1669, BalAcc=0.9297, F1_wtd=0.9594\n",
      "Epoch 43: Train Loss=0.0955, BalAcc=0.9380, F1_wtd=0.9683 | Val Loss=0.1797, BalAcc=0.9381, F1_wtd=0.9577\n",
      "Epoch 44: Train Loss=0.0948, BalAcc=0.9324, F1_wtd=0.9654 | Val Loss=0.2884, BalAcc=0.9164, F1_wtd=0.9175\n",
      "Epoch 45: Train Loss=0.0922, BalAcc=0.9511, F1_wtd=0.9732 | Val Loss=0.1795, BalAcc=0.9263, F1_wtd=0.9569\n",
      "Epoch 46: Train Loss=0.0825, BalAcc=0.9473, F1_wtd=0.9727 | Val Loss=0.1925, BalAcc=0.9163, F1_wtd=0.9339\n",
      "Epoch 47: Train Loss=0.0800, BalAcc=0.9461, F1_wtd=0.9718 | Val Loss=0.2247, BalAcc=0.9361, F1_wtd=0.9485\n",
      "Epoch 48: Train Loss=0.0754, BalAcc=0.9486, F1_wtd=0.9727 | Val Loss=0.2138, BalAcc=0.9308, F1_wtd=0.9490\n",
      "Epoch 49: Train Loss=0.0845, BalAcc=0.9472, F1_wtd=0.9736 | Val Loss=0.1605, BalAcc=0.9323, F1_wtd=0.9483\n",
      "Epoch 50: Train Loss=0.0719, BalAcc=0.9548, F1_wtd=0.9746 | Val Loss=0.1729, BalAcc=0.9309, F1_wtd=0.9433\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "\n",
    "# --- Device setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Train/Val split ---\n",
    "X_train_tensor = torch.from_numpy(train_labeled_X).float()\n",
    "y_train_tensor = torch.from_numpy(train_labeled_y_idx).long()\n",
    "\n",
    "X_train_t, X_val_t, y_train_t, y_val_t = train_test_split(\n",
    "    X_train_tensor, y_train_tensor, test_size=0.1, random_state=42, stratify=y_train_tensor\n",
    ")\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_t, y_train_t)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val_t, y_val_t)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# --- Model, loss, optimizer ---\n",
    "teacher_model = teacher_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(teacher_model.parameters(), lr=0.0003, weight_decay=1e-4)\n",
    "\n",
    "# --- Training loop ---\n",
    "num_epochs = 50\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    teacher_model.train()\n",
    "    total_train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = teacher_model(batch_X)\n",
    "        loss = criterion(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item() * batch_X.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    train_bal_acc = balanced_accuracy_score(train_labels, train_preds)\n",
    "    train_f1_wtd = f1_score(train_labels, train_preds, average='weighted')\n",
    "    avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "\n",
    "    # --- Validation ---\n",
    "    teacher_model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_X, val_y in val_loader:\n",
    "            val_X, val_y = val_X.to(device), val_y.to(device)\n",
    "            logits, _ = teacher_model(val_X)\n",
    "            loss = criterion(logits, val_y)\n",
    "\n",
    "            total_val_loss += loss.item() * val_X.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(val_y.cpu().numpy())\n",
    "\n",
    "    val_bal_acc = balanced_accuracy_score(val_labels, val_preds)\n",
    "    val_f1_wtd = f1_score(val_labels, val_preds, average='weighted')\n",
    "    avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch}: \"\n",
    "          f\"Train Loss={avg_train_loss:.4f}, BalAcc={train_bal_acc:.4f}, F1_wtd={train_f1_wtd:.4f} | \"\n",
    "          f\"Val Loss={avg_val_loss:.4f}, BalAcc={val_bal_acc:.4f}, F1_wtd={val_f1_wtd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher balanced model accuracy on test set: 0.9093\n",
      "Teacher model wtd f1 score on test set: 0.9371\n"
     ]
    }
   ],
   "source": [
    "teacher_model.eval()\n",
    "test_X_tensor = torch.from_numpy(test_X).float().to(device)\n",
    "test_y_tensor = torch.from_numpy(test_y_idx).long().to(device)\n",
    "with torch.no_grad():\n",
    "    logits, _ = teacher_model(test_X_tensor)\n",
    "    predictions = logits.argmax(dim=1)\n",
    "# test_accuracy = (predictions == test_y_tensor).sum().item() / len(test_y_tensor)\n",
    "test_accuracy = balanced_accuracy_score(test_y_tensor.to('cpu').numpy(), predictions.to('cpu').numpy())\n",
    "test_wtdf1 = f1_score(test_y_tensor.to('cpu').numpy(), predictions.to('cpu').numpy(), average='weighted')\n",
    "print(f\"Teacher balanced model accuracy on test set: {test_accuracy:.4f}\")\n",
    "print(f\"Teacher model wtd f1 score on test set: {test_wtdf1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Self-Labeling and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Pseudo-Labeling Unlabeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 571 out of 597 unlabeled samples based on confidence.\n",
      "Pseudo-labeled dataset size: 343\n"
     ]
    }
   ],
   "source": [
    "teacher_model.eval()\n",
    "X_unlabeled = torch.from_numpy(train_unlabeled_X).float().to(device)\n",
    "with torch.no_grad():\n",
    "    logits, _ = teacher_model(X_unlabeled)\n",
    "    probs = F.softmax(logits, dim=1).to('cpu').numpy()\n",
    "\n",
    "pred_classes = probs.argmax(axis=1)\n",
    "pred_confidences = probs.max(axis=1)\n",
    "\n",
    "# Confidence threshold\n",
    "confidence_threshold = 0.5\n",
    "selected_idx = np.where(pred_confidences >= confidence_threshold)[0]\n",
    "print(f\"Selected {len(selected_idx)} out of {train_unlabeled_X.shape[0]} unlabeled samples based on confidence.\")\n",
    "\n",
    "# Optionally balance classes (limit to top K per class)\n",
    "K = 100  # maximum samples per class\n",
    "selected_idx_balanced = []\n",
    "for c in range(num_classes):\n",
    "    class_idxs = selected_idx[pred_classes[selected_idx] == c]\n",
    "    if len(class_idxs) > K:\n",
    "        # sort by confidence and take top K\n",
    "        sorted_idxs = class_idxs[np.argsort(pred_confidences[class_idxs])][-K:]\n",
    "        selected_idx_balanced.extend(sorted_idxs)\n",
    "    else:\n",
    "        selected_idx_balanced.extend(class_idxs)\n",
    "selected_idx_balanced = np.array(selected_idx_balanced)\n",
    "\n",
    "pseudo_labeled_X = train_unlabeled_X[selected_idx_balanced]\n",
    "pseudo_labeled_y = pred_classes[selected_idx_balanced]\n",
    "\n",
    "print(\"Pseudo-labeled dataset size:\", pseudo_labeled_X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data Augmentation with Signal Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented dataset size: 2744\n"
     ]
    }
   ],
   "source": [
    "import math, random\n",
    "\n",
    "def add_noise(signal, sigma=0.1):\n",
    "    return signal + np.random.normal(scale=sigma, size=signal.shape)\n",
    "\n",
    "def scale_signal(signal, sigma=0.1):\n",
    "    factor = np.random.normal(loc=1.0, scale=sigma)\n",
    "    return signal * factor\n",
    "\n",
    "def rotate_signal(signal):\n",
    "    axis = np.random.normal(size=3)\n",
    "    axis = axis / np.linalg.norm(axis)\n",
    "    theta = np.random.uniform(0, 2*math.pi)\n",
    "    K = np.array([[0, -axis[2], axis[1]],\n",
    "                  [axis[2], 0, -axis[0]],\n",
    "                  [-axis[1], axis[0], 0]])\n",
    "    I = np.eye(3)\n",
    "    R = I + math.sin(theta)*K + (1 - math.cos(theta))*(K.dot(K))\n",
    "    return signal.dot(R.T)\n",
    "\n",
    "def invert_signal(signal):\n",
    "    return -signal\n",
    "\n",
    "def reverse_time(signal):\n",
    "    return signal[::-1, :]\n",
    "\n",
    "def scramble_segments(signal, num_segments=4):\n",
    "    seg_len = signal.shape[0] // num_segments\n",
    "    segments = [signal[i*seg_len : (i+1)*seg_len] for i in range(num_segments)]\n",
    "    random.shuffle(segments)\n",
    "    return np.concatenate(segments, axis=0)\n",
    "\n",
    "def shuffle_channels(signal):\n",
    "    perm = np.random.permutation(3)\n",
    "    return signal[:, perm]\n",
    "\n",
    "# List of transformation functions (we assume identity is one of the tasks)\n",
    "transform_funcs = [\n",
    "    add_noise, scale_signal, rotate_signal, invert_signal,\n",
    "    reverse_time, scramble_segments, shuffle_channels, lambda x: x\n",
    "    ]\n",
    "\n",
    "# Augment data: for each sample, apply all 8 transformations\n",
    "augmented_X = []\n",
    "augmented_y_activity = []   # same pseudo label for all augmentations\n",
    "augmented_y_transform = []  # index (0 to 7) indicating the transform\n",
    "for x, label in zip(pseudo_labeled_X, pseudo_labeled_y):\n",
    "    for t_idx, func in enumerate(transform_funcs):\n",
    "        augmented_X.append(func(x))\n",
    "        augmented_y_activity.append(label)\n",
    "        augmented_y_transform.append(t_idx)\n",
    "\n",
    "augmented_X = np.array(augmented_X)  # shape: (N_aug, 400, 3)\n",
    "augmented_y_activity = np.array(augmented_y_activity)\n",
    "augmented_y_transform = np.array(augmented_y_transform)\n",
    "\n",
    "print(\"Augmented dataset size:\", augmented_X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Student Model Training (Multi-task Pre-training + Fine-Tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Pre-Training with Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student pre-training Epoch 1: avg loss = 2.4475\n",
      "Student pre-training Epoch 2: avg loss = 2.3647\n",
      "Student pre-training Epoch 3: avg loss = 2.2844\n",
      "Student pre-training Epoch 4: avg loss = 2.2049\n",
      "Student pre-training Epoch 5: avg loss = 2.1359\n",
      "Student pre-training Epoch 6: avg loss = 2.0732\n",
      "Student pre-training Epoch 7: avg loss = 2.0250\n",
      "Student pre-training Epoch 8: avg loss = 1.9843\n",
      "Student pre-training Epoch 9: avg loss = 1.9608\n",
      "Student pre-training Epoch 10: avg loss = 1.9408\n",
      "Student pre-training Epoch 11: avg loss = 1.9313\n",
      "Student pre-training Epoch 12: avg loss = 1.9243\n",
      "Student pre-training Epoch 13: avg loss = 1.9197\n",
      "Student pre-training Epoch 14: avg loss = 1.9148\n",
      "Student pre-training Epoch 15: avg loss = 1.9104\n",
      "Student pre-training Epoch 16: avg loss = 1.9067\n",
      "Student pre-training Epoch 17: avg loss = 1.9027\n",
      "Student pre-training Epoch 18: avg loss = 1.8951\n",
      "Student pre-training Epoch 19: avg loss = 1.8907\n",
      "Student pre-training Epoch 20: avg loss = 1.8850\n",
      "Student pre-training Epoch 21: avg loss = 1.8769\n",
      "Student pre-training Epoch 22: avg loss = 1.8660\n",
      "Student pre-training Epoch 23: avg loss = 1.8577\n",
      "Student pre-training Epoch 24: avg loss = 1.8399\n",
      "Student pre-training Epoch 25: avg loss = 1.8278\n",
      "Student pre-training Epoch 26: avg loss = 1.8085\n",
      "Student pre-training Epoch 27: avg loss = 1.7886\n",
      "Student pre-training Epoch 28: avg loss = 1.7677\n",
      "Student pre-training Epoch 29: avg loss = 1.7493\n",
      "Student pre-training Epoch 30: avg loss = 1.7293\n",
      "Student pre-training Epoch 31: avg loss = 1.7119\n",
      "Student pre-training Epoch 32: avg loss = 1.6968\n",
      "Student pre-training Epoch 33: avg loss = 1.6851\n",
      "Student pre-training Epoch 34: avg loss = 1.6683\n",
      "Student pre-training Epoch 35: avg loss = 1.6513\n",
      "Student pre-training Epoch 36: avg loss = 1.6390\n",
      "Student pre-training Epoch 37: avg loss = 1.6226\n",
      "Student pre-training Epoch 38: avg loss = 1.6049\n",
      "Student pre-training Epoch 39: avg loss = 1.5903\n",
      "Student pre-training Epoch 40: avg loss = 1.5694\n",
      "Student pre-training Epoch 41: avg loss = 1.5487\n",
      "Student pre-training Epoch 42: avg loss = 1.5337\n",
      "Student pre-training Epoch 43: avg loss = 1.5222\n",
      "Student pre-training Epoch 44: avg loss = 1.5022\n",
      "Student pre-training Epoch 45: avg loss = 1.4897\n",
      "Student pre-training Epoch 46: avg loss = 1.4752\n",
      "Student pre-training Epoch 47: avg loss = 1.4579\n",
      "Student pre-training Epoch 48: avg loss = 1.4470\n",
      "Student pre-training Epoch 49: avg loss = 1.4347\n",
      "Student pre-training Epoch 50: avg loss = 1.4251\n",
      "Student pre-training Epoch 51: avg loss = 1.4160\n",
      "Student pre-training Epoch 52: avg loss = 1.3990\n",
      "Student pre-training Epoch 53: avg loss = 1.3985\n",
      "Student pre-training Epoch 54: avg loss = 1.3908\n",
      "Student pre-training Epoch 55: avg loss = 1.3756\n",
      "Student pre-training Epoch 56: avg loss = 1.3687\n",
      "Student pre-training Epoch 57: avg loss = 1.3582\n",
      "Student pre-training Epoch 58: avg loss = 1.3522\n",
      "Student pre-training Epoch 59: avg loss = 1.3506\n",
      "Student pre-training Epoch 60: avg loss = 1.3402\n",
      "Student pre-training Epoch 61: avg loss = 1.3312\n",
      "Student pre-training Epoch 62: avg loss = 1.3278\n",
      "Student pre-training Epoch 63: avg loss = 1.3220\n",
      "Student pre-training Epoch 64: avg loss = 1.3191\n",
      "Student pre-training Epoch 65: avg loss = 1.3173\n",
      "Student pre-training Epoch 66: avg loss = 1.3090\n",
      "Student pre-training Epoch 67: avg loss = 1.3036\n",
      "Student pre-training Epoch 68: avg loss = 1.3024\n",
      "Student pre-training Epoch 69: avg loss = 1.2932\n",
      "Student pre-training Epoch 70: avg loss = 1.2871\n",
      "Student pre-training Epoch 71: avg loss = 1.2834\n",
      "Student pre-training Epoch 72: avg loss = 1.2794\n",
      "Student pre-training Epoch 73: avg loss = 1.2707\n",
      "Student pre-training Epoch 74: avg loss = 1.2628\n",
      "Student pre-training Epoch 75: avg loss = 1.2610\n",
      "Student pre-training Epoch 76: avg loss = 1.2601\n",
      "Student pre-training Epoch 77: avg loss = 1.2522\n",
      "Student pre-training Epoch 78: avg loss = 1.2484\n",
      "Student pre-training Epoch 79: avg loss = 1.2497\n",
      "Student pre-training Epoch 80: avg loss = 1.2370\n",
      "Student pre-training Epoch 81: avg loss = 1.2362\n",
      "Student pre-training Epoch 82: avg loss = 1.2337\n",
      "Student pre-training Epoch 83: avg loss = 1.2251\n",
      "Student pre-training Epoch 84: avg loss = 1.2240\n",
      "Student pre-training Epoch 85: avg loss = 1.2193\n",
      "Student pre-training Epoch 86: avg loss = 1.2200\n",
      "Student pre-training Epoch 87: avg loss = 1.2204\n",
      "Student pre-training Epoch 88: avg loss = 1.2053\n",
      "Student pre-training Epoch 89: avg loss = 1.2028\n",
      "Student pre-training Epoch 90: avg loss = 1.1943\n",
      "Student pre-training Epoch 91: avg loss = 1.1943\n",
      "Student pre-training Epoch 92: avg loss = 1.1859\n",
      "Student pre-training Epoch 93: avg loss = 1.1752\n",
      "Student pre-training Epoch 94: avg loss = 1.1783\n",
      "Student pre-training Epoch 95: avg loss = 1.1735\n",
      "Student pre-training Epoch 96: avg loss = 1.1642\n",
      "Student pre-training Epoch 97: avg loss = 1.1494\n",
      "Student pre-training Epoch 98: avg loss = 1.1547\n",
      "Student pre-training Epoch 99: avg loss = 1.1558\n",
      "Student pre-training Epoch 100: avg loss = 1.1468\n",
      "Student pre-training Epoch 101: avg loss = 1.1407\n",
      "Student pre-training Epoch 102: avg loss = 1.1392\n",
      "Student pre-training Epoch 103: avg loss = 1.1339\n",
      "Student pre-training Epoch 104: avg loss = 1.1406\n",
      "Student pre-training Epoch 105: avg loss = 1.1339\n",
      "Student pre-training Epoch 106: avg loss = 1.1079\n",
      "Student pre-training Epoch 107: avg loss = 1.1225\n",
      "Student pre-training Epoch 108: avg loss = 1.1118\n",
      "Student pre-training Epoch 109: avg loss = 1.1022\n",
      "Student pre-training Epoch 110: avg loss = 1.0992\n",
      "Student pre-training Epoch 111: avg loss = 1.1005\n",
      "Student pre-training Epoch 112: avg loss = 1.0948\n",
      "Student pre-training Epoch 113: avg loss = 1.0989\n",
      "Student pre-training Epoch 114: avg loss = 1.0882\n",
      "Student pre-training Epoch 115: avg loss = 1.0774\n",
      "Student pre-training Epoch 116: avg loss = 1.0898\n",
      "Student pre-training Epoch 117: avg loss = 1.0726\n",
      "Student pre-training Epoch 118: avg loss = 1.0791\n",
      "Student pre-training Epoch 119: avg loss = 1.0601\n",
      "Student pre-training Epoch 120: avg loss = 1.0648\n",
      "Student pre-training Epoch 121: avg loss = 1.0685\n",
      "Student pre-training Epoch 122: avg loss = 1.0635\n",
      "Student pre-training Epoch 123: avg loss = 1.0607\n",
      "Student pre-training Epoch 124: avg loss = 1.0449\n",
      "Student pre-training Epoch 125: avg loss = 1.0601\n",
      "Student pre-training Epoch 126: avg loss = 1.0558\n",
      "Student pre-training Epoch 127: avg loss = 1.0445\n",
      "Student pre-training Epoch 128: avg loss = 1.0512\n",
      "Student pre-training Epoch 129: avg loss = 1.0483\n",
      "Student pre-training Epoch 130: avg loss = 1.0395\n",
      "Student pre-training Epoch 131: avg loss = 1.0430\n",
      "Student pre-training Epoch 132: avg loss = 1.0239\n",
      "Student pre-training Epoch 133: avg loss = 1.0311\n",
      "Student pre-training Epoch 134: avg loss = 1.0333\n",
      "Student pre-training Epoch 135: avg loss = 1.0270\n",
      "Student pre-training Epoch 136: avg loss = 1.0388\n",
      "Student pre-training Epoch 137: avg loss = 1.0174\n",
      "Student pre-training Epoch 138: avg loss = 1.0180\n",
      "Student pre-training Epoch 139: avg loss = 1.0137\n",
      "Student pre-training Epoch 140: avg loss = 1.0088\n",
      "Student pre-training Epoch 141: avg loss = 1.0101\n",
      "Student pre-training Epoch 142: avg loss = 1.0065\n",
      "Student pre-training Epoch 143: avg loss = 1.0055\n",
      "Student pre-training Epoch 144: avg loss = 0.9991\n",
      "Student pre-training Epoch 145: avg loss = 0.9983\n",
      "Student pre-training Epoch 146: avg loss = 0.9910\n",
      "Student pre-training Epoch 147: avg loss = 1.0118\n",
      "Student pre-training Epoch 148: avg loss = 1.0006\n",
      "Student pre-training Epoch 149: avg loss = 0.9906\n",
      "Student pre-training Epoch 150: avg loss = 1.0109\n",
      "Student pre-training Epoch 151: avg loss = 0.9931\n",
      "Student pre-training Epoch 152: avg loss = 0.9926\n",
      "Student pre-training Epoch 153: avg loss = 0.9855\n",
      "Student pre-training Epoch 154: avg loss = 0.9836\n",
      "Student pre-training Epoch 155: avg loss = 0.9877\n",
      "Student pre-training Epoch 156: avg loss = 0.9881\n",
      "Student pre-training Epoch 157: avg loss = 0.9872\n",
      "Student pre-training Epoch 158: avg loss = 0.9888\n",
      "Student pre-training Epoch 159: avg loss = 0.9630\n",
      "Student pre-training Epoch 160: avg loss = 0.9792\n",
      "Student pre-training Epoch 161: avg loss = 0.9681\n",
      "Student pre-training Epoch 162: avg loss = 0.9719\n",
      "Student pre-training Epoch 163: avg loss = 0.9650\n",
      "Student pre-training Epoch 164: avg loss = 0.9618\n",
      "Student pre-training Epoch 165: avg loss = 0.9710\n",
      "Student pre-training Epoch 166: avg loss = 0.9712\n",
      "Student pre-training Epoch 167: avg loss = 0.9571\n",
      "Student pre-training Epoch 168: avg loss = 0.9618\n",
      "Student pre-training Epoch 169: avg loss = 0.9585\n",
      "Student pre-training Epoch 170: avg loss = 0.9704\n",
      "Student pre-training Epoch 171: avg loss = 0.9442\n",
      "Student pre-training Epoch 172: avg loss = 0.9569\n",
      "Student pre-training Epoch 173: avg loss = 0.9602\n",
      "Student pre-training Epoch 174: avg loss = 0.9476\n",
      "Student pre-training Epoch 175: avg loss = 0.9459\n",
      "Student pre-training Epoch 176: avg loss = 0.9566\n",
      "Student pre-training Epoch 177: avg loss = 0.9474\n",
      "Student pre-training Epoch 178: avg loss = 0.9508\n",
      "Student pre-training Epoch 179: avg loss = 0.9553\n",
      "Student pre-training Epoch 180: avg loss = 0.9363\n",
      "Student pre-training Epoch 181: avg loss = 0.9403\n",
      "Student pre-training Epoch 182: avg loss = 0.9432\n",
      "Student pre-training Epoch 183: avg loss = 0.9438\n",
      "Student pre-training Epoch 184: avg loss = 0.9437\n",
      "Student pre-training Epoch 185: avg loss = 0.9361\n",
      "Student pre-training Epoch 186: avg loss = 0.9340\n",
      "Student pre-training Epoch 187: avg loss = 0.9274\n",
      "Student pre-training Epoch 188: avg loss = 0.9375\n",
      "Student pre-training Epoch 189: avg loss = 0.9338\n",
      "Student pre-training Epoch 190: avg loss = 0.9279\n",
      "Student pre-training Epoch 191: avg loss = 0.9348\n",
      "Student pre-training Epoch 192: avg loss = 0.9274\n",
      "Student pre-training Epoch 193: avg loss = 0.9137\n",
      "Student pre-training Epoch 194: avg loss = 0.9277\n",
      "Student pre-training Epoch 195: avg loss = 0.9311\n",
      "Student pre-training Epoch 196: avg loss = 0.9263\n",
      "Student pre-training Epoch 197: avg loss = 0.9168\n",
      "Student pre-training Epoch 198: avg loss = 0.9269\n",
      "Student pre-training Epoch 199: avg loss = 0.9170\n",
      "Student pre-training Epoch 200: avg loss = 0.9168\n",
      "Student pre-training Epoch 201: avg loss = 0.9074\n",
      "Student pre-training Epoch 202: avg loss = 0.9223\n",
      "Student pre-training Epoch 203: avg loss = 0.9131\n",
      "Student pre-training Epoch 204: avg loss = 0.9121\n",
      "Student pre-training Epoch 205: avg loss = 0.9143\n",
      "Student pre-training Epoch 206: avg loss = 0.9178\n",
      "Student pre-training Epoch 207: avg loss = 0.9161\n",
      "Student pre-training Epoch 208: avg loss = 0.9128\n",
      "Student pre-training Epoch 209: avg loss = 0.9176\n",
      "Student pre-training Epoch 210: avg loss = 0.9152\n",
      "Student pre-training Epoch 211: avg loss = 0.9140\n",
      "Student pre-training Epoch 212: avg loss = 0.9109\n",
      "Student pre-training Epoch 213: avg loss = 0.9022\n",
      "Student pre-training Epoch 214: avg loss = 0.9092\n",
      "Student pre-training Epoch 215: avg loss = 0.9070\n",
      "Student pre-training Epoch 216: avg loss = 0.9070\n",
      "Student pre-training Epoch 217: avg loss = 0.9054\n",
      "Student pre-training Epoch 218: avg loss = 0.8906\n",
      "Student pre-training Epoch 219: avg loss = 0.8938\n",
      "Student pre-training Epoch 220: avg loss = 0.8991\n",
      "Student pre-training Epoch 221: avg loss = 0.8967\n",
      "Student pre-training Epoch 222: avg loss = 0.8824\n",
      "Student pre-training Epoch 223: avg loss = 0.9011\n",
      "Student pre-training Epoch 224: avg loss = 0.8808\n",
      "Student pre-training Epoch 225: avg loss = 0.8913\n",
      "Student pre-training Epoch 226: avg loss = 0.8879\n",
      "Student pre-training Epoch 227: avg loss = 0.8835\n",
      "Student pre-training Epoch 228: avg loss = 0.8911\n",
      "Student pre-training Epoch 229: avg loss = 0.8840\n",
      "Student pre-training Epoch 230: avg loss = 0.8870\n",
      "Student pre-training Epoch 231: avg loss = 0.8803\n",
      "Student pre-training Epoch 232: avg loss = 0.8810\n",
      "Student pre-training Epoch 233: avg loss = 0.8833\n",
      "Student pre-training Epoch 234: avg loss = 0.8805\n",
      "Student pre-training Epoch 235: avg loss = 0.8774\n",
      "Student pre-training Epoch 236: avg loss = 0.8844\n",
      "Student pre-training Epoch 237: avg loss = 0.8773\n",
      "Student pre-training Epoch 238: avg loss = 0.8839\n",
      "Student pre-training Epoch 239: avg loss = 0.8642\n",
      "Student pre-training Epoch 240: avg loss = 0.8639\n",
      "Student pre-training Epoch 241: avg loss = 0.8795\n",
      "Student pre-training Epoch 242: avg loss = 0.8802\n",
      "Student pre-training Epoch 243: avg loss = 0.8675\n",
      "Student pre-training Epoch 244: avg loss = 0.8702\n",
      "Student pre-training Epoch 245: avg loss = 0.8636\n",
      "Student pre-training Epoch 246: avg loss = 0.8679\n",
      "Student pre-training Epoch 247: avg loss = 0.8649\n",
      "Student pre-training Epoch 248: avg loss = 0.8668\n",
      "Student pre-training Epoch 249: avg loss = 0.8698\n",
      "Student pre-training Epoch 250: avg loss = 0.8633\n",
      "Student pre-training Epoch 251: avg loss = 0.8608\n",
      "Student pre-training Epoch 252: avg loss = 0.8499\n",
      "Student pre-training Epoch 253: avg loss = 0.8574\n",
      "Student pre-training Epoch 254: avg loss = 0.8607\n",
      "Student pre-training Epoch 255: avg loss = 0.8610\n",
      "Student pre-training Epoch 256: avg loss = 0.8604\n",
      "Student pre-training Epoch 257: avg loss = 0.8520\n",
      "Student pre-training Epoch 258: avg loss = 0.8611\n",
      "Student pre-training Epoch 259: avg loss = 0.8492\n",
      "Student pre-training Epoch 260: avg loss = 0.8377\n",
      "Student pre-training Epoch 261: avg loss = 0.8508\n",
      "Student pre-training Epoch 262: avg loss = 0.8487\n",
      "Student pre-training Epoch 263: avg loss = 0.8510\n",
      "Student pre-training Epoch 264: avg loss = 0.8374\n",
      "Student pre-training Epoch 265: avg loss = 0.8564\n",
      "Student pre-training Epoch 266: avg loss = 0.8433\n",
      "Student pre-training Epoch 267: avg loss = 0.8447\n",
      "Student pre-training Epoch 268: avg loss = 0.8369\n",
      "Student pre-training Epoch 269: avg loss = 0.8362\n",
      "Student pre-training Epoch 270: avg loss = 0.8350\n",
      "Student pre-training Epoch 271: avg loss = 0.8482\n",
      "Student pre-training Epoch 272: avg loss = 0.8372\n",
      "Student pre-training Epoch 273: avg loss = 0.8516\n",
      "Student pre-training Epoch 274: avg loss = 0.8300\n",
      "Student pre-training Epoch 275: avg loss = 0.8428\n",
      "Student pre-training Epoch 276: avg loss = 0.8415\n",
      "Student pre-training Epoch 277: avg loss = 0.8370\n",
      "Student pre-training Epoch 278: avg loss = 0.8453\n",
      "Student pre-training Epoch 279: avg loss = 0.8371\n",
      "Student pre-training Epoch 280: avg loss = 0.8358\n",
      "Student pre-training Epoch 281: avg loss = 0.8388\n",
      "Student pre-training Epoch 282: avg loss = 0.8307\n",
      "Student pre-training Epoch 283: avg loss = 0.8389\n",
      "Student pre-training Epoch 284: avg loss = 0.8228\n",
      "Student pre-training Epoch 285: avg loss = 0.8291\n",
      "Student pre-training Epoch 286: avg loss = 0.8229\n",
      "Student pre-training Epoch 287: avg loss = 0.8360\n",
      "Student pre-training Epoch 288: avg loss = 0.8217\n",
      "Student pre-training Epoch 289: avg loss = 0.8132\n",
      "Student pre-training Epoch 290: avg loss = 0.8309\n",
      "Student pre-training Epoch 291: avg loss = 0.8256\n",
      "Student pre-training Epoch 292: avg loss = 0.8264\n",
      "Student pre-training Epoch 293: avg loss = 0.8305\n",
      "Student pre-training Epoch 294: avg loss = 0.8327\n",
      "Student pre-training Epoch 295: avg loss = 0.8261\n",
      "Student pre-training Epoch 296: avg loss = 0.8160\n",
      "Student pre-training Epoch 297: avg loss = 0.8224\n",
      "Student pre-training Epoch 298: avg loss = 0.8246\n",
      "Student pre-training Epoch 299: avg loss = 0.8185\n",
      "Student pre-training Epoch 300: avg loss = 0.8114\n",
      "Student pre-training Epoch 301: avg loss = 0.8090\n",
      "Student pre-training Epoch 302: avg loss = 0.8230\n",
      "Student pre-training Epoch 303: avg loss = 0.8112\n",
      "Student pre-training Epoch 304: avg loss = 0.8230\n",
      "Student pre-training Epoch 305: avg loss = 0.8300\n",
      "Student pre-training Epoch 306: avg loss = 0.8207\n",
      "Student pre-training Epoch 307: avg loss = 0.8175\n",
      "Student pre-training Epoch 308: avg loss = 0.8066\n",
      "Student pre-training Epoch 309: avg loss = 0.8159\n",
      "Student pre-training Epoch 310: avg loss = 0.8138\n",
      "Student pre-training Epoch 311: avg loss = 0.8027\n",
      "Student pre-training Epoch 312: avg loss = 0.8103\n",
      "Student pre-training Epoch 313: avg loss = 0.7951\n",
      "Student pre-training Epoch 314: avg loss = 0.7860\n",
      "Student pre-training Epoch 315: avg loss = 0.7857\n",
      "Student pre-training Epoch 316: avg loss = 0.8019\n",
      "Student pre-training Epoch 317: avg loss = 0.8014\n",
      "Student pre-training Epoch 318: avg loss = 0.7914\n",
      "Student pre-training Epoch 319: avg loss = 0.7873\n",
      "Student pre-training Epoch 320: avg loss = 0.7975\n",
      "Student pre-training Epoch 321: avg loss = 0.7927\n",
      "Student pre-training Epoch 322: avg loss = 0.7977\n",
      "Student pre-training Epoch 323: avg loss = 0.8116\n",
      "Student pre-training Epoch 324: avg loss = 0.7962\n",
      "Student pre-training Epoch 325: avg loss = 0.8003\n",
      "Student pre-training Epoch 326: avg loss = 0.7911\n",
      "Student pre-training Epoch 327: avg loss = 0.7914\n",
      "Student pre-training Epoch 328: avg loss = 0.7913\n",
      "Student pre-training Epoch 329: avg loss = 0.7976\n",
      "Student pre-training Epoch 330: avg loss = 0.7974\n",
      "Student pre-training Epoch 331: avg loss = 0.7918\n",
      "Student pre-training Epoch 332: avg loss = 0.7871\n",
      "Student pre-training Epoch 333: avg loss = 0.7854\n",
      "Student pre-training Epoch 334: avg loss = 0.7911\n",
      "Student pre-training Epoch 335: avg loss = 0.7933\n",
      "Student pre-training Epoch 336: avg loss = 0.7928\n",
      "Student pre-training Epoch 337: avg loss = 0.7924\n",
      "Student pre-training Epoch 338: avg loss = 0.7865\n",
      "Student pre-training Epoch 339: avg loss = 0.7859\n",
      "Student pre-training Epoch 340: avg loss = 0.7904\n",
      "Student pre-training Epoch 341: avg loss = 0.7949\n",
      "Student pre-training Epoch 342: avg loss = 0.7635\n",
      "Student pre-training Epoch 343: avg loss = 0.7749\n",
      "Student pre-training Epoch 344: avg loss = 0.7907\n",
      "Student pre-training Epoch 345: avg loss = 0.7847\n",
      "Student pre-training Epoch 346: avg loss = 0.7891\n",
      "Student pre-training Epoch 347: avg loss = 0.7852\n",
      "Student pre-training Epoch 348: avg loss = 0.7824\n",
      "Student pre-training Epoch 349: avg loss = 0.7910\n",
      "Student pre-training Epoch 350: avg loss = 0.7741\n",
      "Student pre-training Epoch 351: avg loss = 0.7816\n",
      "Student pre-training Epoch 352: avg loss = 0.7710\n",
      "Student pre-training Epoch 353: avg loss = 0.7894\n",
      "Student pre-training Epoch 354: avg loss = 0.7670\n",
      "Student pre-training Epoch 355: avg loss = 0.7924\n",
      "Student pre-training Epoch 356: avg loss = 0.7699\n",
      "Student pre-training Epoch 357: avg loss = 0.7732\n",
      "Student pre-training Epoch 358: avg loss = 0.7726\n",
      "Student pre-training Epoch 359: avg loss = 0.7745\n",
      "Student pre-training Epoch 360: avg loss = 0.7702\n",
      "Student pre-training Epoch 361: avg loss = 0.7698\n",
      "Student pre-training Epoch 362: avg loss = 0.7731\n",
      "Student pre-training Epoch 363: avg loss = 0.7719\n",
      "Student pre-training Epoch 364: avg loss = 0.7699\n",
      "Student pre-training Epoch 365: avg loss = 0.7683\n",
      "Student pre-training Epoch 366: avg loss = 0.7712\n",
      "Student pre-training Epoch 367: avg loss = 0.7753\n",
      "Student pre-training Epoch 368: avg loss = 0.7615\n",
      "Student pre-training Epoch 369: avg loss = 0.7713\n",
      "Student pre-training Epoch 370: avg loss = 0.7740\n",
      "Student pre-training Epoch 371: avg loss = 0.7602\n",
      "Student pre-training Epoch 372: avg loss = 0.7679\n",
      "Student pre-training Epoch 373: avg loss = 0.7650\n",
      "Student pre-training Epoch 374: avg loss = 0.7567\n",
      "Student pre-training Epoch 375: avg loss = 0.7759\n",
      "Student pre-training Epoch 376: avg loss = 0.7767\n",
      "Student pre-training Epoch 377: avg loss = 0.7539\n",
      "Student pre-training Epoch 378: avg loss = 0.7665\n",
      "Student pre-training Epoch 379: avg loss = 0.7706\n",
      "Student pre-training Epoch 380: avg loss = 0.7615\n",
      "Student pre-training Epoch 381: avg loss = 0.7506\n",
      "Student pre-training Epoch 382: avg loss = 0.7535\n",
      "Student pre-training Epoch 383: avg loss = 0.7589\n",
      "Student pre-training Epoch 384: avg loss = 0.7460\n",
      "Student pre-training Epoch 385: avg loss = 0.7516\n",
      "Student pre-training Epoch 386: avg loss = 0.7581\n",
      "Student pre-training Epoch 387: avg loss = 0.7653\n",
      "Student pre-training Epoch 388: avg loss = 0.7587\n",
      "Student pre-training Epoch 389: avg loss = 0.7513\n",
      "Student pre-training Epoch 390: avg loss = 0.7528\n",
      "Student pre-training Epoch 391: avg loss = 0.7443\n",
      "Student pre-training Epoch 392: avg loss = 0.7667\n",
      "Student pre-training Epoch 393: avg loss = 0.7614\n",
      "Student pre-training Epoch 394: avg loss = 0.7546\n",
      "Student pre-training Epoch 395: avg loss = 0.7398\n",
      "Student pre-training Epoch 396: avg loss = 0.7351\n",
      "Student pre-training Epoch 397: avg loss = 0.7336\n",
      "Student pre-training Epoch 398: avg loss = 0.7454\n",
      "Student pre-training Epoch 399: avg loss = 0.7451\n",
      "Student pre-training Epoch 400: avg loss = 0.7572\n",
      "Student pre-training Epoch 401: avg loss = 0.7532\n",
      "Student pre-training Epoch 402: avg loss = 0.7451\n",
      "Student pre-training Epoch 403: avg loss = 0.7475\n",
      "Student pre-training Epoch 404: avg loss = 0.7603\n",
      "Student pre-training Epoch 405: avg loss = 0.7467\n",
      "Student pre-training Epoch 406: avg loss = 0.7455\n",
      "Student pre-training Epoch 407: avg loss = 0.7522\n",
      "Student pre-training Epoch 408: avg loss = 0.7524\n",
      "Student pre-training Epoch 409: avg loss = 0.7420\n",
      "Student pre-training Epoch 410: avg loss = 0.7525\n",
      "Student pre-training Epoch 411: avg loss = 0.7479\n",
      "Student pre-training Epoch 412: avg loss = 0.7493\n",
      "Student pre-training Epoch 413: avg loss = 0.7402\n",
      "Student pre-training Epoch 414: avg loss = 0.7419\n",
      "Student pre-training Epoch 415: avg loss = 0.7456\n",
      "Student pre-training Epoch 416: avg loss = 0.7520\n",
      "Student pre-training Epoch 417: avg loss = 0.7454\n",
      "Student pre-training Epoch 418: avg loss = 0.7507\n",
      "Student pre-training Epoch 419: avg loss = 0.7505\n",
      "Student pre-training Epoch 420: avg loss = 0.7390\n",
      "Student pre-training Epoch 421: avg loss = 0.7566\n",
      "Student pre-training Epoch 422: avg loss = 0.7388\n",
      "Student pre-training Epoch 423: avg loss = 0.7338\n",
      "Student pre-training Epoch 424: avg loss = 0.7204\n",
      "Student pre-training Epoch 425: avg loss = 0.7317\n",
      "Student pre-training Epoch 426: avg loss = 0.7286\n",
      "Student pre-training Epoch 427: avg loss = 0.7386\n",
      "Student pre-training Epoch 428: avg loss = 0.7219\n",
      "Student pre-training Epoch 429: avg loss = 0.7355\n",
      "Student pre-training Epoch 430: avg loss = 0.7401\n",
      "Student pre-training Epoch 431: avg loss = 0.7375\n",
      "Student pre-training Epoch 432: avg loss = 0.7418\n",
      "Student pre-training Epoch 433: avg loss = 0.7402\n",
      "Student pre-training Epoch 434: avg loss = 0.7299\n",
      "Student pre-training Epoch 435: avg loss = 0.7284\n",
      "Student pre-training Epoch 436: avg loss = 0.7196\n",
      "Student pre-training Epoch 437: avg loss = 0.7437\n",
      "Student pre-training Epoch 438: avg loss = 0.7207\n",
      "Student pre-training Epoch 439: avg loss = 0.7276\n",
      "Student pre-training Epoch 440: avg loss = 0.7130\n",
      "Student pre-training Epoch 441: avg loss = 0.7353\n",
      "Student pre-training Epoch 442: avg loss = 0.7220\n",
      "Student pre-training Epoch 443: avg loss = 0.7150\n",
      "Student pre-training Epoch 444: avg loss = 0.7175\n",
      "Student pre-training Epoch 445: avg loss = 0.7278\n",
      "Student pre-training Epoch 446: avg loss = 0.7313\n",
      "Student pre-training Epoch 447: avg loss = 0.7265\n",
      "Student pre-training Epoch 448: avg loss = 0.7219\n",
      "Student pre-training Epoch 449: avg loss = 0.7245\n",
      "Student pre-training Epoch 450: avg loss = 0.7348\n",
      "Student pre-training Epoch 451: avg loss = 0.7208\n",
      "Student pre-training Epoch 452: avg loss = 0.7266\n",
      "Student pre-training Epoch 453: avg loss = 0.7238\n",
      "Student pre-training Epoch 454: avg loss = 0.7250\n",
      "Student pre-training Epoch 455: avg loss = 0.7219\n",
      "Student pre-training Epoch 456: avg loss = 0.7246\n",
      "Student pre-training Epoch 457: avg loss = 0.7251\n",
      "Student pre-training Epoch 458: avg loss = 0.7244\n",
      "Student pre-training Epoch 459: avg loss = 0.7167\n",
      "Student pre-training Epoch 460: avg loss = 0.7118\n",
      "Student pre-training Epoch 461: avg loss = 0.7182\n",
      "Student pre-training Epoch 462: avg loss = 0.7272\n",
      "Student pre-training Epoch 463: avg loss = 0.7111\n",
      "Student pre-training Epoch 464: avg loss = 0.7227\n",
      "Student pre-training Epoch 465: avg loss = 0.7238\n",
      "Student pre-training Epoch 466: avg loss = 0.7051\n",
      "Student pre-training Epoch 467: avg loss = 0.7086\n",
      "Student pre-training Epoch 468: avg loss = 0.7147\n",
      "Student pre-training Epoch 469: avg loss = 0.7147\n",
      "Student pre-training Epoch 470: avg loss = 0.7021\n",
      "Student pre-training Epoch 471: avg loss = 0.7284\n",
      "Student pre-training Epoch 472: avg loss = 0.7193\n",
      "Student pre-training Epoch 473: avg loss = 0.7156\n",
      "Student pre-training Epoch 474: avg loss = 0.7064\n",
      "Student pre-training Epoch 475: avg loss = 0.7291\n",
      "Student pre-training Epoch 476: avg loss = 0.7171\n",
      "Student pre-training Epoch 477: avg loss = 0.7197\n",
      "Student pre-training Epoch 478: avg loss = 0.7249\n",
      "Student pre-training Epoch 479: avg loss = 0.7073\n",
      "Student pre-training Epoch 480: avg loss = 0.7084\n",
      "Student pre-training Epoch 481: avg loss = 0.7081\n",
      "Student pre-training Epoch 482: avg loss = 0.7104\n",
      "Student pre-training Epoch 483: avg loss = 0.7028\n",
      "Student pre-training Epoch 484: avg loss = 0.7005\n",
      "Student pre-training Epoch 485: avg loss = 0.7041\n",
      "Student pre-training Epoch 486: avg loss = 0.7053\n",
      "Student pre-training Epoch 487: avg loss = 0.7018\n",
      "Student pre-training Epoch 488: avg loss = 0.7069\n",
      "Student pre-training Epoch 489: avg loss = 0.7081\n",
      "Student pre-training Epoch 490: avg loss = 0.7068\n",
      "Student pre-training Epoch 491: avg loss = 0.7073\n",
      "Student pre-training Epoch 492: avg loss = 0.7112\n",
      "Student pre-training Epoch 493: avg loss = 0.7141\n",
      "Student pre-training Epoch 494: avg loss = 0.7058\n",
      "Student pre-training Epoch 495: avg loss = 0.7053\n",
      "Student pre-training Epoch 496: avg loss = 0.6947\n",
      "Student pre-training Epoch 497: avg loss = 0.6967\n",
      "Student pre-training Epoch 498: avg loss = 0.6921\n",
      "Student pre-training Epoch 499: avg loss = 0.7136\n",
      "Student pre-training Epoch 500: avg loss = 0.7096\n",
      "Student pre-training Epoch 501: avg loss = 0.6894\n",
      "Student pre-training Epoch 502: avg loss = 0.7045\n",
      "Student pre-training Epoch 503: avg loss = 0.7019\n",
      "Student pre-training Epoch 504: avg loss = 0.6977\n",
      "Student pre-training Epoch 505: avg loss = 0.7104\n",
      "Student pre-training Epoch 506: avg loss = 0.6968\n",
      "Student pre-training Epoch 507: avg loss = 0.6950\n",
      "Student pre-training Epoch 508: avg loss = 0.6932\n",
      "Student pre-training Epoch 509: avg loss = 0.6973\n",
      "Student pre-training Epoch 510: avg loss = 0.7030\n",
      "Student pre-training Epoch 511: avg loss = 0.7096\n",
      "Student pre-training Epoch 512: avg loss = 0.7035\n",
      "Student pre-training Epoch 513: avg loss = 0.7046\n",
      "Student pre-training Epoch 514: avg loss = 0.6882\n",
      "Student pre-training Epoch 515: avg loss = 0.6843\n",
      "Student pre-training Epoch 516: avg loss = 0.6951\n",
      "Student pre-training Epoch 517: avg loss = 0.6962\n",
      "Student pre-training Epoch 518: avg loss = 0.6932\n",
      "Student pre-training Epoch 519: avg loss = 0.7018\n",
      "Student pre-training Epoch 520: avg loss = 0.6934\n",
      "Student pre-training Epoch 521: avg loss = 0.7041\n",
      "Student pre-training Epoch 522: avg loss = 0.6913\n",
      "Student pre-training Epoch 523: avg loss = 0.6948\n",
      "Student pre-training Epoch 524: avg loss = 0.6863\n",
      "Student pre-training Epoch 525: avg loss = 0.7050\n",
      "Student pre-training Epoch 526: avg loss = 0.6829\n",
      "Student pre-training Epoch 527: avg loss = 0.6788\n",
      "Student pre-training Epoch 528: avg loss = 0.6974\n",
      "Student pre-training Epoch 529: avg loss = 0.6929\n",
      "Student pre-training Epoch 530: avg loss = 0.6826\n",
      "Student pre-training Epoch 531: avg loss = 0.6876\n",
      "Student pre-training Epoch 532: avg loss = 0.6919\n",
      "Student pre-training Epoch 533: avg loss = 0.6811\n",
      "Student pre-training Epoch 534: avg loss = 0.6818\n",
      "Student pre-training Epoch 535: avg loss = 0.6878\n",
      "Student pre-training Epoch 536: avg loss = 0.6729\n",
      "Student pre-training Epoch 537: avg loss = 0.6849\n",
      "Student pre-training Epoch 538: avg loss = 0.6859\n",
      "Student pre-training Epoch 539: avg loss = 0.6787\n",
      "Student pre-training Epoch 540: avg loss = 0.6882\n",
      "Student pre-training Epoch 541: avg loss = 0.6804\n",
      "Student pre-training Epoch 542: avg loss = 0.6990\n",
      "Student pre-training Epoch 543: avg loss = 0.6875\n",
      "Student pre-training Epoch 544: avg loss = 0.6848\n",
      "Student pre-training Epoch 545: avg loss = 0.6800\n",
      "Student pre-training Epoch 546: avg loss = 0.6883\n",
      "Student pre-training Epoch 547: avg loss = 0.6848\n",
      "Student pre-training Epoch 548: avg loss = 0.6854\n",
      "Student pre-training Epoch 549: avg loss = 0.6828\n",
      "Student pre-training Epoch 550: avg loss = 0.6631\n",
      "Student pre-training Epoch 551: avg loss = 0.6797\n",
      "Student pre-training Epoch 552: avg loss = 0.6734\n",
      "Student pre-training Epoch 553: avg loss = 0.6830\n",
      "Student pre-training Epoch 554: avg loss = 0.6589\n",
      "Student pre-training Epoch 555: avg loss = 0.6766\n",
      "Student pre-training Epoch 556: avg loss = 0.6758\n",
      "Student pre-training Epoch 557: avg loss = 0.6901\n",
      "Student pre-training Epoch 558: avg loss = 0.6837\n",
      "Student pre-training Epoch 559: avg loss = 0.6778\n",
      "Student pre-training Epoch 560: avg loss = 0.6744\n",
      "Student pre-training Epoch 561: avg loss = 0.6593\n",
      "Student pre-training Epoch 562: avg loss = 0.6639\n",
      "Student pre-training Epoch 563: avg loss = 0.6718\n",
      "Student pre-training Epoch 564: avg loss = 0.6591\n",
      "Student pre-training Epoch 565: avg loss = 0.6763\n",
      "Student pre-training Epoch 566: avg loss = 0.6631\n",
      "Student pre-training Epoch 567: avg loss = 0.6682\n",
      "Student pre-training Epoch 568: avg loss = 0.6687\n",
      "Student pre-training Epoch 569: avg loss = 0.6769\n",
      "Student pre-training Epoch 570: avg loss = 0.6639\n",
      "Student pre-training Epoch 571: avg loss = 0.6775\n",
      "Student pre-training Epoch 572: avg loss = 0.6590\n",
      "Student pre-training Epoch 573: avg loss = 0.6684\n",
      "Student pre-training Epoch 574: avg loss = 0.6697\n",
      "Student pre-training Epoch 575: avg loss = 0.6715\n",
      "Student pre-training Epoch 576: avg loss = 0.6777\n",
      "Student pre-training Epoch 577: avg loss = 0.6706\n",
      "Student pre-training Epoch 578: avg loss = 0.6631\n",
      "Student pre-training Epoch 579: avg loss = 0.6620\n",
      "Student pre-training Epoch 580: avg loss = 0.6656\n",
      "Student pre-training Epoch 581: avg loss = 0.6752\n",
      "Student pre-training Epoch 582: avg loss = 0.6659\n",
      "Student pre-training Epoch 583: avg loss = 0.6766\n",
      "Student pre-training Epoch 584: avg loss = 0.6723\n",
      "Student pre-training Epoch 585: avg loss = 0.6709\n",
      "Student pre-training Epoch 586: avg loss = 0.6548\n",
      "Student pre-training Epoch 587: avg loss = 0.6528\n",
      "Student pre-training Epoch 588: avg loss = 0.6727\n",
      "Student pre-training Epoch 589: avg loss = 0.6663\n",
      "Student pre-training Epoch 590: avg loss = 0.6605\n",
      "Student pre-training Epoch 591: avg loss = 0.6607\n",
      "Student pre-training Epoch 592: avg loss = 0.6648\n",
      "Student pre-training Epoch 593: avg loss = 0.6472\n",
      "Student pre-training Epoch 594: avg loss = 0.6624\n",
      "Student pre-training Epoch 595: avg loss = 0.6599\n",
      "Student pre-training Epoch 596: avg loss = 0.6623\n",
      "Student pre-training Epoch 597: avg loss = 0.6441\n",
      "Student pre-training Epoch 598: avg loss = 0.6692\n",
      "Student pre-training Epoch 599: avg loss = 0.6574\n",
      "Student pre-training Epoch 600: avg loss = 0.6688\n",
      "Student pre-training Epoch 601: avg loss = 0.6426\n",
      "Student pre-training Epoch 602: avg loss = 0.6725\n",
      "Student pre-training Epoch 603: avg loss = 0.6633\n",
      "Student pre-training Epoch 604: avg loss = 0.6495\n",
      "Student pre-training Epoch 605: avg loss = 0.6505\n",
      "Student pre-training Epoch 606: avg loss = 0.6672\n",
      "Student pre-training Epoch 607: avg loss = 0.6499\n",
      "Student pre-training Epoch 608: avg loss = 0.6697\n",
      "Student pre-training Epoch 609: avg loss = 0.6600\n",
      "Student pre-training Epoch 610: avg loss = 0.6545\n",
      "Student pre-training Epoch 611: avg loss = 0.6522\n",
      "Student pre-training Epoch 612: avg loss = 0.6542\n",
      "Student pre-training Epoch 613: avg loss = 0.6590\n",
      "Student pre-training Epoch 614: avg loss = 0.6514\n",
      "Student pre-training Epoch 615: avg loss = 0.6576\n",
      "Student pre-training Epoch 616: avg loss = 0.6415\n",
      "Student pre-training Epoch 617: avg loss = 0.6605\n",
      "Student pre-training Epoch 618: avg loss = 0.6383\n",
      "Student pre-training Epoch 619: avg loss = 0.6552\n",
      "Student pre-training Epoch 620: avg loss = 0.6532\n",
      "Student pre-training Epoch 621: avg loss = 0.6564\n",
      "Student pre-training Epoch 622: avg loss = 0.6465\n",
      "Student pre-training Epoch 623: avg loss = 0.6358\n",
      "Student pre-training Epoch 624: avg loss = 0.6436\n",
      "Student pre-training Epoch 625: avg loss = 0.6414\n",
      "Student pre-training Epoch 626: avg loss = 0.6575\n",
      "Student pre-training Epoch 627: avg loss = 0.6540\n",
      "Student pre-training Epoch 628: avg loss = 0.6426\n",
      "Student pre-training Epoch 629: avg loss = 0.6508\n",
      "Student pre-training Epoch 630: avg loss = 0.6219\n",
      "Student pre-training Epoch 631: avg loss = 0.6439\n",
      "Student pre-training Epoch 632: avg loss = 0.6511\n",
      "Student pre-training Epoch 633: avg loss = 0.6345\n",
      "Student pre-training Epoch 634: avg loss = 0.6337\n",
      "Student pre-training Epoch 635: avg loss = 0.6524\n",
      "Student pre-training Epoch 636: avg loss = 0.6441\n",
      "Student pre-training Epoch 637: avg loss = 0.6436\n",
      "Student pre-training Epoch 638: avg loss = 0.6518\n",
      "Student pre-training Epoch 639: avg loss = 0.6507\n",
      "Student pre-training Epoch 640: avg loss = 0.6494\n",
      "Student pre-training Epoch 641: avg loss = 0.6434\n",
      "Student pre-training Epoch 642: avg loss = 0.6364\n",
      "Student pre-training Epoch 643: avg loss = 0.6392\n",
      "Student pre-training Epoch 644: avg loss = 0.6351\n",
      "Student pre-training Epoch 645: avg loss = 0.6436\n",
      "Student pre-training Epoch 646: avg loss = 0.6527\n",
      "Student pre-training Epoch 647: avg loss = 0.6327\n",
      "Student pre-training Epoch 648: avg loss = 0.6420\n",
      "Student pre-training Epoch 649: avg loss = 0.6258\n",
      "Student pre-training Epoch 650: avg loss = 0.6360\n",
      "Student pre-training Epoch 651: avg loss = 0.6506\n",
      "Student pre-training Epoch 652: avg loss = 0.6448\n",
      "Student pre-training Epoch 653: avg loss = 0.6401\n",
      "Student pre-training Epoch 654: avg loss = 0.6330\n",
      "Student pre-training Epoch 655: avg loss = 0.6362\n",
      "Student pre-training Epoch 656: avg loss = 0.6394\n",
      "Student pre-training Epoch 657: avg loss = 0.6259\n",
      "Student pre-training Epoch 658: avg loss = 0.6354\n",
      "Student pre-training Epoch 659: avg loss = 0.6347\n",
      "Student pre-training Epoch 660: avg loss = 0.6342\n",
      "Student pre-training Epoch 661: avg loss = 0.6386\n",
      "Student pre-training Epoch 662: avg loss = 0.6343\n",
      "Student pre-training Epoch 663: avg loss = 0.6320\n",
      "Student pre-training Epoch 664: avg loss = 0.6340\n",
      "Student pre-training Epoch 665: avg loss = 0.6408\n",
      "Student pre-training Epoch 666: avg loss = 0.6168\n",
      "Student pre-training Epoch 667: avg loss = 0.6435\n",
      "Student pre-training Epoch 668: avg loss = 0.6277\n",
      "Student pre-training Epoch 669: avg loss = 0.6369\n",
      "Student pre-training Epoch 670: avg loss = 0.6198\n",
      "Student pre-training Epoch 671: avg loss = 0.6170\n",
      "Student pre-training Epoch 672: avg loss = 0.6207\n",
      "Student pre-training Epoch 673: avg loss = 0.6350\n",
      "Student pre-training Epoch 674: avg loss = 0.6294\n",
      "Student pre-training Epoch 675: avg loss = 0.6473\n",
      "Student pre-training Epoch 676: avg loss = 0.6396\n",
      "Student pre-training Epoch 677: avg loss = 0.6215\n",
      "Student pre-training Epoch 678: avg loss = 0.6211\n",
      "Student pre-training Epoch 679: avg loss = 0.6298\n",
      "Student pre-training Epoch 680: avg loss = 0.6254\n",
      "Student pre-training Epoch 681: avg loss = 0.6353\n",
      "Student pre-training Epoch 682: avg loss = 0.6301\n",
      "Student pre-training Epoch 683: avg loss = 0.6215\n",
      "Student pre-training Epoch 684: avg loss = 0.6241\n",
      "Student pre-training Epoch 685: avg loss = 0.6200\n",
      "Student pre-training Epoch 686: avg loss = 0.6124\n",
      "Student pre-training Epoch 687: avg loss = 0.6282\n",
      "Student pre-training Epoch 688: avg loss = 0.6152\n",
      "Student pre-training Epoch 689: avg loss = 0.6313\n",
      "Student pre-training Epoch 690: avg loss = 0.6181\n",
      "Student pre-training Epoch 691: avg loss = 0.6315\n",
      "Student pre-training Epoch 692: avg loss = 0.6248\n",
      "Student pre-training Epoch 693: avg loss = 0.6356\n",
      "Student pre-training Epoch 694: avg loss = 0.6287\n",
      "Student pre-training Epoch 695: avg loss = 0.6162\n",
      "Student pre-training Epoch 696: avg loss = 0.6237\n",
      "Student pre-training Epoch 697: avg loss = 0.6296\n",
      "Student pre-training Epoch 698: avg loss = 0.6329\n",
      "Student pre-training Epoch 699: avg loss = 0.6100\n",
      "Student pre-training Epoch 700: avg loss = 0.6088\n",
      "Student pre-training Epoch 701: avg loss = 0.6221\n",
      "Student pre-training Epoch 702: avg loss = 0.6230\n",
      "Student pre-training Epoch 703: avg loss = 0.6190\n",
      "Student pre-training Epoch 704: avg loss = 0.6117\n",
      "Student pre-training Epoch 705: avg loss = 0.6207\n",
      "Student pre-training Epoch 706: avg loss = 0.6116\n",
      "Student pre-training Epoch 707: avg loss = 0.6254\n",
      "Student pre-training Epoch 708: avg loss = 0.6096\n",
      "Student pre-training Epoch 709: avg loss = 0.6199\n",
      "Student pre-training Epoch 710: avg loss = 0.6304\n",
      "Student pre-training Epoch 711: avg loss = 0.6127\n",
      "Student pre-training Epoch 712: avg loss = 0.6172\n",
      "Student pre-training Epoch 713: avg loss = 0.6354\n",
      "Student pre-training Epoch 714: avg loss = 0.5973\n",
      "Student pre-training Epoch 715: avg loss = 0.6248\n",
      "Student pre-training Epoch 716: avg loss = 0.6255\n",
      "Student pre-training Epoch 717: avg loss = 0.6149\n",
      "Student pre-training Epoch 718: avg loss = 0.6238\n",
      "Student pre-training Epoch 719: avg loss = 0.6140\n",
      "Student pre-training Epoch 720: avg loss = 0.6092\n",
      "Student pre-training Epoch 721: avg loss = 0.6110\n",
      "Student pre-training Epoch 722: avg loss = 0.6227\n",
      "Student pre-training Epoch 723: avg loss = 0.6090\n",
      "Student pre-training Epoch 724: avg loss = 0.6145\n",
      "Student pre-training Epoch 725: avg loss = 0.6131\n",
      "Student pre-training Epoch 726: avg loss = 0.6078\n",
      "Student pre-training Epoch 727: avg loss = 0.6111\n",
      "Student pre-training Epoch 728: avg loss = 0.6148\n",
      "Student pre-training Epoch 729: avg loss = 0.6088\n",
      "Student pre-training Epoch 730: avg loss = 0.6059\n",
      "Student pre-training Epoch 731: avg loss = 0.5900\n",
      "Student pre-training Epoch 732: avg loss = 0.5995\n",
      "Student pre-training Epoch 733: avg loss = 0.6132\n",
      "Student pre-training Epoch 734: avg loss = 0.6147\n",
      "Student pre-training Epoch 735: avg loss = 0.6141\n",
      "Student pre-training Epoch 736: avg loss = 0.6118\n",
      "Student pre-training Epoch 737: avg loss = 0.6092\n",
      "Student pre-training Epoch 738: avg loss = 0.5931\n",
      "Student pre-training Epoch 739: avg loss = 0.5991\n",
      "Student pre-training Epoch 740: avg loss = 0.6127\n",
      "Student pre-training Epoch 741: avg loss = 0.6083\n",
      "Student pre-training Epoch 742: avg loss = 0.6007\n",
      "Student pre-training Epoch 743: avg loss = 0.6146\n",
      "Student pre-training Epoch 744: avg loss = 0.5901\n",
      "Student pre-training Epoch 745: avg loss = 0.6071\n",
      "Student pre-training Epoch 746: avg loss = 0.6101\n",
      "Student pre-training Epoch 747: avg loss = 0.6011\n",
      "Student pre-training Epoch 748: avg loss = 0.6103\n",
      "Student pre-training Epoch 749: avg loss = 0.6046\n",
      "Student pre-training Epoch 750: avg loss = 0.6000\n",
      "Student pre-training Epoch 751: avg loss = 0.6065\n",
      "Student pre-training Epoch 752: avg loss = 0.5976\n",
      "Student pre-training Epoch 753: avg loss = 0.6043\n",
      "Student pre-training Epoch 754: avg loss = 0.6130\n",
      "Student pre-training Epoch 755: avg loss = 0.5942\n",
      "Student pre-training Epoch 756: avg loss = 0.5960\n",
      "Student pre-training Epoch 757: avg loss = 0.6023\n",
      "Student pre-training Epoch 758: avg loss = 0.6060\n",
      "Student pre-training Epoch 759: avg loss = 0.5984\n",
      "Student pre-training Epoch 760: avg loss = 0.5977\n",
      "Student pre-training Epoch 761: avg loss = 0.6040\n",
      "Student pre-training Epoch 762: avg loss = 0.6095\n",
      "Student pre-training Epoch 763: avg loss = 0.6041\n",
      "Student pre-training Epoch 764: avg loss = 0.6101\n",
      "Student pre-training Epoch 765: avg loss = 0.6061\n",
      "Student pre-training Epoch 766: avg loss = 0.5990\n",
      "Student pre-training Epoch 767: avg loss = 0.6025\n",
      "Student pre-training Epoch 768: avg loss = 0.5841\n",
      "Student pre-training Epoch 769: avg loss = 0.5991\n",
      "Student pre-training Epoch 770: avg loss = 0.6073\n",
      "Student pre-training Epoch 771: avg loss = 0.5877\n",
      "Student pre-training Epoch 772: avg loss = 0.6088\n",
      "Student pre-training Epoch 773: avg loss = 0.5902\n",
      "Student pre-training Epoch 774: avg loss = 0.6029\n",
      "Student pre-training Epoch 775: avg loss = 0.5978\n",
      "Student pre-training Epoch 776: avg loss = 0.5842\n",
      "Student pre-training Epoch 777: avg loss = 0.5992\n",
      "Student pre-training Epoch 778: avg loss = 0.5994\n",
      "Student pre-training Epoch 779: avg loss = 0.6066\n",
      "Student pre-training Epoch 780: avg loss = 0.5975\n",
      "Student pre-training Epoch 781: avg loss = 0.5978\n",
      "Student pre-training Epoch 782: avg loss = 0.5951\n",
      "Student pre-training Epoch 783: avg loss = 0.6041\n",
      "Student pre-training Epoch 784: avg loss = 0.6064\n",
      "Student pre-training Epoch 785: avg loss = 0.6105\n",
      "Student pre-training Epoch 786: avg loss = 0.5953\n",
      "Student pre-training Epoch 787: avg loss = 0.5958\n",
      "Student pre-training Epoch 788: avg loss = 0.6019\n",
      "Student pre-training Epoch 789: avg loss = 0.5948\n",
      "Student pre-training Epoch 790: avg loss = 0.5927\n",
      "Student pre-training Epoch 791: avg loss = 0.5925\n",
      "Student pre-training Epoch 792: avg loss = 0.5771\n",
      "Student pre-training Epoch 793: avg loss = 0.6000\n",
      "Student pre-training Epoch 794: avg loss = 0.5925\n",
      "Student pre-training Epoch 795: avg loss = 0.5870\n",
      "Student pre-training Epoch 796: avg loss = 0.5922\n",
      "Student pre-training Epoch 797: avg loss = 0.5758\n",
      "Student pre-training Epoch 798: avg loss = 0.5948\n",
      "Student pre-training Epoch 799: avg loss = 0.5751\n",
      "Student pre-training Epoch 800: avg loss = 0.5823\n",
      "Student pre-training Epoch 801: avg loss = 0.5955\n",
      "Student pre-training Epoch 802: avg loss = 0.5836\n",
      "Student pre-training Epoch 803: avg loss = 0.5978\n",
      "Student pre-training Epoch 804: avg loss = 0.5897\n",
      "Student pre-training Epoch 805: avg loss = 0.5876\n",
      "Student pre-training Epoch 806: avg loss = 0.5897\n",
      "Student pre-training Epoch 807: avg loss = 0.5876\n",
      "Student pre-training Epoch 808: avg loss = 0.5949\n",
      "Student pre-training Epoch 809: avg loss = 0.5960\n",
      "Student pre-training Epoch 810: avg loss = 0.5843\n",
      "Student pre-training Epoch 811: avg loss = 0.5973\n",
      "Student pre-training Epoch 812: avg loss = 0.5840\n",
      "Student pre-training Epoch 813: avg loss = 0.5833\n",
      "Student pre-training Epoch 814: avg loss = 0.5848\n",
      "Student pre-training Epoch 815: avg loss = 0.5944\n",
      "Student pre-training Epoch 816: avg loss = 0.5832\n",
      "Student pre-training Epoch 817: avg loss = 0.5862\n",
      "Student pre-training Epoch 818: avg loss = 0.5744\n",
      "Student pre-training Epoch 819: avg loss = 0.5977\n",
      "Student pre-training Epoch 820: avg loss = 0.5827\n",
      "Student pre-training Epoch 821: avg loss = 0.5831\n",
      "Student pre-training Epoch 822: avg loss = 0.5805\n",
      "Student pre-training Epoch 823: avg loss = 0.5802\n",
      "Student pre-training Epoch 824: avg loss = 0.5851\n",
      "Student pre-training Epoch 825: avg loss = 0.5824\n",
      "Student pre-training Epoch 826: avg loss = 0.5899\n",
      "Student pre-training Epoch 827: avg loss = 0.5918\n",
      "Student pre-training Epoch 828: avg loss = 0.5777\n",
      "Student pre-training Epoch 829: avg loss = 0.5846\n",
      "Student pre-training Epoch 830: avg loss = 0.5853\n",
      "Student pre-training Epoch 831: avg loss = 0.5797\n",
      "Student pre-training Epoch 832: avg loss = 0.5803\n",
      "Student pre-training Epoch 833: avg loss = 0.5857\n",
      "Student pre-training Epoch 834: avg loss = 0.5807\n",
      "Student pre-training Epoch 835: avg loss = 0.5638\n",
      "Student pre-training Epoch 836: avg loss = 0.5888\n",
      "Student pre-training Epoch 837: avg loss = 0.5834\n",
      "Student pre-training Epoch 838: avg loss = 0.5843\n",
      "Student pre-training Epoch 839: avg loss = 0.5834\n",
      "Student pre-training Epoch 840: avg loss = 0.5726\n",
      "Student pre-training Epoch 841: avg loss = 0.5799\n",
      "Student pre-training Epoch 842: avg loss = 0.5858\n",
      "Student pre-training Epoch 843: avg loss = 0.5721\n",
      "Student pre-training Epoch 844: avg loss = 0.5781\n",
      "Student pre-training Epoch 845: avg loss = 0.5743\n",
      "Student pre-training Epoch 846: avg loss = 0.5897\n",
      "Student pre-training Epoch 847: avg loss = 0.5814\n",
      "Student pre-training Epoch 848: avg loss = 0.5753\n",
      "Student pre-training Epoch 849: avg loss = 0.5884\n",
      "Student pre-training Epoch 850: avg loss = 0.5691\n",
      "Student pre-training Epoch 851: avg loss = 0.5695\n",
      "Student pre-training Epoch 852: avg loss = 0.5643\n",
      "Student pre-training Epoch 853: avg loss = 0.5790\n",
      "Student pre-training Epoch 854: avg loss = 0.5774\n",
      "Student pre-training Epoch 855: avg loss = 0.5656\n",
      "Student pre-training Epoch 856: avg loss = 0.5704\n",
      "Student pre-training Epoch 857: avg loss = 0.5818\n",
      "Student pre-training Epoch 858: avg loss = 0.5880\n",
      "Student pre-training Epoch 859: avg loss = 0.5807\n",
      "Student pre-training Epoch 860: avg loss = 0.5808\n",
      "Student pre-training Epoch 861: avg loss = 0.5511\n",
      "Student pre-training Epoch 862: avg loss = 0.5713\n",
      "Student pre-training Epoch 863: avg loss = 0.5708\n",
      "Student pre-training Epoch 864: avg loss = 0.5771\n",
      "Student pre-training Epoch 865: avg loss = 0.5800\n",
      "Student pre-training Epoch 866: avg loss = 0.5792\n",
      "Student pre-training Epoch 867: avg loss = 0.5726\n",
      "Student pre-training Epoch 868: avg loss = 0.5587\n",
      "Student pre-training Epoch 869: avg loss = 0.5744\n",
      "Student pre-training Epoch 870: avg loss = 0.5610\n",
      "Student pre-training Epoch 871: avg loss = 0.5755\n",
      "Student pre-training Epoch 872: avg loss = 0.5749\n",
      "Student pre-training Epoch 873: avg loss = 0.5697\n",
      "Student pre-training Epoch 874: avg loss = 0.5663\n",
      "Student pre-training Epoch 875: avg loss = 0.5627\n",
      "Student pre-training Epoch 876: avg loss = 0.5715\n",
      "Student pre-training Epoch 877: avg loss = 0.5771\n",
      "Student pre-training Epoch 878: avg loss = 0.5656\n",
      "Student pre-training Epoch 879: avg loss = 0.5743\n",
      "Student pre-training Epoch 880: avg loss = 0.5791\n",
      "Student pre-training Epoch 881: avg loss = 0.5723\n",
      "Student pre-training Epoch 882: avg loss = 0.5596\n",
      "Student pre-training Epoch 883: avg loss = 0.5650\n",
      "Student pre-training Epoch 884: avg loss = 0.5650\n",
      "Student pre-training Epoch 885: avg loss = 0.5629\n",
      "Student pre-training Epoch 886: avg loss = 0.5654\n",
      "Student pre-training Epoch 887: avg loss = 0.5708\n",
      "Student pre-training Epoch 888: avg loss = 0.5697\n",
      "Student pre-training Epoch 889: avg loss = 0.5835\n",
      "Student pre-training Epoch 890: avg loss = 0.5445\n",
      "Student pre-training Epoch 891: avg loss = 0.5584\n",
      "Student pre-training Epoch 892: avg loss = 0.5697\n",
      "Student pre-training Epoch 893: avg loss = 0.5630\n",
      "Student pre-training Epoch 894: avg loss = 0.5779\n",
      "Student pre-training Epoch 895: avg loss = 0.5712\n",
      "Student pre-training Epoch 896: avg loss = 0.5677\n",
      "Student pre-training Epoch 897: avg loss = 0.5558\n",
      "Student pre-training Epoch 898: avg loss = 0.5530\n",
      "Student pre-training Epoch 899: avg loss = 0.5585\n",
      "Student pre-training Epoch 900: avg loss = 0.5656\n",
      "Student pre-training Epoch 901: avg loss = 0.5759\n",
      "Student pre-training Epoch 902: avg loss = 0.5705\n",
      "Student pre-training Epoch 903: avg loss = 0.5560\n",
      "Student pre-training Epoch 904: avg loss = 0.5688\n",
      "Student pre-training Epoch 905: avg loss = 0.5499\n",
      "Student pre-training Epoch 906: avg loss = 0.5614\n",
      "Student pre-training Epoch 907: avg loss = 0.5708\n",
      "Student pre-training Epoch 908: avg loss = 0.5741\n",
      "Student pre-training Epoch 909: avg loss = 0.5719\n",
      "Student pre-training Epoch 910: avg loss = 0.5702\n",
      "Student pre-training Epoch 911: avg loss = 0.5585\n",
      "Student pre-training Epoch 912: avg loss = 0.5720\n",
      "Student pre-training Epoch 913: avg loss = 0.5674\n",
      "Student pre-training Epoch 914: avg loss = 0.5682\n",
      "Student pre-training Epoch 915: avg loss = 0.5673\n",
      "Student pre-training Epoch 916: avg loss = 0.5563\n",
      "Student pre-training Epoch 917: avg loss = 0.5583\n",
      "Student pre-training Epoch 918: avg loss = 0.5505\n",
      "Student pre-training Epoch 919: avg loss = 0.5591\n",
      "Student pre-training Epoch 920: avg loss = 0.5513\n",
      "Student pre-training Epoch 921: avg loss = 0.5494\n",
      "Student pre-training Epoch 922: avg loss = 0.5615\n",
      "Student pre-training Epoch 923: avg loss = 0.5562\n",
      "Student pre-training Epoch 924: avg loss = 0.5666\n",
      "Student pre-training Epoch 925: avg loss = 0.5576\n",
      "Student pre-training Epoch 926: avg loss = 0.5490\n",
      "Student pre-training Epoch 927: avg loss = 0.5625\n",
      "Student pre-training Epoch 928: avg loss = 0.5633\n",
      "Student pre-training Epoch 929: avg loss = 0.5564\n",
      "Student pre-training Epoch 930: avg loss = 0.5576\n",
      "Student pre-training Epoch 931: avg loss = 0.5534\n",
      "Student pre-training Epoch 932: avg loss = 0.5636\n",
      "Student pre-training Epoch 933: avg loss = 0.5550\n",
      "Student pre-training Epoch 934: avg loss = 0.5658\n",
      "Student pre-training Epoch 935: avg loss = 0.5515\n",
      "Student pre-training Epoch 936: avg loss = 0.5373\n",
      "Student pre-training Epoch 937: avg loss = 0.5542\n",
      "Student pre-training Epoch 938: avg loss = 0.5592\n",
      "Student pre-training Epoch 939: avg loss = 0.5467\n",
      "Student pre-training Epoch 940: avg loss = 0.5475\n",
      "Student pre-training Epoch 941: avg loss = 0.5508\n",
      "Student pre-training Epoch 942: avg loss = 0.5431\n",
      "Student pre-training Epoch 943: avg loss = 0.5504\n",
      "Student pre-training Epoch 944: avg loss = 0.5406\n",
      "Student pre-training Epoch 945: avg loss = 0.5586\n",
      "Student pre-training Epoch 946: avg loss = 0.5560\n",
      "Student pre-training Epoch 947: avg loss = 0.5477\n",
      "Student pre-training Epoch 948: avg loss = 0.5550\n",
      "Student pre-training Epoch 949: avg loss = 0.5549\n",
      "Student pre-training Epoch 950: avg loss = 0.5419\n",
      "Student pre-training Epoch 951: avg loss = 0.5490\n",
      "Student pre-training Epoch 952: avg loss = 0.5487\n",
      "Student pre-training Epoch 953: avg loss = 0.5549\n",
      "Student pre-training Epoch 954: avg loss = 0.5446\n",
      "Student pre-training Epoch 955: avg loss = 0.5487\n",
      "Student pre-training Epoch 956: avg loss = 0.5494\n",
      "Student pre-training Epoch 957: avg loss = 0.5422\n",
      "Student pre-training Epoch 958: avg loss = 0.5458\n",
      "Student pre-training Epoch 959: avg loss = 0.5446\n",
      "Student pre-training Epoch 960: avg loss = 0.5596\n",
      "Student pre-training Epoch 961: avg loss = 0.5457\n",
      "Student pre-training Epoch 962: avg loss = 0.5520\n",
      "Student pre-training Epoch 963: avg loss = 0.5483\n",
      "Student pre-training Epoch 964: avg loss = 0.5455\n",
      "Student pre-training Epoch 965: avg loss = 0.5447\n",
      "Student pre-training Epoch 966: avg loss = 0.5507\n",
      "Student pre-training Epoch 967: avg loss = 0.5495\n",
      "Student pre-training Epoch 968: avg loss = 0.5457\n",
      "Student pre-training Epoch 969: avg loss = 0.5446\n",
      "Student pre-training Epoch 970: avg loss = 0.5412\n",
      "Student pre-training Epoch 971: avg loss = 0.5491\n",
      "Student pre-training Epoch 972: avg loss = 0.5471\n",
      "Student pre-training Epoch 973: avg loss = 0.5391\n",
      "Student pre-training Epoch 974: avg loss = 0.5500\n",
      "Student pre-training Epoch 975: avg loss = 0.5446\n",
      "Student pre-training Epoch 976: avg loss = 0.5498\n",
      "Student pre-training Epoch 977: avg loss = 0.5498\n",
      "Student pre-training Epoch 978: avg loss = 0.5379\n",
      "Student pre-training Epoch 979: avg loss = 0.5396\n",
      "Student pre-training Epoch 980: avg loss = 0.5596\n",
      "Student pre-training Epoch 981: avg loss = 0.5328\n",
      "Student pre-training Epoch 982: avg loss = 0.5437\n",
      "Student pre-training Epoch 983: avg loss = 0.5526\n",
      "Student pre-training Epoch 984: avg loss = 0.5306\n",
      "Student pre-training Epoch 985: avg loss = 0.5511\n",
      "Student pre-training Epoch 986: avg loss = 0.5367\n",
      "Student pre-training Epoch 987: avg loss = 0.5533\n",
      "Student pre-training Epoch 988: avg loss = 0.5378\n",
      "Student pre-training Epoch 989: avg loss = 0.5443\n",
      "Student pre-training Epoch 990: avg loss = 0.5415\n",
      "Student pre-training Epoch 991: avg loss = 0.5436\n",
      "Student pre-training Epoch 992: avg loss = 0.5383\n",
      "Student pre-training Epoch 993: avg loss = 0.5465\n",
      "Student pre-training Epoch 994: avg loss = 0.5470\n",
      "Student pre-training Epoch 995: avg loss = 0.5419\n",
      "Student pre-training Epoch 996: avg loss = 0.5389\n",
      "Student pre-training Epoch 997: avg loss = 0.5459\n",
      "Student pre-training Epoch 998: avg loss = 0.5320\n",
      "Student pre-training Epoch 999: avg loss = 0.5346\n",
      "Student pre-training Epoch 1000: avg loss = 0.5411\n"
     ]
    }
   ],
   "source": [
    "# Prepare DataLoader for augmented data\n",
    "# Convert to PyTorch tensors\n",
    "X_aug_tensor = torch.from_numpy(augmented_X).float()\n",
    "y_activity_aug_tensor = torch.from_numpy(augmented_y_activity).long()\n",
    "\n",
    "# For transformation labels, convert to one-hot encoding (each sample is one-hot for the applied transform)\n",
    "num_transforms = len(transform_funcs)\n",
    "y_transform_aug = np.zeros((len(augmented_y_transform), num_transforms), dtype=np.float32)\n",
    "y_transform_aug[np.arange(len(augmented_y_transform)), augmented_y_transform] = 1.0\n",
    "y_transform_aug_tensor = torch.from_numpy(y_transform_aug)\n",
    "\n",
    "aug_dataset = torch.utils.data.TensorDataset(X_aug_tensor, y_activity_aug_tensor, y_transform_aug_tensor)\n",
    "aug_loader = torch.utils.data.DataLoader(aug_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "student_model.to(device)\n",
    "\n",
    "# Student model multi-task training (pre-training)\n",
    "optimizer_student = optim.Adam(student_model.parameters(), lr=0.00003, weight_decay=1e-4)\n",
    "criterion_activity = nn.CrossEntropyLoss()\n",
    "# Use BCEWithLogitsLoss for transformation heads\n",
    "criterion_transform = nn.BCEWithLogitsLoss()\n",
    "\n",
    "student_model.train()\n",
    "num_aug_epochs = 1000\n",
    "for epoch in range(1, num_aug_epochs + 1):\n",
    "    total_loss = 0.0\n",
    "    for batch_X, batch_y_act, batch_y_trans in aug_loader:\n",
    "        optimizer_student.zero_grad()\n",
    "        logits_act, logits_trans = student_model(batch_X.to(device))\n",
    "        loss_act = criterion_activity(logits_act, batch_y_act.to(device))\n",
    "        loss_trans = criterion_transform(logits_trans, batch_y_trans.to(device))\n",
    "        loss = loss_act + loss_trans\n",
    "        loss.backward()\n",
    "        optimizer_student.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(aug_loader)\n",
    "    print(f\"Student pre-training Epoch {epoch}: avg loss = {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Fine-Tuning on True Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 1: Loss=0.2227, BalAcc=0.8736, F1_wtd=0.9260\n",
      "Fine-tune Epoch 2: Loss=0.1791, BalAcc=0.8938, F1_wtd=0.9378\n",
      "Fine-tune Epoch 3: Loss=0.1826, BalAcc=0.8965, F1_wtd=0.9385\n",
      "Fine-tune Epoch 4: Loss=0.1726, BalAcc=0.8941, F1_wtd=0.9391\n",
      "Fine-tune Epoch 5: Loss=0.1662, BalAcc=0.8919, F1_wtd=0.9413\n",
      "Fine-tune Epoch 6: Loss=0.1521, BalAcc=0.9073, F1_wtd=0.9518\n",
      "Fine-tune Epoch 7: Loss=0.1590, BalAcc=0.9034, F1_wtd=0.9465\n",
      "Fine-tune Epoch 8: Loss=0.1512, BalAcc=0.9098, F1_wtd=0.9517\n",
      "Fine-tune Epoch 9: Loss=0.1461, BalAcc=0.9093, F1_wtd=0.9525\n",
      "Fine-tune Epoch 10: Loss=0.1364, BalAcc=0.9148, F1_wtd=0.9539\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "\n",
    "# --- Freeze selected layers ---\n",
    "for param in student_model.conv1.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in student_model.conv2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# --- Optimizer for only trainable parameters ---\n",
    "optimizer_ft = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, student_model.parameters()),\n",
    "    lr=0.0001,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# --- Fine-tune loader ---\n",
    "train_dataset_ft = torch.utils.data.TensorDataset(X_train_t, y_train_t)\n",
    "train_loader_ft = torch.utils.data.DataLoader(train_dataset_ft, batch_size=256, shuffle=True)\n",
    "\n",
    "# --- Fine-tuning loop ---\n",
    "student_model = student_model.to(device)\n",
    "criterion_activity = nn.CrossEntropyLoss()\n",
    "student_model.train()\n",
    "\n",
    "num_ft_epochs = 10\n",
    "for epoch in range(1, num_ft_epochs + 1):\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch_X, batch_y in train_loader_ft:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        optimizer_ft.zero_grad()\n",
    "        logits_act, _ = student_model(batch_X)\n",
    "        loss = criterion_activity(logits_act, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer_ft.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_X.size(0)\n",
    "        preds = logits_act.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader_ft.dataset)\n",
    "    train_bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "    train_f1_wtd = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f\"Fine-tune Epoch {epoch}: \"\n",
    "          f\"Loss={avg_loss:.4f}, BalAcc={train_bal_acc:.4f}, F1_wtd={train_f1_wtd:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Standard Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Downstairs       0.76      0.67      0.71        52\n",
      "     Jogging       1.00      0.85      0.92       310\n",
      "     Sitting       0.84      1.00      0.91        59\n",
      "    Standing       0.79      0.90      0.84        41\n",
      "    Upstairs       0.54      0.78      0.64        65\n",
      "     Walking       0.96      0.98      0.97       333\n",
      "\n",
      "    accuracy                           0.90       860\n",
      "   macro avg       0.81      0.86      0.83       860\n",
      "weighted avg       0.91      0.90      0.90       860\n",
      "\n",
      "Balanced Accuracy: 0.8645457673978524\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "student_model.eval()\n",
    "X_test_tensor = torch.from_numpy(test_X).float().to(device)\n",
    "with torch.no_grad():\n",
    "    logits, _ = student_model(X_test_tensor)\n",
    "    preds = logits.argmax(dim=1).to('cpu').numpy()\n",
    "\n",
    "cm = confusion_matrix(test_y_idx, preds)\n",
    "cm = (cm.T / cm.sum(axis=1)).T\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(test_y_idx, preds, target_names=unique_classes))\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy_score(test_y_idx, preds))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, xticklabels=unique_classes, yticklabels=unique_classes, cmap=\"Blues\")\n",
    "plt.title(\"Student Model Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/SelfHAR_Performance.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Linear Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear evaluation accuracy: 0.9442\n"
     ]
    }
   ],
   "source": [
    "# Modify the student model to extract features (from the conv core output)\n",
    "def extract_features(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.from_numpy(X).float().to(device)\n",
    "        # Forward pass until the features are computed:\n",
    "        X_tensor = X_tensor.transpose(1, 2)\n",
    "        x = model.relu(model.conv1(X_tensor))\n",
    "        x = model.dropout(x)\n",
    "        x = model.relu(model.conv2(x))\n",
    "        x = model.dropout(x)\n",
    "        x = model.relu(model.conv3(x))\n",
    "        x = F.max_pool1d(x, kernel_size=x.shape[2])\n",
    "        features = x.squeeze(2)\n",
    "        return features.to('cpu').numpy()\n",
    "\n",
    "train_features = extract_features(student_model, train_labeled_X)\n",
    "test_features = extract_features(student_model, test_X)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(train_features, train_labeled_y_idx)\n",
    "linear_preds = clf.predict(test_features)\n",
    "linear_acc = np.mean(linear_preds == test_y_idx)\n",
    "print(f\"Linear evaluation accuracy: {linear_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smartyEnvmnt_PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
