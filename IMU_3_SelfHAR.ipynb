{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMU Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 1098209\n",
      "Train records: 898365\n",
      "Test records: 199844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2066530/2003044245.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[['X','Y','Z']] = (train_data[['X','Y','Z']] - mean_vals) / std_vals\n",
      "/tmp/ipykernel_2066530/2003044245.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data[['X','Y','Z']]  = (test_data[['X','Y','Z']]  - mean_vals) / std_vals\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labeled windows: 3891\n",
      "Train unlabeled windows: 597\n",
      "Test windows: 860\n",
      "Unique activity classes: ['Downstairs' 'Jogging' 'Sitting' 'Standing' 'Upstairs' 'Walking']\n",
      "Mapping: {'Downstairs': 0, 'Jogging': 1, 'Sitting': 2, 'Standing': 3, 'Upstairs': 4, 'Walking': 5}\n"
     ]
    }
   ],
   "source": [
    "# --- Parameters ---\n",
    "WINDOW_SIZE = 400\n",
    "STEP_SIZE = 200\n",
    "\n",
    "# --- Load All Data and Split Based on ID ---\n",
    "all_files = [\"data/IMU_case_dataset_part1_.csv\",\n",
    "             \"data/IMU_case_dataset_part2_.csv\",\n",
    "             \"data/IMU_case_dataset_part3_.csv\",\n",
    "             \"data/IMU_case_dataset_part4_.csv\"]\n",
    "\n",
    "dfs = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    # Keep the necessary columns: 'X', 'Y', 'Z', 'activity', and 'ID'\n",
    "    df = df[['X','Y','Z','activity','ID']]\n",
    "    dfs.append(df)\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Ensure that the 'ID' column is numeric\n",
    "data['ID'] = pd.to_numeric(data['ID'], errors='coerce')\n",
    "\n",
    "# Split the dataset:\n",
    "# Training data: IDs from 1 to 30\n",
    "# Testing data: IDs from 31 to 36\n",
    "train_data = data[data['ID'].between(1, 30)]\n",
    "test_data  = data[data['ID'].between(31, 36)]\n",
    "\n",
    "print(\"Total records:\", data.shape[0])\n",
    "print(\"Train records:\", train_data.shape[0])\n",
    "print(\"Test records:\", test_data.shape[0])\n",
    "\n",
    "# --- Normalization ---\n",
    "# Compute normalization parameters (mean and std) on the training set:\n",
    "mean_vals = train_data[['X','Y','Z']].mean()\n",
    "std_vals = train_data[['X','Y','Z']].std()\n",
    "\n",
    "# Apply normalization to both training and test data:\n",
    "train_data[['X','Y','Z']] = (train_data[['X','Y','Z']] - mean_vals) / std_vals\n",
    "test_data[['X','Y','Z']]  = (test_data[['X','Y','Z']]  - mean_vals) / std_vals\n",
    "\n",
    "# --- Windowing Function ---\n",
    "def generate_windows(df):\n",
    "    X_windows = []\n",
    "    y_labels = []\n",
    "    # Assuming the dataframe is sorted by time\n",
    "    for start in range(0, len(df) - WINDOW_SIZE + 1, STEP_SIZE):\n",
    "        window = df.iloc[start : start + WINDOW_SIZE]\n",
    "        labels = window[\"activity\"]\n",
    "        if labels.isna().all():\n",
    "            # Completely unlabeled window\n",
    "            X_windows.append(window[[\"X\", \"Y\", \"Z\"]].values)\n",
    "            y_labels.append(None)\n",
    "        else:\n",
    "            unique_labels = labels.dropna().unique()\n",
    "            if len(unique_labels) == 1:\n",
    "                X_windows.append(window[[\"X\", \"Y\", \"Z\"]].values)\n",
    "                y_labels.append(unique_labels[0])\n",
    "            else:\n",
    "                # Mixed or transitional window is treated as unlabeled\n",
    "                X_windows.append(window[[\"X\", \"Y\", \"Z\"]].values)\n",
    "                y_labels.append(None)\n",
    "    return X_windows, y_labels\n",
    "\n",
    "# --- Generate Windows from Training and Test Data ---\n",
    "# For training data:\n",
    "train_X_all, train_y_all = generate_windows(train_data)\n",
    "# Separate into labeled and unlabeled windows:\n",
    "train_labeled_X = [x for x, y in zip(train_X_all, train_y_all) if y is not None]\n",
    "train_labeled_y = [y for y in train_y_all if y is not None]\n",
    "train_unlabeled_X = [x for x, y in zip(train_X_all, train_y_all) if y is None]\n",
    "\n",
    "# Convert to numpy arrays:\n",
    "train_labeled_X = np.array(train_labeled_X)   # shape: (N_labeled, 400, 3)\n",
    "train_labeled_y = np.array(train_labeled_y)     # shape: (N_labeled,)\n",
    "train_unlabeled_X = np.array(train_unlabeled_X) # shape: (N_unlabeled, 400, 3)\n",
    "\n",
    "# Remove windows with any NaN features (if any)\n",
    "not_nan_idx = np.isnan(train_labeled_X).sum(axis=-1).sum(axis=-1) == 0\n",
    "train_labeled_X = train_labeled_X[not_nan_idx]\n",
    "train_labeled_y = train_labeled_y[not_nan_idx]\n",
    "\n",
    "not_nan_idx = np.isnan(train_unlabeled_X).sum(axis=-1).sum(axis=-1) == 0\n",
    "train_unlabeled_X = train_unlabeled_X[not_nan_idx]\n",
    "\n",
    "# For test data (only use windows with defined labels)\n",
    "test_X, test_y = generate_windows(test_data)\n",
    "test_X = np.array([x for x, y in zip(test_X, test_y) if y is not None])\n",
    "test_y = np.array([y for y in test_y if y is not None])\n",
    "\n",
    "print(\"Train labeled windows:\", len(train_labeled_y))\n",
    "print(\"Train unlabeled windows:\", train_unlabeled_X.shape[0])\n",
    "print(\"Test windows:\", len(test_y))\n",
    "\n",
    "# --- Label Encoding ---\n",
    "# Convert string labels to integer indices\n",
    "unique_classes = np.unique(train_labeled_y)\n",
    "print(\"Unique activity classes:\", unique_classes)\n",
    "class_to_idx = {label: idx for idx, label in enumerate(unique_classes)}\n",
    "print(\"Mapping:\", class_to_idx)\n",
    "\n",
    "train_labeled_y_idx = np.array([class_to_idx[label] for label in train_labeled_y])\n",
    "test_y_idx = np.array([class_to_idx[label] for label in test_y])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architecture: Teacher-Student TPN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPNModel(\n",
      "  (conv1): Conv1d(3, 32, kernel_size=(24,), stride=(1,))\n",
      "  (conv2): Conv1d(32, 64, kernel_size=(16,), stride=(1,))\n",
      "  (conv3): Conv1d(64, 96, kernel_size=(8,), stride=(1,))\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc_har1): Linear(in_features=96, out_features=1024, bias=True)\n",
      "  (fc_har2): Linear(in_features=1024, out_features=6, bias=True)\n",
      ")\n",
      "TPNModel(\n",
      "  (conv1): Conv1d(3, 32, kernel_size=(24,), stride=(1,))\n",
      "  (conv2): Conv1d(32, 64, kernel_size=(16,), stride=(1,))\n",
      "  (conv3): Conv1d(64, 96, kernel_size=(8,), stride=(1,))\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc_har1): Linear(in_features=96, out_features=1024, bias=True)\n",
      "  (fc_har2): Linear(in_features=1024, out_features=6, bias=True)\n",
      "  (transform_heads): ModuleList(\n",
      "    (0-7): 8 x Sequential(\n",
      "      (0): Linear(in_features=96, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TPNModel(nn.Module):\n",
    "    def __init__(self, num_classes, num_transforms=0):\n",
    "        super(TPNModel, self).__init__()\n",
    "        # Convolutional core: three conv layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=32, kernel_size=24, stride=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=16, stride=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=96, kernel_size=8, stride=1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        if num_classes > 0:\n",
    "            self.fc_har1 = nn.Linear(96, 1024)\n",
    "            self.fc_har2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "        self.num_transforms = num_transforms\n",
    "        if num_transforms > 0:\n",
    "            self.transform_heads = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(96, 256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, 1)\n",
    "                )\n",
    "                for _ in range(num_transforms)\n",
    "            ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, 400, 3) -> transpose to (batch, 3, 400)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        # Global max pooling over time dimension\n",
    "        x = F.max_pool1d(x, kernel_size=x.shape[2])\n",
    "        features = x.squeeze(2)  # shape: (batch, 96)\n",
    "\n",
    "        # Activity classification head\n",
    "        class_logits = None\n",
    "        if self.num_classes > 0:\n",
    "            h = self.relu(self.fc_har1(features))\n",
    "            class_logits = self.fc_har2(h)  # raw scores for each class\n",
    "\n",
    "        # Transformation discrimination heads\n",
    "        transform_logits = None\n",
    "        if self.num_transforms > 0:\n",
    "            logits_list = []\n",
    "            for head in self.transform_heads:\n",
    "                logit = head(features)   # shape: (batch, 1)\n",
    "                logits_list.append(logit.squeeze(-1))  # shape: (batch,)\n",
    "            transform_logits = torch.stack(logits_list, dim=1)  # shape: (batch, num_transforms)\n",
    "        return class_logits, transform_logits\n",
    "\n",
    "# Define the number of activity classes (from unique_classes) and transformations (8)\n",
    "num_classes = len(unique_classes)\n",
    "num_transforms = 8\n",
    "\n",
    "# Teacher: only classification head\n",
    "teacher_model = TPNModel(num_classes=num_classes, num_transforms=0)\n",
    "# Student: both classification and transformation heads\n",
    "student_model = TPNModel(num_classes=num_classes, num_transforms=num_transforms)\n",
    "\n",
    "print(teacher_model)\n",
    "print(student_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Teacher Model Training (Supervised Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1: Train Loss=1.4836, BalAcc=0.1575, F1_wtd=0.1723 | Val Loss=1.2834, BalAcc=0.3035, F1_wtd=0.6107\n",
      "Epoch 2: Train Loss=1.2119, BalAcc=0.2799, F1_wtd=0.5383 | Val Loss=1.3042, BalAcc=0.3118, F1_wtd=0.6215\n",
      "Epoch 3: Train Loss=1.0428, BalAcc=0.3049, F1_wtd=0.6018 | Val Loss=0.9758, BalAcc=0.3200, F1_wtd=0.6416\n",
      "Epoch 4: Train Loss=0.7921, BalAcc=0.3202, F1_wtd=0.6382 | Val Loss=0.7455, BalAcc=0.3271, F1_wtd=0.6534\n",
      "Epoch 5: Train Loss=0.6275, BalAcc=0.3283, F1_wtd=0.6556 | Val Loss=0.6343, BalAcc=0.3677, F1_wtd=0.6880\n",
      "Epoch 6: Train Loss=0.5633, BalAcc=0.4468, F1_wtd=0.7145 | Val Loss=0.5806, BalAcc=0.5914, F1_wtd=0.7732\n",
      "Epoch 7: Train Loss=0.5118, BalAcc=0.5431, F1_wtd=0.7625 | Val Loss=0.5349, BalAcc=0.6608, F1_wtd=0.8038\n",
      "Epoch 8: Train Loss=0.4525, BalAcc=0.6096, F1_wtd=0.7908 | Val Loss=0.4658, BalAcc=0.6759, F1_wtd=0.8118\n",
      "Epoch 9: Train Loss=0.4083, BalAcc=0.6521, F1_wtd=0.8145 | Val Loss=0.4097, BalAcc=0.6668, F1_wtd=0.8075\n",
      "Epoch 10: Train Loss=0.3722, BalAcc=0.7045, F1_wtd=0.8359 | Val Loss=0.3845, BalAcc=0.7422, F1_wtd=0.8558\n",
      "Epoch 11: Train Loss=0.3375, BalAcc=0.7373, F1_wtd=0.8564 | Val Loss=0.3320, BalAcc=0.7385, F1_wtd=0.8577\n",
      "Epoch 12: Train Loss=0.2982, BalAcc=0.7606, F1_wtd=0.8732 | Val Loss=0.2908, BalAcc=0.7843, F1_wtd=0.8872\n",
      "Epoch 13: Train Loss=0.2683, BalAcc=0.7790, F1_wtd=0.8857 | Val Loss=0.2719, BalAcc=0.7999, F1_wtd=0.8878\n",
      "Epoch 14: Train Loss=0.2415, BalAcc=0.8089, F1_wtd=0.9035 | Val Loss=0.2495, BalAcc=0.8228, F1_wtd=0.9093\n",
      "Epoch 15: Train Loss=0.2165, BalAcc=0.8274, F1_wtd=0.9152 | Val Loss=0.2254, BalAcc=0.8269, F1_wtd=0.9114\n",
      "Epoch 16: Train Loss=0.2158, BalAcc=0.8208, F1_wtd=0.9129 | Val Loss=0.2309, BalAcc=0.8328, F1_wtd=0.9126\n",
      "Epoch 17: Train Loss=0.1973, BalAcc=0.8332, F1_wtd=0.9178 | Val Loss=0.2212, BalAcc=0.8177, F1_wtd=0.9048\n",
      "Epoch 18: Train Loss=0.1947, BalAcc=0.8627, F1_wtd=0.9312 | Val Loss=0.2289, BalAcc=0.8203, F1_wtd=0.9017\n",
      "Epoch 19: Train Loss=0.1831, BalAcc=0.8451, F1_wtd=0.9248 | Val Loss=0.2153, BalAcc=0.8388, F1_wtd=0.9131\n",
      "Epoch 20: Train Loss=0.1644, BalAcc=0.8809, F1_wtd=0.9416 | Val Loss=0.2170, BalAcc=0.8703, F1_wtd=0.9288\n",
      "Epoch 21: Train Loss=0.1566, BalAcc=0.8832, F1_wtd=0.9426 | Val Loss=0.1676, BalAcc=0.8894, F1_wtd=0.9482\n",
      "Epoch 22: Train Loss=0.1490, BalAcc=0.8827, F1_wtd=0.9434 | Val Loss=0.2040, BalAcc=0.8912, F1_wtd=0.9437\n",
      "Epoch 23: Train Loss=0.1398, BalAcc=0.9056, F1_wtd=0.9546 | Val Loss=0.1698, BalAcc=0.8871, F1_wtd=0.9424\n",
      "Epoch 24: Train Loss=0.1479, BalAcc=0.8910, F1_wtd=0.9456 | Val Loss=0.1993, BalAcc=0.8909, F1_wtd=0.9437\n",
      "Epoch 25: Train Loss=0.1297, BalAcc=0.9077, F1_wtd=0.9545 | Val Loss=0.1431, BalAcc=0.9273, F1_wtd=0.9658\n",
      "Epoch 26: Train Loss=0.1295, BalAcc=0.9152, F1_wtd=0.9553 | Val Loss=0.1407, BalAcc=0.9122, F1_wtd=0.9576\n",
      "Epoch 27: Train Loss=0.1231, BalAcc=0.9082, F1_wtd=0.9532 | Val Loss=0.1394, BalAcc=0.9210, F1_wtd=0.9615\n",
      "Epoch 28: Train Loss=0.1224, BalAcc=0.9238, F1_wtd=0.9604 | Val Loss=0.1741, BalAcc=0.9199, F1_wtd=0.9543\n",
      "Epoch 29: Train Loss=0.1038, BalAcc=0.9309, F1_wtd=0.9650 | Val Loss=0.1446, BalAcc=0.9311, F1_wtd=0.9663\n",
      "Epoch 30: Train Loss=0.1025, BalAcc=0.9380, F1_wtd=0.9693 | Val Loss=0.1363, BalAcc=0.9348, F1_wtd=0.9663\n",
      "Epoch 31: Train Loss=0.0897, BalAcc=0.9421, F1_wtd=0.9699 | Val Loss=0.1742, BalAcc=0.9263, F1_wtd=0.9475\n",
      "Epoch 32: Train Loss=0.0959, BalAcc=0.9400, F1_wtd=0.9683 | Val Loss=0.1402, BalAcc=0.9289, F1_wtd=0.9660\n",
      "Epoch 33: Train Loss=0.0886, BalAcc=0.9421, F1_wtd=0.9713 | Val Loss=0.1309, BalAcc=0.9301, F1_wtd=0.9638\n",
      "Epoch 34: Train Loss=0.0871, BalAcc=0.9450, F1_wtd=0.9721 | Val Loss=0.1366, BalAcc=0.9428, F1_wtd=0.9719\n",
      "Epoch 35: Train Loss=0.0813, BalAcc=0.9437, F1_wtd=0.9711 | Val Loss=0.1102, BalAcc=0.9557, F1_wtd=0.9747\n",
      "Epoch 36: Train Loss=0.0724, BalAcc=0.9560, F1_wtd=0.9779 | Val Loss=0.1153, BalAcc=0.9537, F1_wtd=0.9698\n",
      "Epoch 37: Train Loss=0.0707, BalAcc=0.9520, F1_wtd=0.9756 | Val Loss=0.0999, BalAcc=0.9454, F1_wtd=0.9770\n",
      "Epoch 38: Train Loss=0.0713, BalAcc=0.9572, F1_wtd=0.9775 | Val Loss=0.1018, BalAcc=0.9385, F1_wtd=0.9743\n",
      "Epoch 39: Train Loss=0.0661, BalAcc=0.9621, F1_wtd=0.9795 | Val Loss=0.1328, BalAcc=0.9484, F1_wtd=0.9723\n",
      "Epoch 40: Train Loss=0.0648, BalAcc=0.9604, F1_wtd=0.9782 | Val Loss=0.1248, BalAcc=0.9441, F1_wtd=0.9742\n",
      "Epoch 41: Train Loss=0.0658, BalAcc=0.9627, F1_wtd=0.9796 | Val Loss=0.1238, BalAcc=0.9433, F1_wtd=0.9667\n",
      "Epoch 42: Train Loss=0.0707, BalAcc=0.9638, F1_wtd=0.9774 | Val Loss=0.1062, BalAcc=0.9513, F1_wtd=0.9767\n",
      "Epoch 43: Train Loss=0.0615, BalAcc=0.9675, F1_wtd=0.9816 | Val Loss=0.0810, BalAcc=0.9582, F1_wtd=0.9794\n",
      "Epoch 44: Train Loss=0.0553, BalAcc=0.9647, F1_wtd=0.9816 | Val Loss=0.1020, BalAcc=0.9475, F1_wtd=0.9698\n",
      "Epoch 45: Train Loss=0.0637, BalAcc=0.9599, F1_wtd=0.9770 | Val Loss=0.0862, BalAcc=0.9582, F1_wtd=0.9794\n",
      "Epoch 46: Train Loss=0.0514, BalAcc=0.9640, F1_wtd=0.9819 | Val Loss=0.1010, BalAcc=0.9550, F1_wtd=0.9721\n",
      "Epoch 47: Train Loss=0.0504, BalAcc=0.9706, F1_wtd=0.9837 | Val Loss=0.1203, BalAcc=0.9487, F1_wtd=0.9721\n",
      "Epoch 48: Train Loss=0.0524, BalAcc=0.9659, F1_wtd=0.9833 | Val Loss=0.1006, BalAcc=0.9422, F1_wtd=0.9746\n",
      "Epoch 49: Train Loss=0.0478, BalAcc=0.9713, F1_wtd=0.9848 | Val Loss=0.0937, BalAcc=0.9718, F1_wtd=0.9848\n",
      "Epoch 50: Train Loss=0.0490, BalAcc=0.9688, F1_wtd=0.9830 | Val Loss=0.0991, BalAcc=0.9711, F1_wtd=0.9821\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "\n",
    "# --- Device setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Train/Val split ---\n",
    "X_train_tensor = torch.from_numpy(train_labeled_X).float()\n",
    "y_train_tensor = torch.from_numpy(train_labeled_y_idx).long()\n",
    "\n",
    "X_train_t, X_val_t, y_train_t, y_val_t = train_test_split(\n",
    "    X_train_tensor, y_train_tensor, test_size=0.1, random_state=42, stratify=y_train_tensor\n",
    ")\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_t, y_train_t)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val_t, y_val_t)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# --- Model, loss, optimizer ---\n",
    "teacher_model = teacher_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(teacher_model.parameters(), lr=0.0003, weight_decay=1e-4)\n",
    "\n",
    "# --- Training loop ---\n",
    "num_epochs = 50\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    teacher_model.train()\n",
    "    total_train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = teacher_model(batch_X)\n",
    "        loss = criterion(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item() * batch_X.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    train_bal_acc = balanced_accuracy_score(train_labels, train_preds)\n",
    "    train_f1_wtd = f1_score(train_labels, train_preds, average='weighted')\n",
    "    avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "\n",
    "    # --- Validation ---\n",
    "    teacher_model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_X, val_y in val_loader:\n",
    "            val_X, val_y = val_X.to(device), val_y.to(device)\n",
    "            logits, _ = teacher_model(val_X)\n",
    "            loss = criterion(logits, val_y)\n",
    "\n",
    "            total_val_loss += loss.item() * val_X.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(val_y.cpu().numpy())\n",
    "\n",
    "    val_bal_acc = balanced_accuracy_score(val_labels, val_preds)\n",
    "    val_f1_wtd = f1_score(val_labels, val_preds, average='weighted')\n",
    "    avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch}: \"\n",
    "          f\"Train Loss={avg_train_loss:.4f}, BalAcc={train_bal_acc:.4f}, F1_wtd={train_f1_wtd:.4f} | \"\n",
    "          f\"Val Loss={avg_val_loss:.4f}, BalAcc={val_bal_acc:.4f}, F1_wtd={val_f1_wtd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher balanced model accuracy on test set: 0.8638\n",
      "Teacher model wtd f1 score on test set: 0.9071\n"
     ]
    }
   ],
   "source": [
    "teacher_model.eval()\n",
    "test_X_tensor = torch.from_numpy(test_X).float().to(device)\n",
    "test_y_tensor = torch.from_numpy(test_y_idx).long().to(device)\n",
    "with torch.no_grad():\n",
    "    logits, _ = teacher_model(test_X_tensor)\n",
    "    predictions = logits.argmax(dim=1)\n",
    "# test_accuracy = (predictions == test_y_tensor).sum().item() / len(test_y_tensor)\n",
    "test_accuracy = balanced_accuracy_score(test_y_tensor.to('cpu').numpy(), predictions.to('cpu').numpy())\n",
    "test_wtdf1 = f1_score(test_y_tensor.to('cpu').numpy(), predictions.to('cpu').numpy(), average='weighted')\n",
    "print(f\"Teacher balanced model accuracy on test set: {test_accuracy:.4f}\")\n",
    "print(f\"Teacher model wtd f1 score on test set: {test_wtdf1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Self-Labeling and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Pseudo-Labeling Unlabeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 574 out of 597 unlabeled samples based on confidence.\n",
      "Pseudo-labeled dataset size: 333\n"
     ]
    }
   ],
   "source": [
    "teacher_model.eval()\n",
    "X_unlabeled = torch.from_numpy(train_unlabeled_X).float().to(device)\n",
    "with torch.no_grad():\n",
    "    logits, _ = teacher_model(X_unlabeled)\n",
    "    probs = F.softmax(logits, dim=1).to('cpu').numpy()\n",
    "\n",
    "pred_classes = probs.argmax(axis=1)\n",
    "pred_confidences = probs.max(axis=1)\n",
    "\n",
    "# Confidence threshold\n",
    "confidence_threshold = 0.5\n",
    "selected_idx = np.where(pred_confidences >= confidence_threshold)[0]\n",
    "print(f\"Selected {len(selected_idx)} out of {train_unlabeled_X.shape[0]} unlabeled samples based on confidence.\")\n",
    "\n",
    "# Optionally balance classes (limit to top K per class)\n",
    "K = 100  # maximum samples per class\n",
    "selected_idx_balanced = []\n",
    "for c in range(num_classes):\n",
    "    class_idxs = selected_idx[pred_classes[selected_idx] == c]\n",
    "    if len(class_idxs) > K:\n",
    "        # sort by confidence and take top K\n",
    "        sorted_idxs = class_idxs[np.argsort(pred_confidences[class_idxs])][-K:]\n",
    "        selected_idx_balanced.extend(sorted_idxs)\n",
    "    else:\n",
    "        selected_idx_balanced.extend(class_idxs)\n",
    "selected_idx_balanced = np.array(selected_idx_balanced)\n",
    "\n",
    "pseudo_labeled_X = train_unlabeled_X[selected_idx_balanced]\n",
    "pseudo_labeled_y = pred_classes[selected_idx_balanced]\n",
    "\n",
    "print(\"Pseudo-labeled dataset size:\", pseudo_labeled_X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data Augmentation with Signal Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented dataset size: 2664\n"
     ]
    }
   ],
   "source": [
    "import math, random\n",
    "\n",
    "def add_noise(signal, sigma=0.1):\n",
    "    return signal + np.random.normal(scale=sigma, size=signal.shape)\n",
    "\n",
    "def scale_signal(signal, sigma=0.1):\n",
    "    factor = np.random.normal(loc=1.0, scale=sigma)\n",
    "    return signal * factor\n",
    "\n",
    "def rotate_signal(signal):\n",
    "    axis = np.random.normal(size=3)\n",
    "    axis = axis / np.linalg.norm(axis)\n",
    "    theta = np.random.uniform(0, 2*math.pi)\n",
    "    K = np.array([[0, -axis[2], axis[1]],\n",
    "                  [axis[2], 0, -axis[0]],\n",
    "                  [-axis[1], axis[0], 0]])\n",
    "    I = np.eye(3)\n",
    "    R = I + math.sin(theta)*K + (1 - math.cos(theta))*(K.dot(K))\n",
    "    return signal.dot(R.T)\n",
    "\n",
    "def invert_signal(signal):\n",
    "    return -signal\n",
    "\n",
    "def reverse_time(signal):\n",
    "    return signal[::-1, :]\n",
    "\n",
    "def scramble_segments(signal, num_segments=4):\n",
    "    seg_len = signal.shape[0] // num_segments\n",
    "    segments = [signal[i*seg_len : (i+1)*seg_len] for i in range(num_segments)]\n",
    "    random.shuffle(segments)\n",
    "    return np.concatenate(segments, axis=0)\n",
    "\n",
    "def shuffle_channels(signal):\n",
    "    perm = np.random.permutation(3)\n",
    "    return signal[:, perm]\n",
    "\n",
    "# List of transformation functions (we assume identity is one of the tasks)\n",
    "transform_funcs = [\n",
    "    add_noise, scale_signal, rotate_signal, invert_signal,\n",
    "    reverse_time, scramble_segments, shuffle_channels, lambda x: x\n",
    "    ]\n",
    "\n",
    "# Augment data: for each sample, apply all 8 transformations\n",
    "augmented_X = []\n",
    "augmented_y_activity = []   # same pseudo label for all augmentations\n",
    "augmented_y_transform = []  # index (0 to 7) indicating the transform\n",
    "for x, label in zip(pseudo_labeled_X, pseudo_labeled_y):\n",
    "    for t_idx, func in enumerate(transform_funcs):\n",
    "        augmented_X.append(func(x))\n",
    "        augmented_y_activity.append(label)\n",
    "        augmented_y_transform.append(t_idx)\n",
    "\n",
    "augmented_X = np.array(augmented_X)  # shape: (N_aug, 400, 3)\n",
    "augmented_y_activity = np.array(augmented_y_activity)\n",
    "augmented_y_transform = np.array(augmented_y_transform)\n",
    "\n",
    "print(\"Augmented dataset size:\", augmented_X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Student Model Training (Multi-task Pre-training + Fine-Tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Pre-Training with Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student pre-training Epoch 1: avg loss = 2.3878\n",
      "Student pre-training Epoch 2: avg loss = 2.2971\n",
      "Student pre-training Epoch 3: avg loss = 2.2131\n",
      "Student pre-training Epoch 4: avg loss = 2.1409\n",
      "Student pre-training Epoch 5: avg loss = 2.0799\n",
      "Student pre-training Epoch 6: avg loss = 2.0280\n",
      "Student pre-training Epoch 7: avg loss = 1.9818\n",
      "Student pre-training Epoch 8: avg loss = 1.9462\n",
      "Student pre-training Epoch 9: avg loss = 1.9272\n",
      "Student pre-training Epoch 10: avg loss = 1.9076\n",
      "Student pre-training Epoch 11: avg loss = 1.8967\n",
      "Student pre-training Epoch 12: avg loss = 1.8826\n",
      "Student pre-training Epoch 13: avg loss = 1.8783\n",
      "Student pre-training Epoch 14: avg loss = 1.8685\n",
      "Student pre-training Epoch 15: avg loss = 1.8642\n",
      "Student pre-training Epoch 16: avg loss = 1.8612\n",
      "Student pre-training Epoch 17: avg loss = 1.8514\n",
      "Student pre-training Epoch 18: avg loss = 1.8437\n",
      "Student pre-training Epoch 19: avg loss = 1.8439\n",
      "Student pre-training Epoch 20: avg loss = 1.8336\n",
      "Student pre-training Epoch 21: avg loss = 1.8252\n",
      "Student pre-training Epoch 22: avg loss = 1.8228\n",
      "Student pre-training Epoch 23: avg loss = 1.8149\n",
      "Student pre-training Epoch 24: avg loss = 1.8012\n",
      "Student pre-training Epoch 25: avg loss = 1.7942\n",
      "Student pre-training Epoch 26: avg loss = 1.7833\n",
      "Student pre-training Epoch 27: avg loss = 1.7701\n",
      "Student pre-training Epoch 28: avg loss = 1.7562\n",
      "Student pre-training Epoch 29: avg loss = 1.7495\n",
      "Student pre-training Epoch 30: avg loss = 1.7340\n",
      "Student pre-training Epoch 31: avg loss = 1.7136\n",
      "Student pre-training Epoch 32: avg loss = 1.6990\n",
      "Student pre-training Epoch 33: avg loss = 1.6862\n",
      "Student pre-training Epoch 34: avg loss = 1.6652\n",
      "Student pre-training Epoch 35: avg loss = 1.6423\n",
      "Student pre-training Epoch 36: avg loss = 1.6361\n",
      "Student pre-training Epoch 37: avg loss = 1.6150\n",
      "Student pre-training Epoch 38: avg loss = 1.5982\n",
      "Student pre-training Epoch 39: avg loss = 1.5803\n",
      "Student pre-training Epoch 40: avg loss = 1.5639\n",
      "Student pre-training Epoch 41: avg loss = 1.5431\n",
      "Student pre-training Epoch 42: avg loss = 1.5367\n",
      "Student pre-training Epoch 43: avg loss = 1.5172\n",
      "Student pre-training Epoch 44: avg loss = 1.5087\n",
      "Student pre-training Epoch 45: avg loss = 1.4956\n",
      "Student pre-training Epoch 46: avg loss = 1.4782\n",
      "Student pre-training Epoch 47: avg loss = 1.4694\n",
      "Student pre-training Epoch 48: avg loss = 1.4577\n",
      "Student pre-training Epoch 49: avg loss = 1.4512\n",
      "Student pre-training Epoch 50: avg loss = 1.4380\n",
      "Student pre-training Epoch 51: avg loss = 1.4307\n",
      "Student pre-training Epoch 52: avg loss = 1.4299\n",
      "Student pre-training Epoch 53: avg loss = 1.4181\n",
      "Student pre-training Epoch 54: avg loss = 1.4077\n",
      "Student pre-training Epoch 55: avg loss = 1.3942\n",
      "Student pre-training Epoch 56: avg loss = 1.4008\n",
      "Student pre-training Epoch 57: avg loss = 1.3809\n",
      "Student pre-training Epoch 58: avg loss = 1.3774\n",
      "Student pre-training Epoch 59: avg loss = 1.3792\n",
      "Student pre-training Epoch 60: avg loss = 1.3716\n",
      "Student pre-training Epoch 61: avg loss = 1.3713\n",
      "Student pre-training Epoch 62: avg loss = 1.3584\n",
      "Student pre-training Epoch 63: avg loss = 1.3486\n",
      "Student pre-training Epoch 64: avg loss = 1.3563\n",
      "Student pre-training Epoch 65: avg loss = 1.3441\n",
      "Student pre-training Epoch 66: avg loss = 1.3389\n",
      "Student pre-training Epoch 67: avg loss = 1.3353\n",
      "Student pre-training Epoch 68: avg loss = 1.3371\n",
      "Student pre-training Epoch 69: avg loss = 1.3262\n",
      "Student pre-training Epoch 70: avg loss = 1.3294\n",
      "Student pre-training Epoch 71: avg loss = 1.3192\n",
      "Student pre-training Epoch 72: avg loss = 1.3250\n",
      "Student pre-training Epoch 73: avg loss = 1.3272\n",
      "Student pre-training Epoch 74: avg loss = 1.3141\n",
      "Student pre-training Epoch 75: avg loss = 1.3182\n",
      "Student pre-training Epoch 76: avg loss = 1.3019\n",
      "Student pre-training Epoch 77: avg loss = 1.3064\n",
      "Student pre-training Epoch 78: avg loss = 1.2955\n",
      "Student pre-training Epoch 79: avg loss = 1.3000\n",
      "Student pre-training Epoch 80: avg loss = 1.2907\n",
      "Student pre-training Epoch 81: avg loss = 1.2955\n",
      "Student pre-training Epoch 82: avg loss = 1.2922\n",
      "Student pre-training Epoch 83: avg loss = 1.2848\n",
      "Student pre-training Epoch 84: avg loss = 1.2883\n",
      "Student pre-training Epoch 85: avg loss = 1.2789\n",
      "Student pre-training Epoch 86: avg loss = 1.2759\n",
      "Student pre-training Epoch 87: avg loss = 1.2820\n",
      "Student pre-training Epoch 88: avg loss = 1.2796\n",
      "Student pre-training Epoch 89: avg loss = 1.2757\n",
      "Student pre-training Epoch 90: avg loss = 1.2648\n",
      "Student pre-training Epoch 91: avg loss = 1.2682\n",
      "Student pre-training Epoch 92: avg loss = 1.2768\n",
      "Student pre-training Epoch 93: avg loss = 1.2674\n",
      "Student pre-training Epoch 94: avg loss = 1.2509\n",
      "Student pre-training Epoch 95: avg loss = 1.2520\n",
      "Student pre-training Epoch 96: avg loss = 1.2435\n",
      "Student pre-training Epoch 97: avg loss = 1.2513\n",
      "Student pre-training Epoch 98: avg loss = 1.2454\n",
      "Student pre-training Epoch 99: avg loss = 1.2524\n",
      "Student pre-training Epoch 100: avg loss = 1.2447\n",
      "Student pre-training Epoch 101: avg loss = 1.2346\n",
      "Student pre-training Epoch 102: avg loss = 1.2380\n",
      "Student pre-training Epoch 103: avg loss = 1.2394\n",
      "Student pre-training Epoch 104: avg loss = 1.2328\n",
      "Student pre-training Epoch 105: avg loss = 1.2284\n",
      "Student pre-training Epoch 106: avg loss = 1.2240\n",
      "Student pre-training Epoch 107: avg loss = 1.2168\n",
      "Student pre-training Epoch 108: avg loss = 1.2231\n",
      "Student pre-training Epoch 109: avg loss = 1.2101\n",
      "Student pre-training Epoch 110: avg loss = 1.2186\n",
      "Student pre-training Epoch 111: avg loss = 1.2058\n",
      "Student pre-training Epoch 112: avg loss = 1.2066\n",
      "Student pre-training Epoch 113: avg loss = 1.2066\n",
      "Student pre-training Epoch 114: avg loss = 1.2019\n",
      "Student pre-training Epoch 115: avg loss = 1.2000\n",
      "Student pre-training Epoch 116: avg loss = 1.2018\n",
      "Student pre-training Epoch 117: avg loss = 1.1836\n",
      "Student pre-training Epoch 118: avg loss = 1.1822\n",
      "Student pre-training Epoch 119: avg loss = 1.1778\n",
      "Student pre-training Epoch 120: avg loss = 1.1864\n",
      "Student pre-training Epoch 121: avg loss = 1.1800\n",
      "Student pre-training Epoch 122: avg loss = 1.1726\n",
      "Student pre-training Epoch 123: avg loss = 1.1692\n",
      "Student pre-training Epoch 124: avg loss = 1.1684\n",
      "Student pre-training Epoch 125: avg loss = 1.1604\n",
      "Student pre-training Epoch 126: avg loss = 1.1661\n",
      "Student pre-training Epoch 127: avg loss = 1.1528\n",
      "Student pre-training Epoch 128: avg loss = 1.1468\n",
      "Student pre-training Epoch 129: avg loss = 1.1486\n",
      "Student pre-training Epoch 130: avg loss = 1.1438\n",
      "Student pre-training Epoch 131: avg loss = 1.1540\n",
      "Student pre-training Epoch 132: avg loss = 1.1334\n",
      "Student pre-training Epoch 133: avg loss = 1.1408\n",
      "Student pre-training Epoch 134: avg loss = 1.1385\n",
      "Student pre-training Epoch 135: avg loss = 1.1283\n",
      "Student pre-training Epoch 136: avg loss = 1.1283\n",
      "Student pre-training Epoch 137: avg loss = 1.1305\n",
      "Student pre-training Epoch 138: avg loss = 1.1325\n",
      "Student pre-training Epoch 139: avg loss = 1.1067\n",
      "Student pre-training Epoch 140: avg loss = 1.1150\n",
      "Student pre-training Epoch 141: avg loss = 1.1126\n",
      "Student pre-training Epoch 142: avg loss = 1.1060\n",
      "Student pre-training Epoch 143: avg loss = 1.1043\n",
      "Student pre-training Epoch 144: avg loss = 1.1043\n",
      "Student pre-training Epoch 145: avg loss = 1.0965\n",
      "Student pre-training Epoch 146: avg loss = 1.0962\n",
      "Student pre-training Epoch 147: avg loss = 1.0842\n",
      "Student pre-training Epoch 148: avg loss = 1.0758\n",
      "Student pre-training Epoch 149: avg loss = 1.0703\n",
      "Student pre-training Epoch 150: avg loss = 1.0769\n",
      "Student pre-training Epoch 151: avg loss = 1.0641\n",
      "Student pre-training Epoch 152: avg loss = 1.0757\n",
      "Student pre-training Epoch 153: avg loss = 1.0655\n",
      "Student pre-training Epoch 154: avg loss = 1.0650\n",
      "Student pre-training Epoch 155: avg loss = 1.0631\n",
      "Student pre-training Epoch 156: avg loss = 1.0603\n",
      "Student pre-training Epoch 157: avg loss = 1.0552\n",
      "Student pre-training Epoch 158: avg loss = 1.0466\n",
      "Student pre-training Epoch 159: avg loss = 1.0384\n",
      "Student pre-training Epoch 160: avg loss = 1.0494\n",
      "Student pre-training Epoch 161: avg loss = 1.0488\n",
      "Student pre-training Epoch 162: avg loss = 1.0344\n",
      "Student pre-training Epoch 163: avg loss = 1.0224\n",
      "Student pre-training Epoch 164: avg loss = 1.0257\n",
      "Student pre-training Epoch 165: avg loss = 1.0237\n",
      "Student pre-training Epoch 166: avg loss = 1.0228\n",
      "Student pre-training Epoch 167: avg loss = 1.0123\n",
      "Student pre-training Epoch 168: avg loss = 1.0254\n",
      "Student pre-training Epoch 169: avg loss = 1.0231\n",
      "Student pre-training Epoch 170: avg loss = 1.0092\n",
      "Student pre-training Epoch 171: avg loss = 1.0068\n",
      "Student pre-training Epoch 172: avg loss = 1.0300\n",
      "Student pre-training Epoch 173: avg loss = 1.0023\n",
      "Student pre-training Epoch 174: avg loss = 1.0050\n",
      "Student pre-training Epoch 175: avg loss = 0.9846\n",
      "Student pre-training Epoch 176: avg loss = 0.9981\n",
      "Student pre-training Epoch 177: avg loss = 0.9971\n",
      "Student pre-training Epoch 178: avg loss = 0.9948\n",
      "Student pre-training Epoch 179: avg loss = 0.9989\n",
      "Student pre-training Epoch 180: avg loss = 0.9929\n",
      "Student pre-training Epoch 181: avg loss = 0.9926\n",
      "Student pre-training Epoch 182: avg loss = 0.9916\n",
      "Student pre-training Epoch 183: avg loss = 0.9819\n",
      "Student pre-training Epoch 184: avg loss = 0.9779\n",
      "Student pre-training Epoch 185: avg loss = 0.9885\n",
      "Student pre-training Epoch 186: avg loss = 0.9840\n",
      "Student pre-training Epoch 187: avg loss = 0.9759\n",
      "Student pre-training Epoch 188: avg loss = 0.9636\n",
      "Student pre-training Epoch 189: avg loss = 0.9624\n",
      "Student pre-training Epoch 190: avg loss = 0.9697\n",
      "Student pre-training Epoch 191: avg loss = 0.9628\n",
      "Student pre-training Epoch 192: avg loss = 0.9647\n",
      "Student pre-training Epoch 193: avg loss = 0.9742\n",
      "Student pre-training Epoch 194: avg loss = 0.9750\n",
      "Student pre-training Epoch 195: avg loss = 0.9545\n",
      "Student pre-training Epoch 196: avg loss = 0.9397\n",
      "Student pre-training Epoch 197: avg loss = 0.9657\n",
      "Student pre-training Epoch 198: avg loss = 0.9576\n",
      "Student pre-training Epoch 199: avg loss = 0.9465\n",
      "Student pre-training Epoch 200: avg loss = 0.9489\n",
      "Student pre-training Epoch 201: avg loss = 0.9461\n",
      "Student pre-training Epoch 202: avg loss = 0.9391\n",
      "Student pre-training Epoch 203: avg loss = 0.9418\n",
      "Student pre-training Epoch 204: avg loss = 0.9650\n",
      "Student pre-training Epoch 205: avg loss = 0.9365\n",
      "Student pre-training Epoch 206: avg loss = 0.9410\n",
      "Student pre-training Epoch 207: avg loss = 0.9283\n",
      "Student pre-training Epoch 208: avg loss = 0.9166\n",
      "Student pre-training Epoch 209: avg loss = 0.9439\n",
      "Student pre-training Epoch 210: avg loss = 0.9318\n",
      "Student pre-training Epoch 211: avg loss = 0.9206\n",
      "Student pre-training Epoch 212: avg loss = 0.9153\n",
      "Student pre-training Epoch 213: avg loss = 0.9229\n",
      "Student pre-training Epoch 214: avg loss = 0.9150\n",
      "Student pre-training Epoch 215: avg loss = 0.9261\n",
      "Student pre-training Epoch 216: avg loss = 0.9187\n",
      "Student pre-training Epoch 217: avg loss = 0.9204\n",
      "Student pre-training Epoch 218: avg loss = 0.9207\n",
      "Student pre-training Epoch 219: avg loss = 0.9204\n",
      "Student pre-training Epoch 220: avg loss = 0.9008\n",
      "Student pre-training Epoch 221: avg loss = 0.9154\n",
      "Student pre-training Epoch 222: avg loss = 0.9165\n",
      "Student pre-training Epoch 223: avg loss = 0.9109\n",
      "Student pre-training Epoch 224: avg loss = 0.9061\n",
      "Student pre-training Epoch 225: avg loss = 0.9156\n",
      "Student pre-training Epoch 226: avg loss = 0.9069\n",
      "Student pre-training Epoch 227: avg loss = 0.9068\n",
      "Student pre-training Epoch 228: avg loss = 0.9135\n",
      "Student pre-training Epoch 229: avg loss = 0.9128\n",
      "Student pre-training Epoch 230: avg loss = 0.9228\n",
      "Student pre-training Epoch 231: avg loss = 0.9083\n",
      "Student pre-training Epoch 232: avg loss = 0.9070\n",
      "Student pre-training Epoch 233: avg loss = 0.9004\n",
      "Student pre-training Epoch 234: avg loss = 0.9007\n",
      "Student pre-training Epoch 235: avg loss = 0.8957\n",
      "Student pre-training Epoch 236: avg loss = 0.8780\n",
      "Student pre-training Epoch 237: avg loss = 0.8886\n",
      "Student pre-training Epoch 238: avg loss = 0.9069\n",
      "Student pre-training Epoch 239: avg loss = 0.8968\n",
      "Student pre-training Epoch 240: avg loss = 0.9015\n",
      "Student pre-training Epoch 241: avg loss = 0.8884\n",
      "Student pre-training Epoch 242: avg loss = 0.8982\n",
      "Student pre-training Epoch 243: avg loss = 0.9071\n",
      "Student pre-training Epoch 244: avg loss = 0.8927\n",
      "Student pre-training Epoch 245: avg loss = 0.8945\n",
      "Student pre-training Epoch 246: avg loss = 0.8887\n",
      "Student pre-training Epoch 247: avg loss = 0.8678\n",
      "Student pre-training Epoch 248: avg loss = 0.8803\n",
      "Student pre-training Epoch 249: avg loss = 0.8877\n",
      "Student pre-training Epoch 250: avg loss = 0.8678\n",
      "Student pre-training Epoch 251: avg loss = 0.8820\n",
      "Student pre-training Epoch 252: avg loss = 0.8878\n",
      "Student pre-training Epoch 253: avg loss = 0.8804\n",
      "Student pre-training Epoch 254: avg loss = 0.8747\n",
      "Student pre-training Epoch 255: avg loss = 0.8859\n",
      "Student pre-training Epoch 256: avg loss = 0.8819\n",
      "Student pre-training Epoch 257: avg loss = 0.8795\n",
      "Student pre-training Epoch 258: avg loss = 0.8662\n",
      "Student pre-training Epoch 259: avg loss = 0.8725\n",
      "Student pre-training Epoch 260: avg loss = 0.8728\n",
      "Student pre-training Epoch 261: avg loss = 0.8622\n",
      "Student pre-training Epoch 262: avg loss = 0.8683\n",
      "Student pre-training Epoch 263: avg loss = 0.8792\n",
      "Student pre-training Epoch 264: avg loss = 0.8735\n",
      "Student pre-training Epoch 265: avg loss = 0.8692\n",
      "Student pre-training Epoch 266: avg loss = 0.8568\n",
      "Student pre-training Epoch 267: avg loss = 0.8683\n",
      "Student pre-training Epoch 268: avg loss = 0.8676\n",
      "Student pre-training Epoch 269: avg loss = 0.8595\n",
      "Student pre-training Epoch 270: avg loss = 0.8639\n",
      "Student pre-training Epoch 271: avg loss = 0.8628\n",
      "Student pre-training Epoch 272: avg loss = 0.8824\n",
      "Student pre-training Epoch 273: avg loss = 0.8520\n",
      "Student pre-training Epoch 274: avg loss = 0.8615\n",
      "Student pre-training Epoch 275: avg loss = 0.8652\n",
      "Student pre-training Epoch 276: avg loss = 0.8482\n",
      "Student pre-training Epoch 277: avg loss = 0.8679\n",
      "Student pre-training Epoch 278: avg loss = 0.8686\n",
      "Student pre-training Epoch 279: avg loss = 0.8599\n",
      "Student pre-training Epoch 280: avg loss = 0.8628\n",
      "Student pre-training Epoch 281: avg loss = 0.8630\n",
      "Student pre-training Epoch 282: avg loss = 0.8482\n",
      "Student pre-training Epoch 283: avg loss = 0.8409\n",
      "Student pre-training Epoch 284: avg loss = 0.8505\n",
      "Student pre-training Epoch 285: avg loss = 0.8660\n",
      "Student pre-training Epoch 286: avg loss = 0.8465\n",
      "Student pre-training Epoch 287: avg loss = 0.8580\n",
      "Student pre-training Epoch 288: avg loss = 0.8736\n",
      "Student pre-training Epoch 289: avg loss = 0.8436\n",
      "Student pre-training Epoch 290: avg loss = 0.8375\n",
      "Student pre-training Epoch 291: avg loss = 0.8554\n",
      "Student pre-training Epoch 292: avg loss = 0.8531\n",
      "Student pre-training Epoch 293: avg loss = 0.8379\n",
      "Student pre-training Epoch 294: avg loss = 0.8414\n",
      "Student pre-training Epoch 295: avg loss = 0.8480\n",
      "Student pre-training Epoch 296: avg loss = 0.8545\n",
      "Student pre-training Epoch 297: avg loss = 0.8353\n",
      "Student pre-training Epoch 298: avg loss = 0.8376\n",
      "Student pre-training Epoch 299: avg loss = 0.8512\n",
      "Student pre-training Epoch 300: avg loss = 0.8315\n",
      "Student pre-training Epoch 301: avg loss = 0.8468\n",
      "Student pre-training Epoch 302: avg loss = 0.8291\n",
      "Student pre-training Epoch 303: avg loss = 0.8333\n",
      "Student pre-training Epoch 304: avg loss = 0.8401\n",
      "Student pre-training Epoch 305: avg loss = 0.8420\n",
      "Student pre-training Epoch 306: avg loss = 0.8242\n",
      "Student pre-training Epoch 307: avg loss = 0.8254\n",
      "Student pre-training Epoch 308: avg loss = 0.8346\n",
      "Student pre-training Epoch 309: avg loss = 0.8131\n",
      "Student pre-training Epoch 310: avg loss = 0.8434\n",
      "Student pre-training Epoch 311: avg loss = 0.8340\n",
      "Student pre-training Epoch 312: avg loss = 0.8213\n",
      "Student pre-training Epoch 313: avg loss = 0.8364\n",
      "Student pre-training Epoch 314: avg loss = 0.8308\n",
      "Student pre-training Epoch 315: avg loss = 0.8265\n",
      "Student pre-training Epoch 316: avg loss = 0.8295\n",
      "Student pre-training Epoch 317: avg loss = 0.8333\n",
      "Student pre-training Epoch 318: avg loss = 0.8383\n",
      "Student pre-training Epoch 319: avg loss = 0.8348\n",
      "Student pre-training Epoch 320: avg loss = 0.8395\n",
      "Student pre-training Epoch 321: avg loss = 0.8236\n",
      "Student pre-training Epoch 322: avg loss = 0.8443\n",
      "Student pre-training Epoch 323: avg loss = 0.8233\n",
      "Student pre-training Epoch 324: avg loss = 0.8270\n",
      "Student pre-training Epoch 325: avg loss = 0.8181\n",
      "Student pre-training Epoch 326: avg loss = 0.8180\n",
      "Student pre-training Epoch 327: avg loss = 0.8354\n",
      "Student pre-training Epoch 328: avg loss = 0.8289\n",
      "Student pre-training Epoch 329: avg loss = 0.8174\n",
      "Student pre-training Epoch 330: avg loss = 0.8183\n",
      "Student pre-training Epoch 331: avg loss = 0.8336\n",
      "Student pre-training Epoch 332: avg loss = 0.8264\n",
      "Student pre-training Epoch 333: avg loss = 0.8152\n",
      "Student pre-training Epoch 334: avg loss = 0.8262\n",
      "Student pre-training Epoch 335: avg loss = 0.8312\n",
      "Student pre-training Epoch 336: avg loss = 0.8347\n",
      "Student pre-training Epoch 337: avg loss = 0.8224\n",
      "Student pre-training Epoch 338: avg loss = 0.8078\n",
      "Student pre-training Epoch 339: avg loss = 0.8321\n",
      "Student pre-training Epoch 340: avg loss = 0.8202\n",
      "Student pre-training Epoch 341: avg loss = 0.8155\n",
      "Student pre-training Epoch 342: avg loss = 0.8075\n",
      "Student pre-training Epoch 343: avg loss = 0.8050\n",
      "Student pre-training Epoch 344: avg loss = 0.8216\n",
      "Student pre-training Epoch 345: avg loss = 0.8082\n",
      "Student pre-training Epoch 346: avg loss = 0.8244\n",
      "Student pre-training Epoch 347: avg loss = 0.8196\n",
      "Student pre-training Epoch 348: avg loss = 0.7982\n",
      "Student pre-training Epoch 349: avg loss = 0.8135\n",
      "Student pre-training Epoch 350: avg loss = 0.7973\n",
      "Student pre-training Epoch 351: avg loss = 0.8068\n",
      "Student pre-training Epoch 352: avg loss = 0.7881\n",
      "Student pre-training Epoch 353: avg loss = 0.8214\n",
      "Student pre-training Epoch 354: avg loss = 0.8072\n",
      "Student pre-training Epoch 355: avg loss = 0.8027\n",
      "Student pre-training Epoch 356: avg loss = 0.8020\n",
      "Student pre-training Epoch 357: avg loss = 0.8185\n",
      "Student pre-training Epoch 358: avg loss = 0.8088\n",
      "Student pre-training Epoch 359: avg loss = 0.8081\n",
      "Student pre-training Epoch 360: avg loss = 0.7959\n",
      "Student pre-training Epoch 361: avg loss = 0.7836\n",
      "Student pre-training Epoch 362: avg loss = 0.8068\n",
      "Student pre-training Epoch 363: avg loss = 0.7983\n",
      "Student pre-training Epoch 364: avg loss = 0.7995\n",
      "Student pre-training Epoch 365: avg loss = 0.7987\n",
      "Student pre-training Epoch 366: avg loss = 0.8064\n",
      "Student pre-training Epoch 367: avg loss = 0.8119\n",
      "Student pre-training Epoch 368: avg loss = 0.8063\n",
      "Student pre-training Epoch 369: avg loss = 0.8016\n",
      "Student pre-training Epoch 370: avg loss = 0.8059\n",
      "Student pre-training Epoch 371: avg loss = 0.7848\n",
      "Student pre-training Epoch 372: avg loss = 0.7814\n",
      "Student pre-training Epoch 373: avg loss = 0.8030\n",
      "Student pre-training Epoch 374: avg loss = 0.8044\n",
      "Student pre-training Epoch 375: avg loss = 0.8027\n",
      "Student pre-training Epoch 376: avg loss = 0.7975\n",
      "Student pre-training Epoch 377: avg loss = 0.7892\n",
      "Student pre-training Epoch 378: avg loss = 0.7932\n",
      "Student pre-training Epoch 379: avg loss = 0.7910\n",
      "Student pre-training Epoch 380: avg loss = 0.8020\n",
      "Student pre-training Epoch 381: avg loss = 0.7902\n",
      "Student pre-training Epoch 382: avg loss = 0.7924\n",
      "Student pre-training Epoch 383: avg loss = 0.7895\n",
      "Student pre-training Epoch 384: avg loss = 0.7806\n",
      "Student pre-training Epoch 385: avg loss = 0.7905\n",
      "Student pre-training Epoch 386: avg loss = 0.7867\n",
      "Student pre-training Epoch 387: avg loss = 0.7830\n",
      "Student pre-training Epoch 388: avg loss = 0.7949\n",
      "Student pre-training Epoch 389: avg loss = 0.7993\n",
      "Student pre-training Epoch 390: avg loss = 0.7826\n",
      "Student pre-training Epoch 391: avg loss = 0.7942\n",
      "Student pre-training Epoch 392: avg loss = 0.7964\n",
      "Student pre-training Epoch 393: avg loss = 0.7735\n",
      "Student pre-training Epoch 394: avg loss = 0.7932\n",
      "Student pre-training Epoch 395: avg loss = 0.7856\n",
      "Student pre-training Epoch 396: avg loss = 0.7793\n",
      "Student pre-training Epoch 397: avg loss = 0.7806\n",
      "Student pre-training Epoch 398: avg loss = 0.7926\n",
      "Student pre-training Epoch 399: avg loss = 0.7852\n",
      "Student pre-training Epoch 400: avg loss = 0.7765\n",
      "Student pre-training Epoch 401: avg loss = 0.8030\n",
      "Student pre-training Epoch 402: avg loss = 0.7897\n",
      "Student pre-training Epoch 403: avg loss = 0.7924\n",
      "Student pre-training Epoch 404: avg loss = 0.7723\n",
      "Student pre-training Epoch 405: avg loss = 0.7892\n",
      "Student pre-training Epoch 406: avg loss = 0.7705\n",
      "Student pre-training Epoch 407: avg loss = 0.7795\n",
      "Student pre-training Epoch 408: avg loss = 0.7906\n",
      "Student pre-training Epoch 409: avg loss = 0.7880\n",
      "Student pre-training Epoch 410: avg loss = 0.7770\n",
      "Student pre-training Epoch 411: avg loss = 0.7694\n",
      "Student pre-training Epoch 412: avg loss = 0.7611\n",
      "Student pre-training Epoch 413: avg loss = 0.7688\n",
      "Student pre-training Epoch 414: avg loss = 0.7802\n",
      "Student pre-training Epoch 415: avg loss = 0.7685\n",
      "Student pre-training Epoch 416: avg loss = 0.7816\n",
      "Student pre-training Epoch 417: avg loss = 0.7722\n",
      "Student pre-training Epoch 418: avg loss = 0.7856\n",
      "Student pre-training Epoch 419: avg loss = 0.7750\n",
      "Student pre-training Epoch 420: avg loss = 0.7608\n",
      "Student pre-training Epoch 421: avg loss = 0.7751\n",
      "Student pre-training Epoch 422: avg loss = 0.7524\n",
      "Student pre-training Epoch 423: avg loss = 0.7790\n",
      "Student pre-training Epoch 424: avg loss = 0.7773\n",
      "Student pre-training Epoch 425: avg loss = 0.7702\n",
      "Student pre-training Epoch 426: avg loss = 0.7820\n",
      "Student pre-training Epoch 427: avg loss = 0.7742\n",
      "Student pre-training Epoch 428: avg loss = 0.7768\n",
      "Student pre-training Epoch 429: avg loss = 0.7598\n",
      "Student pre-training Epoch 430: avg loss = 0.7611\n",
      "Student pre-training Epoch 431: avg loss = 0.7573\n",
      "Student pre-training Epoch 432: avg loss = 0.7790\n",
      "Student pre-training Epoch 433: avg loss = 0.7624\n",
      "Student pre-training Epoch 434: avg loss = 0.7487\n",
      "Student pre-training Epoch 435: avg loss = 0.7610\n",
      "Student pre-training Epoch 436: avg loss = 0.7694\n",
      "Student pre-training Epoch 437: avg loss = 0.7654\n",
      "Student pre-training Epoch 438: avg loss = 0.7779\n",
      "Student pre-training Epoch 439: avg loss = 0.7666\n",
      "Student pre-training Epoch 440: avg loss = 0.7579\n",
      "Student pre-training Epoch 441: avg loss = 0.7569\n",
      "Student pre-training Epoch 442: avg loss = 0.7665\n",
      "Student pre-training Epoch 443: avg loss = 0.7606\n",
      "Student pre-training Epoch 444: avg loss = 0.7586\n",
      "Student pre-training Epoch 445: avg loss = 0.7512\n",
      "Student pre-training Epoch 446: avg loss = 0.7619\n",
      "Student pre-training Epoch 447: avg loss = 0.7623\n",
      "Student pre-training Epoch 448: avg loss = 0.7669\n",
      "Student pre-training Epoch 449: avg loss = 0.7663\n",
      "Student pre-training Epoch 450: avg loss = 0.7697\n",
      "Student pre-training Epoch 451: avg loss = 0.7562\n",
      "Student pre-training Epoch 452: avg loss = 0.7739\n",
      "Student pre-training Epoch 453: avg loss = 0.7535\n",
      "Student pre-training Epoch 454: avg loss = 0.7695\n",
      "Student pre-training Epoch 455: avg loss = 0.7561\n",
      "Student pre-training Epoch 456: avg loss = 0.7523\n",
      "Student pre-training Epoch 457: avg loss = 0.7649\n",
      "Student pre-training Epoch 458: avg loss = 0.7537\n",
      "Student pre-training Epoch 459: avg loss = 0.7330\n",
      "Student pre-training Epoch 460: avg loss = 0.7499\n",
      "Student pre-training Epoch 461: avg loss = 0.7438\n",
      "Student pre-training Epoch 462: avg loss = 0.7549\n",
      "Student pre-training Epoch 463: avg loss = 0.7619\n",
      "Student pre-training Epoch 464: avg loss = 0.7581\n",
      "Student pre-training Epoch 465: avg loss = 0.7593\n",
      "Student pre-training Epoch 466: avg loss = 0.7425\n",
      "Student pre-training Epoch 467: avg loss = 0.7504\n",
      "Student pre-training Epoch 468: avg loss = 0.7516\n",
      "Student pre-training Epoch 469: avg loss = 0.7483\n",
      "Student pre-training Epoch 470: avg loss = 0.7597\n",
      "Student pre-training Epoch 471: avg loss = 0.7267\n",
      "Student pre-training Epoch 472: avg loss = 0.7296\n",
      "Student pre-training Epoch 473: avg loss = 0.7498\n",
      "Student pre-training Epoch 474: avg loss = 0.7611\n",
      "Student pre-training Epoch 475: avg loss = 0.7469\n",
      "Student pre-training Epoch 476: avg loss = 0.7643\n",
      "Student pre-training Epoch 477: avg loss = 0.7361\n",
      "Student pre-training Epoch 478: avg loss = 0.7463\n",
      "Student pre-training Epoch 479: avg loss = 0.7567\n",
      "Student pre-training Epoch 480: avg loss = 0.7394\n",
      "Student pre-training Epoch 481: avg loss = 0.7424\n",
      "Student pre-training Epoch 482: avg loss = 0.7495\n",
      "Student pre-training Epoch 483: avg loss = 0.7462\n",
      "Student pre-training Epoch 484: avg loss = 0.7362\n",
      "Student pre-training Epoch 485: avg loss = 0.7297\n",
      "Student pre-training Epoch 486: avg loss = 0.7470\n",
      "Student pre-training Epoch 487: avg loss = 0.7482\n",
      "Student pre-training Epoch 488: avg loss = 0.7321\n",
      "Student pre-training Epoch 489: avg loss = 0.7248\n",
      "Student pre-training Epoch 490: avg loss = 0.7406\n",
      "Student pre-training Epoch 491: avg loss = 0.7422\n",
      "Student pre-training Epoch 492: avg loss = 0.7404\n",
      "Student pre-training Epoch 493: avg loss = 0.7412\n",
      "Student pre-training Epoch 494: avg loss = 0.7361\n",
      "Student pre-training Epoch 495: avg loss = 0.7438\n",
      "Student pre-training Epoch 496: avg loss = 0.7383\n",
      "Student pre-training Epoch 497: avg loss = 0.7342\n",
      "Student pre-training Epoch 498: avg loss = 0.7380\n",
      "Student pre-training Epoch 499: avg loss = 0.7290\n",
      "Student pre-training Epoch 500: avg loss = 0.7239\n",
      "Student pre-training Epoch 501: avg loss = 0.7367\n",
      "Student pre-training Epoch 502: avg loss = 0.7279\n",
      "Student pre-training Epoch 503: avg loss = 0.7173\n",
      "Student pre-training Epoch 504: avg loss = 0.7401\n",
      "Student pre-training Epoch 505: avg loss = 0.7305\n",
      "Student pre-training Epoch 506: avg loss = 0.7327\n",
      "Student pre-training Epoch 507: avg loss = 0.7250\n",
      "Student pre-training Epoch 508: avg loss = 0.7281\n",
      "Student pre-training Epoch 509: avg loss = 0.7284\n",
      "Student pre-training Epoch 510: avg loss = 0.7346\n",
      "Student pre-training Epoch 511: avg loss = 0.7116\n",
      "Student pre-training Epoch 512: avg loss = 0.7306\n",
      "Student pre-training Epoch 513: avg loss = 0.7317\n",
      "Student pre-training Epoch 514: avg loss = 0.7373\n",
      "Student pre-training Epoch 515: avg loss = 0.7390\n",
      "Student pre-training Epoch 516: avg loss = 0.7370\n",
      "Student pre-training Epoch 517: avg loss = 0.7298\n",
      "Student pre-training Epoch 518: avg loss = 0.7333\n",
      "Student pre-training Epoch 519: avg loss = 0.7247\n",
      "Student pre-training Epoch 520: avg loss = 0.7378\n",
      "Student pre-training Epoch 521: avg loss = 0.7296\n",
      "Student pre-training Epoch 522: avg loss = 0.7197\n",
      "Student pre-training Epoch 523: avg loss = 0.7200\n",
      "Student pre-training Epoch 524: avg loss = 0.7307\n",
      "Student pre-training Epoch 525: avg loss = 0.7241\n",
      "Student pre-training Epoch 526: avg loss = 0.7205\n",
      "Student pre-training Epoch 527: avg loss = 0.7082\n",
      "Student pre-training Epoch 528: avg loss = 0.7188\n",
      "Student pre-training Epoch 529: avg loss = 0.7217\n",
      "Student pre-training Epoch 530: avg loss = 0.7269\n",
      "Student pre-training Epoch 531: avg loss = 0.7210\n",
      "Student pre-training Epoch 532: avg loss = 0.7261\n",
      "Student pre-training Epoch 533: avg loss = 0.7060\n",
      "Student pre-training Epoch 534: avg loss = 0.7239\n",
      "Student pre-training Epoch 535: avg loss = 0.7087\n",
      "Student pre-training Epoch 536: avg loss = 0.7195\n",
      "Student pre-training Epoch 537: avg loss = 0.7288\n",
      "Student pre-training Epoch 538: avg loss = 0.7074\n",
      "Student pre-training Epoch 539: avg loss = 0.7134\n",
      "Student pre-training Epoch 540: avg loss = 0.7001\n",
      "Student pre-training Epoch 541: avg loss = 0.7125\n",
      "Student pre-training Epoch 542: avg loss = 0.6940\n",
      "Student pre-training Epoch 543: avg loss = 0.6911\n",
      "Student pre-training Epoch 544: avg loss = 0.7162\n",
      "Student pre-training Epoch 545: avg loss = 0.7061\n",
      "Student pre-training Epoch 546: avg loss = 0.7202\n",
      "Student pre-training Epoch 547: avg loss = 0.7140\n",
      "Student pre-training Epoch 548: avg loss = 0.7127\n",
      "Student pre-training Epoch 549: avg loss = 0.7088\n",
      "Student pre-training Epoch 550: avg loss = 0.7077\n",
      "Student pre-training Epoch 551: avg loss = 0.7099\n",
      "Student pre-training Epoch 552: avg loss = 0.7063\n",
      "Student pre-training Epoch 553: avg loss = 0.7024\n",
      "Student pre-training Epoch 554: avg loss = 0.6913\n",
      "Student pre-training Epoch 555: avg loss = 0.7087\n",
      "Student pre-training Epoch 556: avg loss = 0.7036\n",
      "Student pre-training Epoch 557: avg loss = 0.7008\n",
      "Student pre-training Epoch 558: avg loss = 0.7213\n",
      "Student pre-training Epoch 559: avg loss = 0.7130\n",
      "Student pre-training Epoch 560: avg loss = 0.7091\n",
      "Student pre-training Epoch 561: avg loss = 0.6944\n",
      "Student pre-training Epoch 562: avg loss = 0.7079\n",
      "Student pre-training Epoch 563: avg loss = 0.7100\n",
      "Student pre-training Epoch 564: avg loss = 0.7059\n",
      "Student pre-training Epoch 565: avg loss = 0.7147\n",
      "Student pre-training Epoch 566: avg loss = 0.6984\n",
      "Student pre-training Epoch 567: avg loss = 0.7131\n",
      "Student pre-training Epoch 568: avg loss = 0.7056\n",
      "Student pre-training Epoch 569: avg loss = 0.6895\n",
      "Student pre-training Epoch 570: avg loss = 0.7005\n",
      "Student pre-training Epoch 571: avg loss = 0.7092\n",
      "Student pre-training Epoch 572: avg loss = 0.6917\n",
      "Student pre-training Epoch 573: avg loss = 0.7125\n",
      "Student pre-training Epoch 574: avg loss = 0.6929\n",
      "Student pre-training Epoch 575: avg loss = 0.6926\n",
      "Student pre-training Epoch 576: avg loss = 0.6931\n",
      "Student pre-training Epoch 577: avg loss = 0.6892\n",
      "Student pre-training Epoch 578: avg loss = 0.6922\n",
      "Student pre-training Epoch 579: avg loss = 0.6970\n",
      "Student pre-training Epoch 580: avg loss = 0.6884\n",
      "Student pre-training Epoch 581: avg loss = 0.7102\n",
      "Student pre-training Epoch 582: avg loss = 0.6924\n",
      "Student pre-training Epoch 583: avg loss = 0.6792\n",
      "Student pre-training Epoch 584: avg loss = 0.7021\n",
      "Student pre-training Epoch 585: avg loss = 0.6832\n",
      "Student pre-training Epoch 586: avg loss = 0.7050\n",
      "Student pre-training Epoch 587: avg loss = 0.7095\n",
      "Student pre-training Epoch 588: avg loss = 0.6936\n",
      "Student pre-training Epoch 589: avg loss = 0.6825\n",
      "Student pre-training Epoch 590: avg loss = 0.6831\n",
      "Student pre-training Epoch 591: avg loss = 0.6941\n",
      "Student pre-training Epoch 592: avg loss = 0.6853\n",
      "Student pre-training Epoch 593: avg loss = 0.6973\n",
      "Student pre-training Epoch 594: avg loss = 0.6913\n",
      "Student pre-training Epoch 595: avg loss = 0.6776\n",
      "Student pre-training Epoch 596: avg loss = 0.6818\n",
      "Student pre-training Epoch 597: avg loss = 0.7003\n",
      "Student pre-training Epoch 598: avg loss = 0.6805\n",
      "Student pre-training Epoch 599: avg loss = 0.6812\n",
      "Student pre-training Epoch 600: avg loss = 0.6790\n",
      "Student pre-training Epoch 601: avg loss = 0.6737\n",
      "Student pre-training Epoch 602: avg loss = 0.6799\n",
      "Student pre-training Epoch 603: avg loss = 0.6940\n",
      "Student pre-training Epoch 604: avg loss = 0.6841\n",
      "Student pre-training Epoch 605: avg loss = 0.6914\n",
      "Student pre-training Epoch 606: avg loss = 0.6789\n",
      "Student pre-training Epoch 607: avg loss = 0.6706\n",
      "Student pre-training Epoch 608: avg loss = 0.6809\n",
      "Student pre-training Epoch 609: avg loss = 0.6833\n",
      "Student pre-training Epoch 610: avg loss = 0.6806\n",
      "Student pre-training Epoch 611: avg loss = 0.6728\n",
      "Student pre-training Epoch 612: avg loss = 0.6705\n",
      "Student pre-training Epoch 613: avg loss = 0.6656\n",
      "Student pre-training Epoch 614: avg loss = 0.6736\n",
      "Student pre-training Epoch 615: avg loss = 0.6913\n",
      "Student pre-training Epoch 616: avg loss = 0.6854\n",
      "Student pre-training Epoch 617: avg loss = 0.6786\n",
      "Student pre-training Epoch 618: avg loss = 0.6669\n",
      "Student pre-training Epoch 619: avg loss = 0.6767\n",
      "Student pre-training Epoch 620: avg loss = 0.6770\n",
      "Student pre-training Epoch 621: avg loss = 0.6701\n",
      "Student pre-training Epoch 622: avg loss = 0.6687\n",
      "Student pre-training Epoch 623: avg loss = 0.6886\n",
      "Student pre-training Epoch 624: avg loss = 0.6822\n",
      "Student pre-training Epoch 625: avg loss = 0.6661\n",
      "Student pre-training Epoch 626: avg loss = 0.6996\n",
      "Student pre-training Epoch 627: avg loss = 0.6726\n",
      "Student pre-training Epoch 628: avg loss = 0.6752\n",
      "Student pre-training Epoch 629: avg loss = 0.6762\n",
      "Student pre-training Epoch 630: avg loss = 0.6645\n",
      "Student pre-training Epoch 631: avg loss = 0.6779\n",
      "Student pre-training Epoch 632: avg loss = 0.6754\n",
      "Student pre-training Epoch 633: avg loss = 0.6570\n",
      "Student pre-training Epoch 634: avg loss = 0.6584\n",
      "Student pre-training Epoch 635: avg loss = 0.6724\n",
      "Student pre-training Epoch 636: avg loss = 0.6591\n",
      "Student pre-training Epoch 637: avg loss = 0.6721\n",
      "Student pre-training Epoch 638: avg loss = 0.6684\n",
      "Student pre-training Epoch 639: avg loss = 0.6650\n",
      "Student pre-training Epoch 640: avg loss = 0.6725\n",
      "Student pre-training Epoch 641: avg loss = 0.6625\n",
      "Student pre-training Epoch 642: avg loss = 0.6520\n",
      "Student pre-training Epoch 643: avg loss = 0.6612\n",
      "Student pre-training Epoch 644: avg loss = 0.6683\n",
      "Student pre-training Epoch 645: avg loss = 0.6647\n",
      "Student pre-training Epoch 646: avg loss = 0.6565\n",
      "Student pre-training Epoch 647: avg loss = 0.6702\n",
      "Student pre-training Epoch 648: avg loss = 0.6720\n",
      "Student pre-training Epoch 649: avg loss = 0.6652\n",
      "Student pre-training Epoch 650: avg loss = 0.6732\n",
      "Student pre-training Epoch 651: avg loss = 0.6488\n",
      "Student pre-training Epoch 652: avg loss = 0.6683\n",
      "Student pre-training Epoch 653: avg loss = 0.6609\n",
      "Student pre-training Epoch 654: avg loss = 0.6463\n",
      "Student pre-training Epoch 655: avg loss = 0.6602\n",
      "Student pre-training Epoch 656: avg loss = 0.6696\n",
      "Student pre-training Epoch 657: avg loss = 0.6643\n",
      "Student pre-training Epoch 658: avg loss = 0.6621\n",
      "Student pre-training Epoch 659: avg loss = 0.6604\n",
      "Student pre-training Epoch 660: avg loss = 0.6564\n",
      "Student pre-training Epoch 661: avg loss = 0.6409\n",
      "Student pre-training Epoch 662: avg loss = 0.6626\n",
      "Student pre-training Epoch 663: avg loss = 0.6407\n",
      "Student pre-training Epoch 664: avg loss = 0.6474\n",
      "Student pre-training Epoch 665: avg loss = 0.6502\n",
      "Student pre-training Epoch 666: avg loss = 0.6475\n",
      "Student pre-training Epoch 667: avg loss = 0.6494\n",
      "Student pre-training Epoch 668: avg loss = 0.6715\n",
      "Student pre-training Epoch 669: avg loss = 0.6667\n",
      "Student pre-training Epoch 670: avg loss = 0.6448\n",
      "Student pre-training Epoch 671: avg loss = 0.6560\n",
      "Student pre-training Epoch 672: avg loss = 0.6544\n",
      "Student pre-training Epoch 673: avg loss = 0.6604\n",
      "Student pre-training Epoch 674: avg loss = 0.6422\n",
      "Student pre-training Epoch 675: avg loss = 0.6509\n",
      "Student pre-training Epoch 676: avg loss = 0.6674\n",
      "Student pre-training Epoch 677: avg loss = 0.6590\n",
      "Student pre-training Epoch 678: avg loss = 0.6586\n",
      "Student pre-training Epoch 679: avg loss = 0.6523\n",
      "Student pre-training Epoch 680: avg loss = 0.6453\n",
      "Student pre-training Epoch 681: avg loss = 0.6548\n",
      "Student pre-training Epoch 682: avg loss = 0.6585\n",
      "Student pre-training Epoch 683: avg loss = 0.6423\n",
      "Student pre-training Epoch 684: avg loss = 0.6602\n",
      "Student pre-training Epoch 685: avg loss = 0.6458\n",
      "Student pre-training Epoch 686: avg loss = 0.6555\n",
      "Student pre-training Epoch 687: avg loss = 0.6640\n",
      "Student pre-training Epoch 688: avg loss = 0.6374\n",
      "Student pre-training Epoch 689: avg loss = 0.6590\n",
      "Student pre-training Epoch 690: avg loss = 0.6464\n",
      "Student pre-training Epoch 691: avg loss = 0.6518\n",
      "Student pre-training Epoch 692: avg loss = 0.6526\n",
      "Student pre-training Epoch 693: avg loss = 0.6550\n",
      "Student pre-training Epoch 694: avg loss = 0.6364\n",
      "Student pre-training Epoch 695: avg loss = 0.6491\n",
      "Student pre-training Epoch 696: avg loss = 0.6281\n",
      "Student pre-training Epoch 697: avg loss = 0.6391\n",
      "Student pre-training Epoch 698: avg loss = 0.6447\n",
      "Student pre-training Epoch 699: avg loss = 0.6483\n",
      "Student pre-training Epoch 700: avg loss = 0.6302\n",
      "Student pre-training Epoch 701: avg loss = 0.6452\n",
      "Student pre-training Epoch 702: avg loss = 0.6448\n",
      "Student pre-training Epoch 703: avg loss = 0.6294\n",
      "Student pre-training Epoch 704: avg loss = 0.6453\n",
      "Student pre-training Epoch 705: avg loss = 0.6461\n",
      "Student pre-training Epoch 706: avg loss = 0.6422\n",
      "Student pre-training Epoch 707: avg loss = 0.6479\n",
      "Student pre-training Epoch 708: avg loss = 0.6401\n",
      "Student pre-training Epoch 709: avg loss = 0.6323\n",
      "Student pre-training Epoch 710: avg loss = 0.6406\n",
      "Student pre-training Epoch 711: avg loss = 0.6233\n",
      "Student pre-training Epoch 712: avg loss = 0.6361\n",
      "Student pre-training Epoch 713: avg loss = 0.6364\n",
      "Student pre-training Epoch 714: avg loss = 0.6428\n",
      "Student pre-training Epoch 715: avg loss = 0.6343\n",
      "Student pre-training Epoch 716: avg loss = 0.6276\n",
      "Student pre-training Epoch 717: avg loss = 0.6504\n",
      "Student pre-training Epoch 718: avg loss = 0.6255\n",
      "Student pre-training Epoch 719: avg loss = 0.6390\n",
      "Student pre-training Epoch 720: avg loss = 0.6360\n",
      "Student pre-training Epoch 721: avg loss = 0.6347\n",
      "Student pre-training Epoch 722: avg loss = 0.6289\n",
      "Student pre-training Epoch 723: avg loss = 0.6336\n",
      "Student pre-training Epoch 724: avg loss = 0.6327\n",
      "Student pre-training Epoch 725: avg loss = 0.6336\n",
      "Student pre-training Epoch 726: avg loss = 0.6237\n",
      "Student pre-training Epoch 727: avg loss = 0.6188\n",
      "Student pre-training Epoch 728: avg loss = 0.6292\n",
      "Student pre-training Epoch 729: avg loss = 0.6230\n",
      "Student pre-training Epoch 730: avg loss = 0.6342\n",
      "Student pre-training Epoch 731: avg loss = 0.6358\n",
      "Student pre-training Epoch 732: avg loss = 0.6326\n",
      "Student pre-training Epoch 733: avg loss = 0.6293\n",
      "Student pre-training Epoch 734: avg loss = 0.6377\n",
      "Student pre-training Epoch 735: avg loss = 0.6243\n",
      "Student pre-training Epoch 736: avg loss = 0.6376\n",
      "Student pre-training Epoch 737: avg loss = 0.6181\n",
      "Student pre-training Epoch 738: avg loss = 0.6272\n",
      "Student pre-training Epoch 739: avg loss = 0.6198\n",
      "Student pre-training Epoch 740: avg loss = 0.6284\n",
      "Student pre-training Epoch 741: avg loss = 0.6183\n",
      "Student pre-training Epoch 742: avg loss = 0.6242\n",
      "Student pre-training Epoch 743: avg loss = 0.6272\n",
      "Student pre-training Epoch 744: avg loss = 0.6260\n",
      "Student pre-training Epoch 745: avg loss = 0.6322\n",
      "Student pre-training Epoch 746: avg loss = 0.6355\n",
      "Student pre-training Epoch 747: avg loss = 0.6313\n",
      "Student pre-training Epoch 748: avg loss = 0.6229\n",
      "Student pre-training Epoch 749: avg loss = 0.6262\n",
      "Student pre-training Epoch 750: avg loss = 0.6237\n",
      "Student pre-training Epoch 751: avg loss = 0.6174\n",
      "Student pre-training Epoch 752: avg loss = 0.6233\n",
      "Student pre-training Epoch 753: avg loss = 0.6171\n",
      "Student pre-training Epoch 754: avg loss = 0.6307\n",
      "Student pre-training Epoch 755: avg loss = 0.6136\n",
      "Student pre-training Epoch 756: avg loss = 0.6111\n",
      "Student pre-training Epoch 757: avg loss = 0.6231\n",
      "Student pre-training Epoch 758: avg loss = 0.6144\n",
      "Student pre-training Epoch 759: avg loss = 0.6156\n",
      "Student pre-training Epoch 760: avg loss = 0.6107\n",
      "Student pre-training Epoch 761: avg loss = 0.6119\n",
      "Student pre-training Epoch 762: avg loss = 0.6191\n",
      "Student pre-training Epoch 763: avg loss = 0.6118\n",
      "Student pre-training Epoch 764: avg loss = 0.6226\n",
      "Student pre-training Epoch 765: avg loss = 0.6077\n",
      "Student pre-training Epoch 766: avg loss = 0.6235\n",
      "Student pre-training Epoch 767: avg loss = 0.6224\n",
      "Student pre-training Epoch 768: avg loss = 0.6099\n",
      "Student pre-training Epoch 769: avg loss = 0.6079\n",
      "Student pre-training Epoch 770: avg loss = 0.6198\n",
      "Student pre-training Epoch 771: avg loss = 0.6254\n",
      "Student pre-training Epoch 772: avg loss = 0.6183\n",
      "Student pre-training Epoch 773: avg loss = 0.6055\n",
      "Student pre-training Epoch 774: avg loss = 0.6156\n",
      "Student pre-training Epoch 775: avg loss = 0.6212\n",
      "Student pre-training Epoch 776: avg loss = 0.6237\n",
      "Student pre-training Epoch 777: avg loss = 0.5974\n",
      "Student pre-training Epoch 778: avg loss = 0.5967\n",
      "Student pre-training Epoch 779: avg loss = 0.6160\n",
      "Student pre-training Epoch 780: avg loss = 0.6126\n",
      "Student pre-training Epoch 781: avg loss = 0.6308\n",
      "Student pre-training Epoch 782: avg loss = 0.6151\n",
      "Student pre-training Epoch 783: avg loss = 0.6179\n",
      "Student pre-training Epoch 784: avg loss = 0.6109\n",
      "Student pre-training Epoch 785: avg loss = 0.6146\n",
      "Student pre-training Epoch 786: avg loss = 0.6061\n",
      "Student pre-training Epoch 787: avg loss = 0.6298\n",
      "Student pre-training Epoch 788: avg loss = 0.6125\n",
      "Student pre-training Epoch 789: avg loss = 0.6093\n",
      "Student pre-training Epoch 790: avg loss = 0.6118\n",
      "Student pre-training Epoch 791: avg loss = 0.6176\n",
      "Student pre-training Epoch 792: avg loss = 0.6111\n",
      "Student pre-training Epoch 793: avg loss = 0.6212\n",
      "Student pre-training Epoch 794: avg loss = 0.6201\n",
      "Student pre-training Epoch 795: avg loss = 0.5973\n",
      "Student pre-training Epoch 796: avg loss = 0.6016\n",
      "Student pre-training Epoch 797: avg loss = 0.6110\n",
      "Student pre-training Epoch 798: avg loss = 0.6062\n",
      "Student pre-training Epoch 799: avg loss = 0.6047\n",
      "Student pre-training Epoch 800: avg loss = 0.5937\n",
      "Student pre-training Epoch 801: avg loss = 0.6072\n",
      "Student pre-training Epoch 802: avg loss = 0.6020\n",
      "Student pre-training Epoch 803: avg loss = 0.6196\n",
      "Student pre-training Epoch 804: avg loss = 0.6041\n",
      "Student pre-training Epoch 805: avg loss = 0.6091\n",
      "Student pre-training Epoch 806: avg loss = 0.6174\n",
      "Student pre-training Epoch 807: avg loss = 0.6051\n",
      "Student pre-training Epoch 808: avg loss = 0.6035\n",
      "Student pre-training Epoch 809: avg loss = 0.6112\n",
      "Student pre-training Epoch 810: avg loss = 0.5976\n",
      "Student pre-training Epoch 811: avg loss = 0.6066\n",
      "Student pre-training Epoch 812: avg loss = 0.6133\n",
      "Student pre-training Epoch 813: avg loss = 0.5982\n",
      "Student pre-training Epoch 814: avg loss = 0.5987\n",
      "Student pre-training Epoch 815: avg loss = 0.5997\n",
      "Student pre-training Epoch 816: avg loss = 0.5924\n",
      "Student pre-training Epoch 817: avg loss = 0.5862\n",
      "Student pre-training Epoch 818: avg loss = 0.5905\n",
      "Student pre-training Epoch 819: avg loss = 0.6033\n",
      "Student pre-training Epoch 820: avg loss = 0.5894\n",
      "Student pre-training Epoch 821: avg loss = 0.5947\n",
      "Student pre-training Epoch 822: avg loss = 0.5984\n",
      "Student pre-training Epoch 823: avg loss = 0.6019\n",
      "Student pre-training Epoch 824: avg loss = 0.6041\n",
      "Student pre-training Epoch 825: avg loss = 0.5946\n",
      "Student pre-training Epoch 826: avg loss = 0.6050\n",
      "Student pre-training Epoch 827: avg loss = 0.6016\n",
      "Student pre-training Epoch 828: avg loss = 0.5974\n",
      "Student pre-training Epoch 829: avg loss = 0.5901\n",
      "Student pre-training Epoch 830: avg loss = 0.5980\n",
      "Student pre-training Epoch 831: avg loss = 0.5957\n",
      "Student pre-training Epoch 832: avg loss = 0.5909\n",
      "Student pre-training Epoch 833: avg loss = 0.6013\n",
      "Student pre-training Epoch 834: avg loss = 0.5928\n",
      "Student pre-training Epoch 835: avg loss = 0.5889\n",
      "Student pre-training Epoch 836: avg loss = 0.5974\n",
      "Student pre-training Epoch 837: avg loss = 0.5960\n",
      "Student pre-training Epoch 838: avg loss = 0.5898\n",
      "Student pre-training Epoch 839: avg loss = 0.5938\n",
      "Student pre-training Epoch 840: avg loss = 0.5974\n",
      "Student pre-training Epoch 841: avg loss = 0.5927\n",
      "Student pre-training Epoch 842: avg loss = 0.5997\n",
      "Student pre-training Epoch 843: avg loss = 0.5785\n",
      "Student pre-training Epoch 844: avg loss = 0.5975\n",
      "Student pre-training Epoch 845: avg loss = 0.5963\n",
      "Student pre-training Epoch 846: avg loss = 0.5928\n",
      "Student pre-training Epoch 847: avg loss = 0.5839\n",
      "Student pre-training Epoch 848: avg loss = 0.5948\n",
      "Student pre-training Epoch 849: avg loss = 0.5987\n",
      "Student pre-training Epoch 850: avg loss = 0.5845\n",
      "Student pre-training Epoch 851: avg loss = 0.5927\n",
      "Student pre-training Epoch 852: avg loss = 0.5913\n",
      "Student pre-training Epoch 853: avg loss = 0.5891\n",
      "Student pre-training Epoch 854: avg loss = 0.5838\n",
      "Student pre-training Epoch 855: avg loss = 0.5874\n",
      "Student pre-training Epoch 856: avg loss = 0.5956\n",
      "Student pre-training Epoch 857: avg loss = 0.6032\n",
      "Student pre-training Epoch 858: avg loss = 0.5847\n",
      "Student pre-training Epoch 859: avg loss = 0.5717\n",
      "Student pre-training Epoch 860: avg loss = 0.5824\n",
      "Student pre-training Epoch 861: avg loss = 0.5993\n",
      "Student pre-training Epoch 862: avg loss = 0.5829\n",
      "Student pre-training Epoch 863: avg loss = 0.5894\n",
      "Student pre-training Epoch 864: avg loss = 0.5840\n",
      "Student pre-training Epoch 865: avg loss = 0.5892\n",
      "Student pre-training Epoch 866: avg loss = 0.5826\n",
      "Student pre-training Epoch 867: avg loss = 0.5869\n",
      "Student pre-training Epoch 868: avg loss = 0.5810\n",
      "Student pre-training Epoch 869: avg loss = 0.5852\n",
      "Student pre-training Epoch 870: avg loss = 0.5736\n",
      "Student pre-training Epoch 871: avg loss = 0.5693\n",
      "Student pre-training Epoch 872: avg loss = 0.5827\n",
      "Student pre-training Epoch 873: avg loss = 0.6010\n",
      "Student pre-training Epoch 874: avg loss = 0.5791\n",
      "Student pre-training Epoch 875: avg loss = 0.5858\n",
      "Student pre-training Epoch 876: avg loss = 0.5868\n",
      "Student pre-training Epoch 877: avg loss = 0.5733\n",
      "Student pre-training Epoch 878: avg loss = 0.5815\n",
      "Student pre-training Epoch 879: avg loss = 0.5911\n",
      "Student pre-training Epoch 880: avg loss = 0.5829\n",
      "Student pre-training Epoch 881: avg loss = 0.5765\n",
      "Student pre-training Epoch 882: avg loss = 0.5883\n",
      "Student pre-training Epoch 883: avg loss = 0.5843\n",
      "Student pre-training Epoch 884: avg loss = 0.5901\n",
      "Student pre-training Epoch 885: avg loss = 0.5753\n",
      "Student pre-training Epoch 886: avg loss = 0.5696\n",
      "Student pre-training Epoch 887: avg loss = 0.5829\n",
      "Student pre-training Epoch 888: avg loss = 0.5804\n",
      "Student pre-training Epoch 889: avg loss = 0.5895\n",
      "Student pre-training Epoch 890: avg loss = 0.5853\n",
      "Student pre-training Epoch 891: avg loss = 0.5843\n",
      "Student pre-training Epoch 892: avg loss = 0.5839\n",
      "Student pre-training Epoch 893: avg loss = 0.5686\n",
      "Student pre-training Epoch 894: avg loss = 0.5750\n",
      "Student pre-training Epoch 895: avg loss = 0.5649\n",
      "Student pre-training Epoch 896: avg loss = 0.5962\n",
      "Student pre-training Epoch 897: avg loss = 0.5740\n",
      "Student pre-training Epoch 898: avg loss = 0.5619\n",
      "Student pre-training Epoch 899: avg loss = 0.5656\n",
      "Student pre-training Epoch 900: avg loss = 0.5716\n",
      "Student pre-training Epoch 901: avg loss = 0.5739\n",
      "Student pre-training Epoch 902: avg loss = 0.5743\n",
      "Student pre-training Epoch 903: avg loss = 0.5734\n",
      "Student pre-training Epoch 904: avg loss = 0.5847\n",
      "Student pre-training Epoch 905: avg loss = 0.5723\n",
      "Student pre-training Epoch 906: avg loss = 0.5704\n",
      "Student pre-training Epoch 907: avg loss = 0.5625\n",
      "Student pre-training Epoch 908: avg loss = 0.5703\n",
      "Student pre-training Epoch 909: avg loss = 0.5851\n",
      "Student pre-training Epoch 910: avg loss = 0.5638\n",
      "Student pre-training Epoch 911: avg loss = 0.5496\n",
      "Student pre-training Epoch 912: avg loss = 0.5734\n",
      "Student pre-training Epoch 913: avg loss = 0.5782\n",
      "Student pre-training Epoch 914: avg loss = 0.5718\n",
      "Student pre-training Epoch 915: avg loss = 0.5705\n",
      "Student pre-training Epoch 916: avg loss = 0.5642\n",
      "Student pre-training Epoch 917: avg loss = 0.5668\n",
      "Student pre-training Epoch 918: avg loss = 0.5683\n",
      "Student pre-training Epoch 919: avg loss = 0.5716\n",
      "Student pre-training Epoch 920: avg loss = 0.5651\n",
      "Student pre-training Epoch 921: avg loss = 0.5628\n",
      "Student pre-training Epoch 922: avg loss = 0.5572\n",
      "Student pre-training Epoch 923: avg loss = 0.5639\n",
      "Student pre-training Epoch 924: avg loss = 0.5801\n",
      "Student pre-training Epoch 925: avg loss = 0.5609\n",
      "Student pre-training Epoch 926: avg loss = 0.5771\n",
      "Student pre-training Epoch 927: avg loss = 0.5554\n",
      "Student pre-training Epoch 928: avg loss = 0.5544\n",
      "Student pre-training Epoch 929: avg loss = 0.5727\n",
      "Student pre-training Epoch 930: avg loss = 0.5869\n",
      "Student pre-training Epoch 931: avg loss = 0.5648\n",
      "Student pre-training Epoch 932: avg loss = 0.5594\n",
      "Student pre-training Epoch 933: avg loss = 0.5678\n",
      "Student pre-training Epoch 934: avg loss = 0.5623\n",
      "Student pre-training Epoch 935: avg loss = 0.5709\n",
      "Student pre-training Epoch 936: avg loss = 0.5802\n",
      "Student pre-training Epoch 937: avg loss = 0.5555\n",
      "Student pre-training Epoch 938: avg loss = 0.5720\n",
      "Student pre-training Epoch 939: avg loss = 0.5730\n",
      "Student pre-training Epoch 940: avg loss = 0.5831\n",
      "Student pre-training Epoch 941: avg loss = 0.5653\n",
      "Student pre-training Epoch 942: avg loss = 0.5708\n",
      "Student pre-training Epoch 943: avg loss = 0.5676\n",
      "Student pre-training Epoch 944: avg loss = 0.5668\n",
      "Student pre-training Epoch 945: avg loss = 0.5746\n",
      "Student pre-training Epoch 946: avg loss = 0.5703\n",
      "Student pre-training Epoch 947: avg loss = 0.5611\n",
      "Student pre-training Epoch 948: avg loss = 0.5587\n",
      "Student pre-training Epoch 949: avg loss = 0.5578\n",
      "Student pre-training Epoch 950: avg loss = 0.5649\n",
      "Student pre-training Epoch 951: avg loss = 0.5572\n",
      "Student pre-training Epoch 952: avg loss = 0.5528\n",
      "Student pre-training Epoch 953: avg loss = 0.5787\n",
      "Student pre-training Epoch 954: avg loss = 0.5773\n",
      "Student pre-training Epoch 955: avg loss = 0.5565\n",
      "Student pre-training Epoch 956: avg loss = 0.5517\n",
      "Student pre-training Epoch 957: avg loss = 0.5518\n",
      "Student pre-training Epoch 958: avg loss = 0.5654\n",
      "Student pre-training Epoch 959: avg loss = 0.5503\n",
      "Student pre-training Epoch 960: avg loss = 0.5669\n",
      "Student pre-training Epoch 961: avg loss = 0.5651\n",
      "Student pre-training Epoch 962: avg loss = 0.5624\n",
      "Student pre-training Epoch 963: avg loss = 0.5545\n",
      "Student pre-training Epoch 964: avg loss = 0.5639\n",
      "Student pre-training Epoch 965: avg loss = 0.5558\n",
      "Student pre-training Epoch 966: avg loss = 0.5645\n",
      "Student pre-training Epoch 967: avg loss = 0.5498\n",
      "Student pre-training Epoch 968: avg loss = 0.5557\n",
      "Student pre-training Epoch 969: avg loss = 0.5588\n",
      "Student pre-training Epoch 970: avg loss = 0.5553\n",
      "Student pre-training Epoch 971: avg loss = 0.5635\n",
      "Student pre-training Epoch 972: avg loss = 0.5491\n",
      "Student pre-training Epoch 973: avg loss = 0.5401\n",
      "Student pre-training Epoch 974: avg loss = 0.5614\n",
      "Student pre-training Epoch 975: avg loss = 0.5545\n",
      "Student pre-training Epoch 976: avg loss = 0.5592\n",
      "Student pre-training Epoch 977: avg loss = 0.5461\n",
      "Student pre-training Epoch 978: avg loss = 0.5506\n",
      "Student pre-training Epoch 979: avg loss = 0.5640\n",
      "Student pre-training Epoch 980: avg loss = 0.5531\n",
      "Student pre-training Epoch 981: avg loss = 0.5482\n",
      "Student pre-training Epoch 982: avg loss = 0.5583\n",
      "Student pre-training Epoch 983: avg loss = 0.5552\n",
      "Student pre-training Epoch 984: avg loss = 0.5615\n",
      "Student pre-training Epoch 985: avg loss = 0.5573\n",
      "Student pre-training Epoch 986: avg loss = 0.5570\n",
      "Student pre-training Epoch 987: avg loss = 0.5509\n",
      "Student pre-training Epoch 988: avg loss = 0.5605\n",
      "Student pre-training Epoch 989: avg loss = 0.5539\n",
      "Student pre-training Epoch 990: avg loss = 0.5485\n",
      "Student pre-training Epoch 991: avg loss = 0.5554\n",
      "Student pre-training Epoch 992: avg loss = 0.5592\n",
      "Student pre-training Epoch 993: avg loss = 0.5642\n",
      "Student pre-training Epoch 994: avg loss = 0.5366\n",
      "Student pre-training Epoch 995: avg loss = 0.5546\n",
      "Student pre-training Epoch 996: avg loss = 0.5460\n",
      "Student pre-training Epoch 997: avg loss = 0.5489\n",
      "Student pre-training Epoch 998: avg loss = 0.5575\n",
      "Student pre-training Epoch 999: avg loss = 0.5471\n",
      "Student pre-training Epoch 1000: avg loss = 0.5412\n"
     ]
    }
   ],
   "source": [
    "# Prepare DataLoader for augmented data\n",
    "# Convert to PyTorch tensors\n",
    "X_aug_tensor = torch.from_numpy(augmented_X).float()\n",
    "y_activity_aug_tensor = torch.from_numpy(augmented_y_activity).long()\n",
    "\n",
    "# For transformation labels, convert to one-hot encoding (each sample is one-hot for the applied transform)\n",
    "num_transforms = len(transform_funcs)\n",
    "y_transform_aug = np.zeros((len(augmented_y_transform), num_transforms), dtype=np.float32)\n",
    "y_transform_aug[np.arange(len(augmented_y_transform)), augmented_y_transform] = 1.0\n",
    "y_transform_aug_tensor = torch.from_numpy(y_transform_aug)\n",
    "\n",
    "aug_dataset = torch.utils.data.TensorDataset(X_aug_tensor, y_activity_aug_tensor, y_transform_aug_tensor)\n",
    "aug_loader = torch.utils.data.DataLoader(aug_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "student_model.to(device)\n",
    "\n",
    "# Student model multi-task training (pre-training)\n",
    "optimizer_student = optim.Adam(student_model.parameters(), lr=0.00003, weight_decay=1e-4)\n",
    "criterion_activity = nn.CrossEntropyLoss()\n",
    "# Use BCEWithLogitsLoss for transformation heads\n",
    "criterion_transform = nn.BCEWithLogitsLoss()\n",
    "\n",
    "student_model.train()\n",
    "num_aug_epochs = 1000\n",
    "for epoch in range(1, num_aug_epochs + 1):\n",
    "    total_loss = 0.0\n",
    "    for batch_X, batch_y_act, batch_y_trans in aug_loader:\n",
    "        optimizer_student.zero_grad()\n",
    "        logits_act, logits_trans = student_model(batch_X.to(device))\n",
    "        loss_act = criterion_activity(logits_act, batch_y_act.to(device))\n",
    "        loss_trans = criterion_transform(logits_trans, batch_y_trans.to(device))\n",
    "        loss = loss_act + loss_trans\n",
    "        loss.backward()\n",
    "        optimizer_student.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(aug_loader)\n",
    "    print(f\"Student pre-training Epoch {epoch}: avg loss = {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Fine-Tuning on True Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 1: Loss=0.2819, BalAcc=0.8463, F1_wtd=0.9031\n",
      "Fine-tune Epoch 2: Loss=0.2084, BalAcc=0.8794, F1_wtd=0.9269\n",
      "Fine-tune Epoch 3: Loss=0.1910, BalAcc=0.8832, F1_wtd=0.9297\n",
      "Fine-tune Epoch 4: Loss=0.1806, BalAcc=0.8927, F1_wtd=0.9373\n",
      "Fine-tune Epoch 5: Loss=0.1818, BalAcc=0.8999, F1_wtd=0.9413\n",
      "Fine-tune Epoch 6: Loss=0.1633, BalAcc=0.9060, F1_wtd=0.9458\n",
      "Fine-tune Epoch 7: Loss=0.1748, BalAcc=0.9055, F1_wtd=0.9435\n",
      "Fine-tune Epoch 8: Loss=0.1553, BalAcc=0.9121, F1_wtd=0.9507\n",
      "Fine-tune Epoch 9: Loss=0.1527, BalAcc=0.9071, F1_wtd=0.9493\n",
      "Fine-tune Epoch 10: Loss=0.1568, BalAcc=0.9164, F1_wtd=0.9500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "\n",
    "# --- Freeze selected layers ---\n",
    "for param in student_model.conv1.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in student_model.conv2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# --- Optimizer for only trainable parameters ---\n",
    "optimizer_ft = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, student_model.parameters()),\n",
    "    lr=0.0001,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# --- Fine-tune loader ---\n",
    "train_dataset_ft = torch.utils.data.TensorDataset(X_train_t, y_train_t)\n",
    "train_loader_ft = torch.utils.data.DataLoader(train_dataset_ft, batch_size=256, shuffle=True)\n",
    "\n",
    "# --- Fine-tuning loop ---\n",
    "student_model = student_model.to(device)\n",
    "criterion_activity = nn.CrossEntropyLoss()\n",
    "student_model.train()\n",
    "\n",
    "num_ft_epochs = 10\n",
    "for epoch in range(1, num_ft_epochs + 1):\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch_X, batch_y in train_loader_ft:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        optimizer_ft.zero_grad()\n",
    "        logits_act, _ = student_model(batch_X)\n",
    "        loss = criterion_activity(logits_act, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer_ft.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_X.size(0)\n",
    "        preds = logits_act.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader_ft.dataset)\n",
    "    train_bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "    train_f1_wtd = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f\"Fine-tune Epoch {epoch}: \"\n",
    "          f\"Loss={avg_loss:.4f}, BalAcc={train_bal_acc:.4f}, F1_wtd={train_f1_wtd:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Standard Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Downstairs       0.72      0.83      0.77        52\n",
      "     Jogging       1.00      0.94      0.97       310\n",
      "     Sitting       0.66      1.00      0.80        59\n",
      "    Standing       1.00      0.51      0.68        41\n",
      "    Upstairs       0.91      0.74      0.81        65\n",
      "     Walking       0.95      0.98      0.97       333\n",
      "\n",
      "    accuracy                           0.92       860\n",
      "   macro avg       0.87      0.83      0.83       860\n",
      "weighted avg       0.93      0.92      0.92       860\n",
      "\n",
      "Balanced Accuracy: 0.8340833676986312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Student Model Confusion Matrix')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "student_model.eval()\n",
    "X_test_tensor = torch.from_numpy(test_X).float().to(device)\n",
    "with torch.no_grad():\n",
    "    logits, _ = student_model(X_test_tensor)\n",
    "    preds = logits.argmax(dim=1).to('cpu').numpy()\n",
    "\n",
    "cm = confusion_matrix(test_y_idx, preds)\n",
    "cm = (cm.T / cm.sum(axis=1)).T\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(test_y_idx, preds, target_names=unique_classes))\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy_score(test_y_idx, preds))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, xticklabels=unique_classes, yticklabels=unique_classes, cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Activity\")\n",
    "plt.ylabel(\"True Activity\")\n",
    "plt.title(\"Student Model Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Linear Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear evaluation accuracy: 0.9407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home_as/shared/arbeitsgruppen/flowsim/dependencies/RHEL8/2022.06.0/linux-rhel8-haswell/gcc-11.3.0/py-scikit-learn-1.0.1-72r4tcjwohltlr33uxehss6bdz4thm3a/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Modify the student model to extract features (from the conv core output)\n",
    "def extract_features(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.from_numpy(X).float().to(device)\n",
    "        # Forward pass until the features are computed:\n",
    "        X_tensor = X_tensor.transpose(1, 2)\n",
    "        x = model.relu(model.conv1(X_tensor))\n",
    "        x = model.dropout(x)\n",
    "        x = model.relu(model.conv2(x))\n",
    "        x = model.dropout(x)\n",
    "        x = model.relu(model.conv3(x))\n",
    "        x = F.max_pool1d(x, kernel_size=x.shape[2])\n",
    "        features = x.squeeze(2)\n",
    "        return features.to('cpu').numpy()\n",
    "\n",
    "train_features = extract_features(student_model, train_labeled_X)\n",
    "test_features = extract_features(student_model, test_X)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(train_features, train_labeled_y_idx)\n",
    "linear_preds = clf.predict(test_features)\n",
    "linear_acc = np.mean(linear_preds == test_y_idx)\n",
    "print(f\"Linear evaluation accuracy: {linear_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smartyEnvmnt_PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
