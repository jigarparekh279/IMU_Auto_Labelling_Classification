{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMU Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 1098209\n",
      "Train records: 898365\n",
      "Test records: 199844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2730419/2003044245.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[['X','Y','Z']] = (train_data[['X','Y','Z']] - mean_vals) / std_vals\n",
      "/tmp/ipykernel_2730419/2003044245.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data[['X','Y','Z']]  = (test_data[['X','Y','Z']]  - mean_vals) / std_vals\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labeled windows: 3891\n",
      "Train unlabeled windows: 597\n",
      "Test windows: 860\n",
      "Unique activity classes: ['Downstairs' 'Jogging' 'Sitting' 'Standing' 'Upstairs' 'Walking']\n",
      "Mapping: {'Downstairs': 0, 'Jogging': 1, 'Sitting': 2, 'Standing': 3, 'Upstairs': 4, 'Walking': 5}\n"
     ]
    }
   ],
   "source": [
    "# --- Parameters ---\n",
    "WINDOW_SIZE = 400\n",
    "STEP_SIZE = 200\n",
    "\n",
    "# --- Load All Data and Split Based on ID ---\n",
    "all_files = [\"data/IMU_case_dataset_part1_.csv\",\n",
    "             \"data/IMU_case_dataset_part2_.csv\",\n",
    "             \"data/IMU_case_dataset_part3_.csv\",\n",
    "             \"data/IMU_case_dataset_part4_.csv\"]\n",
    "\n",
    "dfs = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    # Keep the necessary columns: 'X', 'Y', 'Z', 'activity', and 'ID'\n",
    "    df = df[['X','Y','Z','activity','ID']]\n",
    "    dfs.append(df)\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Ensure that the 'ID' column is numeric\n",
    "data['ID'] = pd.to_numeric(data['ID'], errors='coerce')\n",
    "\n",
    "# Split the dataset:\n",
    "# Training data: IDs from 1 to 30\n",
    "# Testing data: IDs from 31 to 36\n",
    "train_data = data[data['ID'].between(1, 30)]\n",
    "test_data  = data[data['ID'].between(31, 36)]\n",
    "\n",
    "print(\"Total records:\", data.shape[0])\n",
    "print(\"Train records:\", train_data.shape[0])\n",
    "print(\"Test records:\", test_data.shape[0])\n",
    "\n",
    "# --- Normalization ---\n",
    "# Compute normalization parameters (mean and std) on the training set:\n",
    "mean_vals = train_data[['X','Y','Z']].mean()\n",
    "std_vals = train_data[['X','Y','Z']].std()\n",
    "\n",
    "# Apply normalization to both training and test data:\n",
    "train_data[['X','Y','Z']] = (train_data[['X','Y','Z']] - mean_vals) / std_vals\n",
    "test_data[['X','Y','Z']]  = (test_data[['X','Y','Z']]  - mean_vals) / std_vals\n",
    "\n",
    "# --- Windowing Function ---\n",
    "def generate_windows(df):\n",
    "    X_windows = []\n",
    "    y_labels = []\n",
    "    # Assuming the dataframe is sorted by time\n",
    "    for start in range(0, len(df) - WINDOW_SIZE + 1, STEP_SIZE):\n",
    "        window = df.iloc[start : start + WINDOW_SIZE]\n",
    "        labels = window[\"activity\"]\n",
    "        if labels.isna().all():\n",
    "            # Completely unlabeled window\n",
    "            X_windows.append(window[[\"X\", \"Y\", \"Z\"]].values)\n",
    "            y_labels.append(None)\n",
    "        else:\n",
    "            unique_labels = labels.dropna().unique()\n",
    "            if len(unique_labels) == 1:\n",
    "                X_windows.append(window[[\"X\", \"Y\", \"Z\"]].values)\n",
    "                y_labels.append(unique_labels[0])\n",
    "            else:\n",
    "                # Mixed or transitional window is treated as unlabeled\n",
    "                X_windows.append(window[[\"X\", \"Y\", \"Z\"]].values)\n",
    "                y_labels.append(None)\n",
    "    return X_windows, y_labels\n",
    "\n",
    "# --- Generate Windows from Training and Test Data ---\n",
    "# For training data:\n",
    "train_X_all, train_y_all = generate_windows(train_data)\n",
    "# Separate into labeled and unlabeled windows:\n",
    "train_labeled_X = [x for x, y in zip(train_X_all, train_y_all) if y is not None]\n",
    "train_labeled_y = [y for y in train_y_all if y is not None]\n",
    "train_unlabeled_X = [x for x, y in zip(train_X_all, train_y_all) if y is None]\n",
    "\n",
    "# Convert to numpy arrays:\n",
    "train_labeled_X = np.array(train_labeled_X)   # shape: (N_labeled, 400, 3)\n",
    "train_labeled_y = np.array(train_labeled_y)     # shape: (N_labeled,)\n",
    "train_unlabeled_X = np.array(train_unlabeled_X) # shape: (N_unlabeled, 400, 3)\n",
    "\n",
    "# Remove windows with any NaN features (if any)\n",
    "not_nan_idx = np.isnan(train_labeled_X).sum(axis=-1).sum(axis=-1) == 0\n",
    "train_labeled_X = train_labeled_X[not_nan_idx]\n",
    "train_labeled_y = train_labeled_y[not_nan_idx]\n",
    "\n",
    "not_nan_idx = np.isnan(train_unlabeled_X).sum(axis=-1).sum(axis=-1) == 0\n",
    "train_unlabeled_X = train_unlabeled_X[not_nan_idx]\n",
    "\n",
    "# For test data (only use windows with defined labels)\n",
    "test_X, test_y = generate_windows(test_data)\n",
    "test_X = np.array([x for x, y in zip(test_X, test_y) if y is not None])\n",
    "test_y = np.array([y for y in test_y if y is not None])\n",
    "\n",
    "print(\"Train labeled windows:\", len(train_labeled_y))\n",
    "print(\"Train unlabeled windows:\", train_unlabeled_X.shape[0])\n",
    "print(\"Test windows:\", len(test_y))\n",
    "\n",
    "# --- Label Encoding ---\n",
    "# Convert string labels to integer indices\n",
    "unique_classes = np.unique(train_labeled_y)\n",
    "print(\"Unique activity classes:\", unique_classes)\n",
    "class_to_idx = {label: idx for idx, label in enumerate(unique_classes)}\n",
    "print(\"Mapping:\", class_to_idx)\n",
    "\n",
    "train_labeled_y_idx = np.array([class_to_idx[label] for label in train_labeled_y])\n",
    "test_y_idx = np.array([class_to_idx[label] for label in test_y])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architecture: Teacher-Student TPN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPNModel(\n",
      "  (conv1): Conv1d(3, 32, kernel_size=(24,), stride=(1,))\n",
      "  (conv2): Conv1d(32, 64, kernel_size=(16,), stride=(1,))\n",
      "  (conv3): Conv1d(64, 96, kernel_size=(8,), stride=(1,))\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc_har1): Linear(in_features=96, out_features=1024, bias=True)\n",
      "  (fc_har2): Linear(in_features=1024, out_features=6, bias=True)\n",
      ")\n",
      "TPNModel(\n",
      "  (conv1): Conv1d(3, 32, kernel_size=(24,), stride=(1,))\n",
      "  (conv2): Conv1d(32, 64, kernel_size=(16,), stride=(1,))\n",
      "  (conv3): Conv1d(64, 96, kernel_size=(8,), stride=(1,))\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc_har1): Linear(in_features=96, out_features=1024, bias=True)\n",
      "  (fc_har2): Linear(in_features=1024, out_features=6, bias=True)\n",
      "  (transform_heads): ModuleList(\n",
      "    (0-7): 8 x Sequential(\n",
      "      (0): Linear(in_features=96, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TPNModel(nn.Module):\n",
    "    def __init__(self, num_classes, num_transforms=0):\n",
    "        super(TPNModel, self).__init__()\n",
    "        # Convolutional core: three conv layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=32, kernel_size=24, stride=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=16, stride=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=96, kernel_size=8, stride=1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        if num_classes > 0:\n",
    "            self.fc_har1 = nn.Linear(96, 1024)\n",
    "            self.fc_har2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "        self.num_transforms = num_transforms\n",
    "        if num_transforms > 0:\n",
    "            self.transform_heads = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(96, 256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, 1)\n",
    "                )\n",
    "                for _ in range(num_transforms)\n",
    "            ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, 400, 3) -> transpose to (batch, 3, 400)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        # Global max pooling over time dimension\n",
    "        x = F.max_pool1d(x, kernel_size=x.shape[2])\n",
    "        features = x.squeeze(2)  # shape: (batch, 96)\n",
    "\n",
    "        # Activity classification head\n",
    "        class_logits = None\n",
    "        if self.num_classes > 0:\n",
    "            h = self.relu(self.fc_har1(features))\n",
    "            class_logits = self.fc_har2(h)  # raw scores for each class\n",
    "\n",
    "        # Transformation discrimination heads\n",
    "        transform_logits = None\n",
    "        if self.num_transforms > 0:\n",
    "            logits_list = []\n",
    "            for head in self.transform_heads:\n",
    "                logit = head(features)   # shape: (batch, 1)\n",
    "                logits_list.append(logit.squeeze(-1))  # shape: (batch,)\n",
    "            transform_logits = torch.stack(logits_list, dim=1)  # shape: (batch, num_transforms)\n",
    "        return class_logits, transform_logits\n",
    "\n",
    "# Define the number of activity classes (from unique_classes) and transformations (8)\n",
    "num_classes = len(unique_classes)\n",
    "num_transforms = 8\n",
    "\n",
    "# Teacher: only classification head\n",
    "teacher_model = TPNModel(num_classes=num_classes, num_transforms=0)\n",
    "# Student: both classification and transformation heads\n",
    "student_model = TPNModel(num_classes=num_classes, num_transforms=num_transforms)\n",
    "\n",
    "print(teacher_model)\n",
    "print(student_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Teacher Model Training (Supervised Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1: Train Loss=1.4096, BalAcc=0.1892, F1_wtd=0.3559 | Val Loss=1.2765, BalAcc=0.2376, F1_wtd=0.4869\n",
      "Epoch 2: Train Loss=1.1742, BalAcc=0.2829, F1_wtd=0.5623 | Val Loss=1.2610, BalAcc=0.2714, F1_wtd=0.5536\n",
      "Epoch 3: Train Loss=0.9939, BalAcc=0.2983, F1_wtd=0.5935 | Val Loss=1.0141, BalAcc=0.3060, F1_wtd=0.6139\n",
      "Epoch 4: Train Loss=0.8035, BalAcc=0.3205, F1_wtd=0.6364 | Val Loss=0.8607, BalAcc=0.4649, F1_wtd=0.7084\n",
      "Epoch 5: Train Loss=0.6465, BalAcc=0.4073, F1_wtd=0.6966 | Val Loss=0.7092, BalAcc=0.4672, F1_wtd=0.7130\n",
      "Epoch 6: Train Loss=0.5445, BalAcc=0.4741, F1_wtd=0.7222 | Val Loss=0.6043, BalAcc=0.5827, F1_wtd=0.7666\n",
      "Epoch 7: Train Loss=0.4701, BalAcc=0.5390, F1_wtd=0.7549 | Val Loss=0.4994, BalAcc=0.6566, F1_wtd=0.7863\n",
      "Epoch 8: Train Loss=0.4150, BalAcc=0.6562, F1_wtd=0.7996 | Val Loss=0.4553, BalAcc=0.6640, F1_wtd=0.7988\n",
      "Epoch 9: Train Loss=0.3769, BalAcc=0.6855, F1_wtd=0.8236 | Val Loss=0.3923, BalAcc=0.7224, F1_wtd=0.8224\n",
      "Epoch 10: Train Loss=0.3507, BalAcc=0.7053, F1_wtd=0.8366 | Val Loss=0.3649, BalAcc=0.7196, F1_wtd=0.8286\n",
      "Epoch 11: Train Loss=0.3171, BalAcc=0.7166, F1_wtd=0.8483 | Val Loss=0.3277, BalAcc=0.7347, F1_wtd=0.8525\n",
      "Epoch 12: Train Loss=0.2891, BalAcc=0.7521, F1_wtd=0.8716 | Val Loss=0.2999, BalAcc=0.7579, F1_wtd=0.8713\n",
      "Epoch 13: Train Loss=0.2756, BalAcc=0.7710, F1_wtd=0.8820 | Val Loss=0.2846, BalAcc=0.7701, F1_wtd=0.8735\n",
      "Epoch 14: Train Loss=0.2579, BalAcc=0.7714, F1_wtd=0.8847 | Val Loss=0.2673, BalAcc=0.7802, F1_wtd=0.8799\n",
      "Epoch 15: Train Loss=0.2347, BalAcc=0.7901, F1_wtd=0.8937 | Val Loss=0.2634, BalAcc=0.8077, F1_wtd=0.8980\n",
      "Epoch 16: Train Loss=0.2292, BalAcc=0.8013, F1_wtd=0.8992 | Val Loss=0.2467, BalAcc=0.8118, F1_wtd=0.8996\n",
      "Epoch 17: Train Loss=0.2095, BalAcc=0.8286, F1_wtd=0.9124 | Val Loss=0.2393, BalAcc=0.8138, F1_wtd=0.8956\n",
      "Epoch 18: Train Loss=0.2145, BalAcc=0.8336, F1_wtd=0.9136 | Val Loss=0.2310, BalAcc=0.8228, F1_wtd=0.9054\n",
      "Epoch 19: Train Loss=0.1979, BalAcc=0.8509, F1_wtd=0.9252 | Val Loss=0.2176, BalAcc=0.8218, F1_wtd=0.9034\n",
      "Epoch 20: Train Loss=0.1823, BalAcc=0.8543, F1_wtd=0.9277 | Val Loss=0.2019, BalAcc=0.8644, F1_wtd=0.9314\n",
      "Epoch 21: Train Loss=0.1769, BalAcc=0.8667, F1_wtd=0.9339 | Val Loss=0.2215, BalAcc=0.8277, F1_wtd=0.9074\n",
      "Epoch 22: Train Loss=0.1660, BalAcc=0.8672, F1_wtd=0.9343 | Val Loss=0.1811, BalAcc=0.8714, F1_wtd=0.9344\n",
      "Epoch 23: Train Loss=0.1572, BalAcc=0.8858, F1_wtd=0.9435 | Val Loss=0.2088, BalAcc=0.8624, F1_wtd=0.9288\n",
      "Epoch 24: Train Loss=0.1506, BalAcc=0.8875, F1_wtd=0.9444 | Val Loss=0.1793, BalAcc=0.8814, F1_wtd=0.9369\n",
      "Epoch 25: Train Loss=0.1536, BalAcc=0.8907, F1_wtd=0.9448 | Val Loss=0.1900, BalAcc=0.8882, F1_wtd=0.9369\n",
      "Epoch 26: Train Loss=0.1481, BalAcc=0.8937, F1_wtd=0.9455 | Val Loss=0.1607, BalAcc=0.8912, F1_wtd=0.9422\n",
      "Epoch 27: Train Loss=0.1367, BalAcc=0.8921, F1_wtd=0.9473 | Val Loss=0.1892, BalAcc=0.9002, F1_wtd=0.9433\n",
      "Epoch 28: Train Loss=0.1316, BalAcc=0.9155, F1_wtd=0.9565 | Val Loss=0.1850, BalAcc=0.9267, F1_wtd=0.9518\n",
      "Epoch 29: Train Loss=0.1202, BalAcc=0.9177, F1_wtd=0.9594 | Val Loss=0.1481, BalAcc=0.9249, F1_wtd=0.9560\n",
      "Epoch 30: Train Loss=0.1127, BalAcc=0.9231, F1_wtd=0.9614 | Val Loss=0.1739, BalAcc=0.8870, F1_wtd=0.9393\n",
      "Epoch 31: Train Loss=0.1200, BalAcc=0.9188, F1_wtd=0.9586 | Val Loss=0.1467, BalAcc=0.9191, F1_wtd=0.9601\n",
      "Epoch 32: Train Loss=0.1223, BalAcc=0.9161, F1_wtd=0.9572 | Val Loss=0.1429, BalAcc=0.9180, F1_wtd=0.9532\n",
      "Epoch 33: Train Loss=0.1099, BalAcc=0.9154, F1_wtd=0.9574 | Val Loss=0.1508, BalAcc=0.9180, F1_wtd=0.9535\n",
      "Epoch 34: Train Loss=0.1118, BalAcc=0.9186, F1_wtd=0.9595 | Val Loss=0.1306, BalAcc=0.9250, F1_wtd=0.9607\n",
      "Epoch 35: Train Loss=0.1014, BalAcc=0.9307, F1_wtd=0.9659 | Val Loss=0.1345, BalAcc=0.9111, F1_wtd=0.9545\n",
      "Epoch 36: Train Loss=0.0997, BalAcc=0.9241, F1_wtd=0.9631 | Val Loss=0.1205, BalAcc=0.9349, F1_wtd=0.9662\n",
      "Epoch 37: Train Loss=0.1013, BalAcc=0.9220, F1_wtd=0.9605 | Val Loss=0.1441, BalAcc=0.9318, F1_wtd=0.9547\n",
      "Epoch 38: Train Loss=0.0994, BalAcc=0.9318, F1_wtd=0.9661 | Val Loss=0.1222, BalAcc=0.9247, F1_wtd=0.9609\n",
      "Epoch 39: Train Loss=0.0959, BalAcc=0.9354, F1_wtd=0.9674 | Val Loss=0.1458, BalAcc=0.9328, F1_wtd=0.9570\n",
      "Epoch 40: Train Loss=0.0872, BalAcc=0.9451, F1_wtd=0.9710 | Val Loss=0.1238, BalAcc=0.9320, F1_wtd=0.9637\n",
      "Epoch 41: Train Loss=0.0928, BalAcc=0.9333, F1_wtd=0.9661 | Val Loss=0.1271, BalAcc=0.9267, F1_wtd=0.9662\n",
      "Epoch 42: Train Loss=0.0852, BalAcc=0.9338, F1_wtd=0.9658 | Val Loss=0.1304, BalAcc=0.9350, F1_wtd=0.9619\n",
      "Epoch 43: Train Loss=0.0774, BalAcc=0.9458, F1_wtd=0.9725 | Val Loss=0.1126, BalAcc=0.9380, F1_wtd=0.9691\n",
      "Epoch 44: Train Loss=0.0797, BalAcc=0.9505, F1_wtd=0.9745 | Val Loss=0.1227, BalAcc=0.9350, F1_wtd=0.9616\n",
      "Epoch 45: Train Loss=0.0703, BalAcc=0.9474, F1_wtd=0.9731 | Val Loss=0.1187, BalAcc=0.9609, F1_wtd=0.9722\n",
      "Epoch 46: Train Loss=0.0640, BalAcc=0.9591, F1_wtd=0.9787 | Val Loss=0.1058, BalAcc=0.9371, F1_wtd=0.9712\n",
      "Epoch 47: Train Loss=0.0704, BalAcc=0.9510, F1_wtd=0.9760 | Val Loss=0.1214, BalAcc=0.9281, F1_wtd=0.9591\n",
      "Epoch 48: Train Loss=0.0708, BalAcc=0.9507, F1_wtd=0.9744 | Val Loss=0.0998, BalAcc=0.9499, F1_wtd=0.9693\n",
      "Epoch 49: Train Loss=0.0648, BalAcc=0.9553, F1_wtd=0.9781 | Val Loss=0.0912, BalAcc=0.9430, F1_wtd=0.9666\n",
      "Epoch 50: Train Loss=0.0688, BalAcc=0.9529, F1_wtd=0.9763 | Val Loss=0.0889, BalAcc=0.9570, F1_wtd=0.9769\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "\n",
    "# --- Device setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Train/Val split ---\n",
    "X_train_tensor = torch.from_numpy(train_labeled_X).float()\n",
    "y_train_tensor = torch.from_numpy(train_labeled_y_idx).long()\n",
    "\n",
    "X_train_t, X_val_t, y_train_t, y_val_t = train_test_split(\n",
    "    X_train_tensor, y_train_tensor, test_size=0.1, random_state=42, stratify=y_train_tensor\n",
    ")\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_t, y_train_t)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val_t, y_val_t)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# --- Model, loss, optimizer ---\n",
    "teacher_model = teacher_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(teacher_model.parameters(), lr=0.0003, weight_decay=1e-4)\n",
    "\n",
    "# --- Training loop ---\n",
    "num_epochs = 50\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    teacher_model.train()\n",
    "    total_train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = teacher_model(batch_X)\n",
    "        loss = criterion(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item() * batch_X.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    train_bal_acc = balanced_accuracy_score(train_labels, train_preds)\n",
    "    train_f1_wtd = f1_score(train_labels, train_preds, average='weighted')\n",
    "    avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "\n",
    "    # --- Validation ---\n",
    "    teacher_model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_X, val_y in val_loader:\n",
    "            val_X, val_y = val_X.to(device), val_y.to(device)\n",
    "            logits, _ = teacher_model(val_X)\n",
    "            loss = criterion(logits, val_y)\n",
    "\n",
    "            total_val_loss += loss.item() * val_X.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(val_y.cpu().numpy())\n",
    "\n",
    "    val_bal_acc = balanced_accuracy_score(val_labels, val_preds)\n",
    "    val_f1_wtd = f1_score(val_labels, val_preds, average='weighted')\n",
    "    avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch}: \"\n",
    "          f\"Train Loss={avg_train_loss:.4f}, BalAcc={train_bal_acc:.4f}, F1_wtd={train_f1_wtd:.4f} | \"\n",
    "          f\"Val Loss={avg_val_loss:.4f}, BalAcc={val_bal_acc:.4f}, F1_wtd={val_f1_wtd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher balanced model accuracy on test set: 0.8530\n",
      "Teacher model wtd f1 score on test set: 0.9243\n"
     ]
    }
   ],
   "source": [
    "teacher_model.eval()\n",
    "test_X_tensor = torch.from_numpy(test_X).float().to(device)\n",
    "test_y_tensor = torch.from_numpy(test_y_idx).long().to(device)\n",
    "with torch.no_grad():\n",
    "    logits, _ = teacher_model(test_X_tensor)\n",
    "    predictions = logits.argmax(dim=1)\n",
    "# test_accuracy = (predictions == test_y_tensor).sum().item() / len(test_y_tensor)\n",
    "test_accuracy = balanced_accuracy_score(test_y_tensor.to('cpu').numpy(), predictions.to('cpu').numpy())\n",
    "test_wtdf1 = f1_score(test_y_tensor.to('cpu').numpy(), predictions.to('cpu').numpy(), average='weighted')\n",
    "print(f\"Teacher balanced model accuracy on test set: {test_accuracy:.4f}\")\n",
    "print(f\"Teacher model wtd f1 score on test set: {test_wtdf1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Self-Labeling and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Pseudo-Labeling Unlabeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 563 out of 597 unlabeled samples based on confidence.\n",
      "Pseudo-labeled dataset size: 347\n"
     ]
    }
   ],
   "source": [
    "teacher_model.eval()\n",
    "X_unlabeled = torch.from_numpy(train_unlabeled_X).float().to(device)\n",
    "with torch.no_grad():\n",
    "    logits, _ = teacher_model(X_unlabeled)\n",
    "    probs = F.softmax(logits, dim=1).to('cpu').numpy()\n",
    "\n",
    "pred_classes = probs.argmax(axis=1)\n",
    "pred_confidences = probs.max(axis=1)\n",
    "\n",
    "# Confidence threshold\n",
    "confidence_threshold = 0.5\n",
    "selected_idx = np.where(pred_confidences >= confidence_threshold)[0]\n",
    "print(f\"Selected {len(selected_idx)} out of {train_unlabeled_X.shape[0]} unlabeled samples based on confidence.\")\n",
    "\n",
    "# Optionally balance classes (limit to top K per class)\n",
    "K = 100  # maximum samples per class\n",
    "selected_idx_balanced = []\n",
    "for c in range(num_classes):\n",
    "    class_idxs = selected_idx[pred_classes[selected_idx] == c]\n",
    "    if len(class_idxs) > K:\n",
    "        # sort by confidence and take top K\n",
    "        sorted_idxs = class_idxs[np.argsort(pred_confidences[class_idxs])][-K:]\n",
    "        selected_idx_balanced.extend(sorted_idxs)\n",
    "    else:\n",
    "        selected_idx_balanced.extend(class_idxs)\n",
    "selected_idx_balanced = np.array(selected_idx_balanced)\n",
    "\n",
    "pseudo_labeled_X = train_unlabeled_X[selected_idx_balanced]\n",
    "pseudo_labeled_y = pred_classes[selected_idx_balanced]\n",
    "\n",
    "print(\"Pseudo-labeled dataset size:\", pseudo_labeled_X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data Augmentation with Signal Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented dataset size: 2776\n"
     ]
    }
   ],
   "source": [
    "import math, random\n",
    "\n",
    "def add_noise(signal, sigma=0.1):\n",
    "    return signal + np.random.normal(scale=sigma, size=signal.shape)\n",
    "\n",
    "def scale_signal(signal, sigma=0.1):\n",
    "    factor = np.random.normal(loc=1.0, scale=sigma)\n",
    "    return signal * factor\n",
    "\n",
    "def rotate_signal(signal):\n",
    "    axis = np.random.normal(size=3)\n",
    "    axis = axis / np.linalg.norm(axis)\n",
    "    theta = np.random.uniform(0, 2*math.pi)\n",
    "    K = np.array([[0, -axis[2], axis[1]],\n",
    "                  [axis[2], 0, -axis[0]],\n",
    "                  [-axis[1], axis[0], 0]])\n",
    "    I = np.eye(3)\n",
    "    R = I + math.sin(theta)*K + (1 - math.cos(theta))*(K.dot(K))\n",
    "    return signal.dot(R.T)\n",
    "\n",
    "def invert_signal(signal):\n",
    "    return -signal\n",
    "\n",
    "def reverse_time(signal):\n",
    "    return signal[::-1, :]\n",
    "\n",
    "def scramble_segments(signal, num_segments=4):\n",
    "    seg_len = signal.shape[0] // num_segments\n",
    "    segments = [signal[i*seg_len : (i+1)*seg_len] for i in range(num_segments)]\n",
    "    random.shuffle(segments)\n",
    "    return np.concatenate(segments, axis=0)\n",
    "\n",
    "def shuffle_channels(signal):\n",
    "    perm = np.random.permutation(3)\n",
    "    return signal[:, perm]\n",
    "\n",
    "# List of transformation functions (we assume identity is one of the tasks)\n",
    "transform_funcs = [\n",
    "    add_noise, scale_signal, rotate_signal, invert_signal,\n",
    "    reverse_time, scramble_segments, shuffle_channels, lambda x: x\n",
    "    ]\n",
    "\n",
    "# Augment data: for each sample, apply all 8 transformations\n",
    "augmented_X = []\n",
    "augmented_y_activity = []   # same pseudo label for all augmentations\n",
    "augmented_y_transform = []  # index (0 to 7) indicating the transform\n",
    "for x, label in zip(pseudo_labeled_X, pseudo_labeled_y):\n",
    "    for t_idx, func in enumerate(transform_funcs):\n",
    "        augmented_X.append(func(x))\n",
    "        augmented_y_activity.append(label)\n",
    "        augmented_y_transform.append(t_idx)\n",
    "\n",
    "augmented_X = np.array(augmented_X)  # shape: (N_aug, 400, 3)\n",
    "augmented_y_activity = np.array(augmented_y_activity)\n",
    "augmented_y_transform = np.array(augmented_y_transform)\n",
    "\n",
    "print(\"Augmented dataset size:\", augmented_X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Student Model Training (Multi-task Pre-training + Fine-Tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Pre-Training with Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student pre-training Epoch 1: avg loss = 2.4505\n",
      "Student pre-training Epoch 2: avg loss = 2.3782\n",
      "Student pre-training Epoch 3: avg loss = 2.3095\n",
      "Student pre-training Epoch 4: avg loss = 2.2455\n",
      "Student pre-training Epoch 5: avg loss = 2.1823\n",
      "Student pre-training Epoch 6: avg loss = 2.1269\n",
      "Student pre-training Epoch 7: avg loss = 2.0775\n",
      "Student pre-training Epoch 8: avg loss = 2.0338\n",
      "Student pre-training Epoch 9: avg loss = 2.0008\n",
      "Student pre-training Epoch 10: avg loss = 1.9756\n",
      "Student pre-training Epoch 11: avg loss = 1.9596\n",
      "Student pre-training Epoch 12: avg loss = 1.9466\n",
      "Student pre-training Epoch 13: avg loss = 1.9387\n",
      "Student pre-training Epoch 14: avg loss = 1.9329\n",
      "Student pre-training Epoch 15: avg loss = 1.9283\n",
      "Student pre-training Epoch 16: avg loss = 1.9229\n",
      "Student pre-training Epoch 17: avg loss = 1.9181\n",
      "Student pre-training Epoch 18: avg loss = 1.9127\n",
      "Student pre-training Epoch 19: avg loss = 1.9089\n",
      "Student pre-training Epoch 20: avg loss = 1.9045\n",
      "Student pre-training Epoch 21: avg loss = 1.9004\n",
      "Student pre-training Epoch 22: avg loss = 1.8941\n",
      "Student pre-training Epoch 23: avg loss = 1.8894\n",
      "Student pre-training Epoch 24: avg loss = 1.8853\n",
      "Student pre-training Epoch 25: avg loss = 1.8778\n",
      "Student pre-training Epoch 26: avg loss = 1.8724\n",
      "Student pre-training Epoch 27: avg loss = 1.8649\n",
      "Student pre-training Epoch 28: avg loss = 1.8606\n",
      "Student pre-training Epoch 29: avg loss = 1.8509\n",
      "Student pre-training Epoch 30: avg loss = 1.8413\n",
      "Student pre-training Epoch 31: avg loss = 1.8322\n",
      "Student pre-training Epoch 32: avg loss = 1.8178\n",
      "Student pre-training Epoch 33: avg loss = 1.8056\n",
      "Student pre-training Epoch 34: avg loss = 1.7884\n",
      "Student pre-training Epoch 35: avg loss = 1.7712\n",
      "Student pre-training Epoch 36: avg loss = 1.7533\n",
      "Student pre-training Epoch 37: avg loss = 1.7373\n",
      "Student pre-training Epoch 38: avg loss = 1.7167\n",
      "Student pre-training Epoch 39: avg loss = 1.6968\n",
      "Student pre-training Epoch 40: avg loss = 1.6784\n",
      "Student pre-training Epoch 41: avg loss = 1.6618\n",
      "Student pre-training Epoch 42: avg loss = 1.6424\n",
      "Student pre-training Epoch 43: avg loss = 1.6276\n",
      "Student pre-training Epoch 44: avg loss = 1.6108\n",
      "Student pre-training Epoch 45: avg loss = 1.5961\n",
      "Student pre-training Epoch 46: avg loss = 1.5818\n",
      "Student pre-training Epoch 47: avg loss = 1.5672\n",
      "Student pre-training Epoch 48: avg loss = 1.5551\n",
      "Student pre-training Epoch 49: avg loss = 1.5381\n",
      "Student pre-training Epoch 50: avg loss = 1.5218\n",
      "Student pre-training Epoch 51: avg loss = 1.5061\n",
      "Student pre-training Epoch 52: avg loss = 1.4918\n",
      "Student pre-training Epoch 53: avg loss = 1.4803\n",
      "Student pre-training Epoch 54: avg loss = 1.4661\n",
      "Student pre-training Epoch 55: avg loss = 1.4518\n",
      "Student pre-training Epoch 56: avg loss = 1.4355\n",
      "Student pre-training Epoch 57: avg loss = 1.4272\n",
      "Student pre-training Epoch 58: avg loss = 1.4177\n",
      "Student pre-training Epoch 59: avg loss = 1.4013\n",
      "Student pre-training Epoch 60: avg loss = 1.4011\n",
      "Student pre-training Epoch 61: avg loss = 1.3892\n",
      "Student pre-training Epoch 62: avg loss = 1.3719\n",
      "Student pre-training Epoch 63: avg loss = 1.3669\n",
      "Student pre-training Epoch 64: avg loss = 1.3588\n",
      "Student pre-training Epoch 65: avg loss = 1.3458\n",
      "Student pre-training Epoch 66: avg loss = 1.3369\n",
      "Student pre-training Epoch 67: avg loss = 1.3444\n",
      "Student pre-training Epoch 68: avg loss = 1.3248\n",
      "Student pre-training Epoch 69: avg loss = 1.3219\n",
      "Student pre-training Epoch 70: avg loss = 1.3091\n",
      "Student pre-training Epoch 71: avg loss = 1.3012\n",
      "Student pre-training Epoch 72: avg loss = 1.2986\n",
      "Student pre-training Epoch 73: avg loss = 1.2877\n",
      "Student pre-training Epoch 74: avg loss = 1.2841\n",
      "Student pre-training Epoch 75: avg loss = 1.2764\n",
      "Student pre-training Epoch 76: avg loss = 1.2710\n",
      "Student pre-training Epoch 77: avg loss = 1.2665\n",
      "Student pre-training Epoch 78: avg loss = 1.2573\n",
      "Student pre-training Epoch 79: avg loss = 1.2532\n",
      "Student pre-training Epoch 80: avg loss = 1.2462\n",
      "Student pre-training Epoch 81: avg loss = 1.2371\n",
      "Student pre-training Epoch 82: avg loss = 1.2352\n",
      "Student pre-training Epoch 83: avg loss = 1.2278\n",
      "Student pre-training Epoch 84: avg loss = 1.2213\n",
      "Student pre-training Epoch 85: avg loss = 1.2148\n",
      "Student pre-training Epoch 86: avg loss = 1.2088\n",
      "Student pre-training Epoch 87: avg loss = 1.2014\n",
      "Student pre-training Epoch 88: avg loss = 1.1963\n",
      "Student pre-training Epoch 89: avg loss = 1.1901\n",
      "Student pre-training Epoch 90: avg loss = 1.1998\n",
      "Student pre-training Epoch 91: avg loss = 1.1859\n",
      "Student pre-training Epoch 92: avg loss = 1.1813\n",
      "Student pre-training Epoch 93: avg loss = 1.1785\n",
      "Student pre-training Epoch 94: avg loss = 1.1721\n",
      "Student pre-training Epoch 95: avg loss = 1.1717\n",
      "Student pre-training Epoch 96: avg loss = 1.1668\n",
      "Student pre-training Epoch 97: avg loss = 1.1632\n",
      "Student pre-training Epoch 98: avg loss = 1.1579\n",
      "Student pre-training Epoch 99: avg loss = 1.1455\n",
      "Student pre-training Epoch 100: avg loss = 1.1436\n",
      "Student pre-training Epoch 101: avg loss = 1.1363\n",
      "Student pre-training Epoch 102: avg loss = 1.1382\n",
      "Student pre-training Epoch 103: avg loss = 1.1258\n",
      "Student pre-training Epoch 104: avg loss = 1.1286\n",
      "Student pre-training Epoch 105: avg loss = 1.1281\n",
      "Student pre-training Epoch 106: avg loss = 1.1170\n",
      "Student pre-training Epoch 107: avg loss = 1.1191\n",
      "Student pre-training Epoch 108: avg loss = 1.0999\n",
      "Student pre-training Epoch 109: avg loss = 1.1055\n",
      "Student pre-training Epoch 110: avg loss = 1.1035\n",
      "Student pre-training Epoch 111: avg loss = 1.0960\n",
      "Student pre-training Epoch 112: avg loss = 1.0912\n",
      "Student pre-training Epoch 113: avg loss = 1.0893\n",
      "Student pre-training Epoch 114: avg loss = 1.0917\n",
      "Student pre-training Epoch 115: avg loss = 1.0932\n",
      "Student pre-training Epoch 116: avg loss = 1.0792\n",
      "Student pre-training Epoch 117: avg loss = 1.0791\n",
      "Student pre-training Epoch 118: avg loss = 1.0740\n",
      "Student pre-training Epoch 119: avg loss = 1.0715\n",
      "Student pre-training Epoch 120: avg loss = 1.0692\n",
      "Student pre-training Epoch 121: avg loss = 1.0640\n",
      "Student pre-training Epoch 122: avg loss = 1.0623\n",
      "Student pre-training Epoch 123: avg loss = 1.0813\n",
      "Student pre-training Epoch 124: avg loss = 1.0634\n",
      "Student pre-training Epoch 125: avg loss = 1.0548\n",
      "Student pre-training Epoch 126: avg loss = 1.0407\n",
      "Student pre-training Epoch 127: avg loss = 1.0493\n",
      "Student pre-training Epoch 128: avg loss = 1.0560\n",
      "Student pre-training Epoch 129: avg loss = 1.0328\n",
      "Student pre-training Epoch 130: avg loss = 1.0338\n",
      "Student pre-training Epoch 131: avg loss = 1.0444\n",
      "Student pre-training Epoch 132: avg loss = 1.0352\n",
      "Student pre-training Epoch 133: avg loss = 1.0321\n",
      "Student pre-training Epoch 134: avg loss = 1.0362\n",
      "Student pre-training Epoch 135: avg loss = 1.0330\n",
      "Student pre-training Epoch 136: avg loss = 1.0346\n",
      "Student pre-training Epoch 137: avg loss = 1.0290\n",
      "Student pre-training Epoch 138: avg loss = 1.0090\n",
      "Student pre-training Epoch 139: avg loss = 1.0202\n",
      "Student pre-training Epoch 140: avg loss = 1.0123\n",
      "Student pre-training Epoch 141: avg loss = 1.0019\n",
      "Student pre-training Epoch 142: avg loss = 1.0084\n",
      "Student pre-training Epoch 143: avg loss = 1.0095\n",
      "Student pre-training Epoch 144: avg loss = 1.0061\n",
      "Student pre-training Epoch 145: avg loss = 1.0013\n",
      "Student pre-training Epoch 146: avg loss = 1.0069\n",
      "Student pre-training Epoch 147: avg loss = 1.0060\n",
      "Student pre-training Epoch 148: avg loss = 0.9898\n",
      "Student pre-training Epoch 149: avg loss = 1.0077\n",
      "Student pre-training Epoch 150: avg loss = 0.9929\n",
      "Student pre-training Epoch 151: avg loss = 0.9944\n",
      "Student pre-training Epoch 152: avg loss = 0.9959\n",
      "Student pre-training Epoch 153: avg loss = 0.9825\n",
      "Student pre-training Epoch 154: avg loss = 0.9887\n",
      "Student pre-training Epoch 155: avg loss = 0.9901\n",
      "Student pre-training Epoch 156: avg loss = 0.9896\n",
      "Student pre-training Epoch 157: avg loss = 0.9837\n",
      "Student pre-training Epoch 158: avg loss = 0.9800\n",
      "Student pre-training Epoch 159: avg loss = 0.9911\n",
      "Student pre-training Epoch 160: avg loss = 0.9780\n",
      "Student pre-training Epoch 161: avg loss = 0.9674\n",
      "Student pre-training Epoch 162: avg loss = 0.9672\n",
      "Student pre-training Epoch 163: avg loss = 0.9695\n",
      "Student pre-training Epoch 164: avg loss = 0.9645\n",
      "Student pre-training Epoch 165: avg loss = 0.9777\n",
      "Student pre-training Epoch 166: avg loss = 0.9466\n",
      "Student pre-training Epoch 167: avg loss = 0.9648\n",
      "Student pre-training Epoch 168: avg loss = 0.9553\n",
      "Student pre-training Epoch 169: avg loss = 0.9630\n",
      "Student pre-training Epoch 170: avg loss = 0.9532\n",
      "Student pre-training Epoch 171: avg loss = 0.9544\n",
      "Student pre-training Epoch 172: avg loss = 0.9538\n",
      "Student pre-training Epoch 173: avg loss = 0.9561\n",
      "Student pre-training Epoch 174: avg loss = 0.9451\n",
      "Student pre-training Epoch 175: avg loss = 0.9540\n",
      "Student pre-training Epoch 176: avg loss = 0.9555\n",
      "Student pre-training Epoch 177: avg loss = 0.9513\n",
      "Student pre-training Epoch 178: avg loss = 0.9503\n",
      "Student pre-training Epoch 179: avg loss = 0.9427\n",
      "Student pre-training Epoch 180: avg loss = 0.9371\n",
      "Student pre-training Epoch 181: avg loss = 0.9435\n",
      "Student pre-training Epoch 182: avg loss = 0.9550\n",
      "Student pre-training Epoch 183: avg loss = 0.9542\n",
      "Student pre-training Epoch 184: avg loss = 0.9376\n",
      "Student pre-training Epoch 185: avg loss = 0.9282\n",
      "Student pre-training Epoch 186: avg loss = 0.9378\n",
      "Student pre-training Epoch 187: avg loss = 0.9483\n",
      "Student pre-training Epoch 188: avg loss = 0.9231\n",
      "Student pre-training Epoch 189: avg loss = 0.9285\n",
      "Student pre-training Epoch 190: avg loss = 0.9303\n",
      "Student pre-training Epoch 191: avg loss = 0.9360\n",
      "Student pre-training Epoch 192: avg loss = 0.9293\n",
      "Student pre-training Epoch 193: avg loss = 0.9165\n",
      "Student pre-training Epoch 194: avg loss = 0.9231\n",
      "Student pre-training Epoch 195: avg loss = 0.9307\n",
      "Student pre-training Epoch 196: avg loss = 0.9283\n",
      "Student pre-training Epoch 197: avg loss = 0.9117\n",
      "Student pre-training Epoch 198: avg loss = 0.9194\n",
      "Student pre-training Epoch 199: avg loss = 0.9085\n",
      "Student pre-training Epoch 200: avg loss = 0.9262\n",
      "Student pre-training Epoch 201: avg loss = 0.9006\n",
      "Student pre-training Epoch 202: avg loss = 0.9070\n",
      "Student pre-training Epoch 203: avg loss = 0.9073\n",
      "Student pre-training Epoch 204: avg loss = 0.9114\n",
      "Student pre-training Epoch 205: avg loss = 0.9088\n",
      "Student pre-training Epoch 206: avg loss = 0.9099\n",
      "Student pre-training Epoch 207: avg loss = 0.9076\n",
      "Student pre-training Epoch 208: avg loss = 0.9066\n",
      "Student pre-training Epoch 209: avg loss = 0.8899\n",
      "Student pre-training Epoch 210: avg loss = 0.9088\n",
      "Student pre-training Epoch 211: avg loss = 0.9072\n",
      "Student pre-training Epoch 212: avg loss = 0.9038\n",
      "Student pre-training Epoch 213: avg loss = 0.9019\n",
      "Student pre-training Epoch 214: avg loss = 0.8967\n",
      "Student pre-training Epoch 215: avg loss = 0.9013\n",
      "Student pre-training Epoch 216: avg loss = 0.9056\n",
      "Student pre-training Epoch 217: avg loss = 0.8946\n",
      "Student pre-training Epoch 218: avg loss = 0.8976\n",
      "Student pre-training Epoch 219: avg loss = 0.8850\n",
      "Student pre-training Epoch 220: avg loss = 0.8847\n",
      "Student pre-training Epoch 221: avg loss = 0.8948\n",
      "Student pre-training Epoch 222: avg loss = 0.8962\n",
      "Student pre-training Epoch 223: avg loss = 0.8962\n",
      "Student pre-training Epoch 224: avg loss = 0.8923\n",
      "Student pre-training Epoch 225: avg loss = 0.8765\n",
      "Student pre-training Epoch 226: avg loss = 0.8931\n",
      "Student pre-training Epoch 227: avg loss = 0.8953\n",
      "Student pre-training Epoch 228: avg loss = 0.8800\n",
      "Student pre-training Epoch 229: avg loss = 0.8928\n",
      "Student pre-training Epoch 230: avg loss = 0.8743\n",
      "Student pre-training Epoch 231: avg loss = 0.8878\n",
      "Student pre-training Epoch 232: avg loss = 0.8940\n",
      "Student pre-training Epoch 233: avg loss = 0.8847\n",
      "Student pre-training Epoch 234: avg loss = 0.8787\n",
      "Student pre-training Epoch 235: avg loss = 0.8724\n",
      "Student pre-training Epoch 236: avg loss = 0.8873\n",
      "Student pre-training Epoch 237: avg loss = 0.8827\n",
      "Student pre-training Epoch 238: avg loss = 0.8665\n",
      "Student pre-training Epoch 239: avg loss = 0.8731\n",
      "Student pre-training Epoch 240: avg loss = 0.8787\n",
      "Student pre-training Epoch 241: avg loss = 0.8805\n",
      "Student pre-training Epoch 242: avg loss = 0.8666\n",
      "Student pre-training Epoch 243: avg loss = 0.8687\n",
      "Student pre-training Epoch 244: avg loss = 0.8823\n",
      "Student pre-training Epoch 245: avg loss = 0.8680\n",
      "Student pre-training Epoch 246: avg loss = 0.8827\n",
      "Student pre-training Epoch 247: avg loss = 0.8639\n",
      "Student pre-training Epoch 248: avg loss = 0.8730\n",
      "Student pre-training Epoch 249: avg loss = 0.8525\n",
      "Student pre-training Epoch 250: avg loss = 0.8630\n",
      "Student pre-training Epoch 251: avg loss = 0.8674\n",
      "Student pre-training Epoch 252: avg loss = 0.8603\n",
      "Student pre-training Epoch 253: avg loss = 0.8680\n",
      "Student pre-training Epoch 254: avg loss = 0.8671\n",
      "Student pre-training Epoch 255: avg loss = 0.8635\n",
      "Student pre-training Epoch 256: avg loss = 0.8649\n",
      "Student pre-training Epoch 257: avg loss = 0.8564\n",
      "Student pre-training Epoch 258: avg loss = 0.8453\n",
      "Student pre-training Epoch 259: avg loss = 0.8714\n",
      "Student pre-training Epoch 260: avg loss = 0.8564\n",
      "Student pre-training Epoch 261: avg loss = 0.8524\n",
      "Student pre-training Epoch 262: avg loss = 0.8577\n",
      "Student pre-training Epoch 263: avg loss = 0.8552\n",
      "Student pre-training Epoch 264: avg loss = 0.8382\n",
      "Student pre-training Epoch 265: avg loss = 0.8613\n",
      "Student pre-training Epoch 266: avg loss = 0.8485\n",
      "Student pre-training Epoch 267: avg loss = 0.8589\n",
      "Student pre-training Epoch 268: avg loss = 0.8491\n",
      "Student pre-training Epoch 269: avg loss = 0.8567\n",
      "Student pre-training Epoch 270: avg loss = 0.8447\n",
      "Student pre-training Epoch 271: avg loss = 0.8508\n",
      "Student pre-training Epoch 272: avg loss = 0.8438\n",
      "Student pre-training Epoch 273: avg loss = 0.8564\n",
      "Student pre-training Epoch 274: avg loss = 0.8484\n",
      "Student pre-training Epoch 275: avg loss = 0.8454\n",
      "Student pre-training Epoch 276: avg loss = 0.8489\n",
      "Student pre-training Epoch 277: avg loss = 0.8330\n",
      "Student pre-training Epoch 278: avg loss = 0.8434\n",
      "Student pre-training Epoch 279: avg loss = 0.8482\n",
      "Student pre-training Epoch 280: avg loss = 0.8459\n",
      "Student pre-training Epoch 281: avg loss = 0.8396\n",
      "Student pre-training Epoch 282: avg loss = 0.8354\n",
      "Student pre-training Epoch 283: avg loss = 0.8537\n",
      "Student pre-training Epoch 284: avg loss = 0.8528\n",
      "Student pre-training Epoch 285: avg loss = 0.8537\n",
      "Student pre-training Epoch 286: avg loss = 0.8490\n",
      "Student pre-training Epoch 287: avg loss = 0.8368\n",
      "Student pre-training Epoch 288: avg loss = 0.8343\n",
      "Student pre-training Epoch 289: avg loss = 0.8491\n",
      "Student pre-training Epoch 290: avg loss = 0.8253\n",
      "Student pre-training Epoch 291: avg loss = 0.8332\n",
      "Student pre-training Epoch 292: avg loss = 0.8405\n",
      "Student pre-training Epoch 293: avg loss = 0.8375\n",
      "Student pre-training Epoch 294: avg loss = 0.8397\n",
      "Student pre-training Epoch 295: avg loss = 0.8373\n",
      "Student pre-training Epoch 296: avg loss = 0.8418\n",
      "Student pre-training Epoch 297: avg loss = 0.8251\n",
      "Student pre-training Epoch 298: avg loss = 0.8174\n",
      "Student pre-training Epoch 299: avg loss = 0.8376\n",
      "Student pre-training Epoch 300: avg loss = 0.8262\n",
      "Student pre-training Epoch 301: avg loss = 0.8182\n",
      "Student pre-training Epoch 302: avg loss = 0.8322\n",
      "Student pre-training Epoch 303: avg loss = 0.8150\n",
      "Student pre-training Epoch 304: avg loss = 0.8175\n",
      "Student pre-training Epoch 305: avg loss = 0.8342\n",
      "Student pre-training Epoch 306: avg loss = 0.8213\n",
      "Student pre-training Epoch 307: avg loss = 0.8239\n",
      "Student pre-training Epoch 308: avg loss = 0.8322\n",
      "Student pre-training Epoch 309: avg loss = 0.8236\n",
      "Student pre-training Epoch 310: avg loss = 0.8216\n",
      "Student pre-training Epoch 311: avg loss = 0.8232\n",
      "Student pre-training Epoch 312: avg loss = 0.8333\n",
      "Student pre-training Epoch 313: avg loss = 0.8275\n",
      "Student pre-training Epoch 314: avg loss = 0.8207\n",
      "Student pre-training Epoch 315: avg loss = 0.8246\n",
      "Student pre-training Epoch 316: avg loss = 0.8245\n",
      "Student pre-training Epoch 317: avg loss = 0.8116\n",
      "Student pre-training Epoch 318: avg loss = 0.8230\n",
      "Student pre-training Epoch 319: avg loss = 0.8208\n",
      "Student pre-training Epoch 320: avg loss = 0.8231\n",
      "Student pre-training Epoch 321: avg loss = 0.8279\n",
      "Student pre-training Epoch 322: avg loss = 0.8039\n",
      "Student pre-training Epoch 323: avg loss = 0.8056\n",
      "Student pre-training Epoch 324: avg loss = 0.8198\n",
      "Student pre-training Epoch 325: avg loss = 0.8122\n",
      "Student pre-training Epoch 326: avg loss = 0.8092\n",
      "Student pre-training Epoch 327: avg loss = 0.8208\n",
      "Student pre-training Epoch 328: avg loss = 0.8008\n",
      "Student pre-training Epoch 329: avg loss = 0.8091\n",
      "Student pre-training Epoch 330: avg loss = 0.8127\n",
      "Student pre-training Epoch 331: avg loss = 0.8210\n",
      "Student pre-training Epoch 332: avg loss = 0.8140\n",
      "Student pre-training Epoch 333: avg loss = 0.8009\n",
      "Student pre-training Epoch 334: avg loss = 0.8113\n",
      "Student pre-training Epoch 335: avg loss = 0.8171\n",
      "Student pre-training Epoch 336: avg loss = 0.8091\n",
      "Student pre-training Epoch 337: avg loss = 0.8065\n",
      "Student pre-training Epoch 338: avg loss = 0.7963\n",
      "Student pre-training Epoch 339: avg loss = 0.7985\n",
      "Student pre-training Epoch 340: avg loss = 0.8014\n",
      "Student pre-training Epoch 341: avg loss = 0.8057\n",
      "Student pre-training Epoch 342: avg loss = 0.7955\n",
      "Student pre-training Epoch 343: avg loss = 0.8039\n",
      "Student pre-training Epoch 344: avg loss = 0.7898\n",
      "Student pre-training Epoch 345: avg loss = 0.7956\n",
      "Student pre-training Epoch 346: avg loss = 0.7961\n",
      "Student pre-training Epoch 347: avg loss = 0.8014\n",
      "Student pre-training Epoch 348: avg loss = 0.8058\n",
      "Student pre-training Epoch 349: avg loss = 0.7997\n",
      "Student pre-training Epoch 350: avg loss = 0.8001\n",
      "Student pre-training Epoch 351: avg loss = 0.8062\n",
      "Student pre-training Epoch 352: avg loss = 0.7945\n",
      "Student pre-training Epoch 353: avg loss = 0.7921\n",
      "Student pre-training Epoch 354: avg loss = 0.7926\n",
      "Student pre-training Epoch 355: avg loss = 0.8063\n",
      "Student pre-training Epoch 356: avg loss = 0.7830\n",
      "Student pre-training Epoch 357: avg loss = 0.7943\n",
      "Student pre-training Epoch 358: avg loss = 0.7805\n",
      "Student pre-training Epoch 359: avg loss = 0.8016\n",
      "Student pre-training Epoch 360: avg loss = 0.7858\n",
      "Student pre-training Epoch 361: avg loss = 0.8011\n",
      "Student pre-training Epoch 362: avg loss = 0.7930\n",
      "Student pre-training Epoch 363: avg loss = 0.7931\n",
      "Student pre-training Epoch 364: avg loss = 0.7919\n",
      "Student pre-training Epoch 365: avg loss = 0.7964\n",
      "Student pre-training Epoch 366: avg loss = 0.7937\n",
      "Student pre-training Epoch 367: avg loss = 0.8001\n",
      "Student pre-training Epoch 368: avg loss = 0.7937\n",
      "Student pre-training Epoch 369: avg loss = 0.7921\n",
      "Student pre-training Epoch 370: avg loss = 0.7843\n",
      "Student pre-training Epoch 371: avg loss = 0.7882\n",
      "Student pre-training Epoch 372: avg loss = 0.7781\n",
      "Student pre-training Epoch 373: avg loss = 0.7881\n",
      "Student pre-training Epoch 374: avg loss = 0.7865\n",
      "Student pre-training Epoch 375: avg loss = 0.7907\n",
      "Student pre-training Epoch 376: avg loss = 0.7820\n",
      "Student pre-training Epoch 377: avg loss = 0.7802\n",
      "Student pre-training Epoch 378: avg loss = 0.7767\n",
      "Student pre-training Epoch 379: avg loss = 0.7771\n",
      "Student pre-training Epoch 380: avg loss = 0.7780\n",
      "Student pre-training Epoch 381: avg loss = 0.7864\n",
      "Student pre-training Epoch 382: avg loss = 0.7770\n",
      "Student pre-training Epoch 383: avg loss = 0.7979\n",
      "Student pre-training Epoch 384: avg loss = 0.7824\n",
      "Student pre-training Epoch 385: avg loss = 0.7776\n",
      "Student pre-training Epoch 386: avg loss = 0.7730\n",
      "Student pre-training Epoch 387: avg loss = 0.7845\n",
      "Student pre-training Epoch 388: avg loss = 0.7714\n",
      "Student pre-training Epoch 389: avg loss = 0.7915\n",
      "Student pre-training Epoch 390: avg loss = 0.7758\n",
      "Student pre-training Epoch 391: avg loss = 0.7920\n",
      "Student pre-training Epoch 392: avg loss = 0.7692\n",
      "Student pre-training Epoch 393: avg loss = 0.7726\n",
      "Student pre-training Epoch 394: avg loss = 0.7681\n",
      "Student pre-training Epoch 395: avg loss = 0.7776\n",
      "Student pre-training Epoch 396: avg loss = 0.7835\n",
      "Student pre-training Epoch 397: avg loss = 0.7780\n",
      "Student pre-training Epoch 398: avg loss = 0.7844\n",
      "Student pre-training Epoch 399: avg loss = 0.7701\n",
      "Student pre-training Epoch 400: avg loss = 0.7834\n",
      "Student pre-training Epoch 401: avg loss = 0.7627\n",
      "Student pre-training Epoch 402: avg loss = 0.7695\n",
      "Student pre-training Epoch 403: avg loss = 0.7691\n",
      "Student pre-training Epoch 404: avg loss = 0.7713\n",
      "Student pre-training Epoch 405: avg loss = 0.7768\n",
      "Student pre-training Epoch 406: avg loss = 0.7543\n",
      "Student pre-training Epoch 407: avg loss = 0.7715\n",
      "Student pre-training Epoch 408: avg loss = 0.7678\n",
      "Student pre-training Epoch 409: avg loss = 0.7800\n",
      "Student pre-training Epoch 410: avg loss = 0.7784\n",
      "Student pre-training Epoch 411: avg loss = 0.7763\n",
      "Student pre-training Epoch 412: avg loss = 0.7689\n",
      "Student pre-training Epoch 413: avg loss = 0.7762\n",
      "Student pre-training Epoch 414: avg loss = 0.7726\n",
      "Student pre-training Epoch 415: avg loss = 0.7753\n",
      "Student pre-training Epoch 416: avg loss = 0.7717\n",
      "Student pre-training Epoch 417: avg loss = 0.7688\n",
      "Student pre-training Epoch 418: avg loss = 0.7663\n",
      "Student pre-training Epoch 419: avg loss = 0.7648\n",
      "Student pre-training Epoch 420: avg loss = 0.7730\n",
      "Student pre-training Epoch 421: avg loss = 0.7669\n",
      "Student pre-training Epoch 422: avg loss = 0.7692\n",
      "Student pre-training Epoch 423: avg loss = 0.7536\n",
      "Student pre-training Epoch 424: avg loss = 0.7710\n",
      "Student pre-training Epoch 425: avg loss = 0.7618\n",
      "Student pre-training Epoch 426: avg loss = 0.7795\n",
      "Student pre-training Epoch 427: avg loss = 0.7690\n",
      "Student pre-training Epoch 428: avg loss = 0.7633\n",
      "Student pre-training Epoch 429: avg loss = 0.7618\n",
      "Student pre-training Epoch 430: avg loss = 0.7544\n",
      "Student pre-training Epoch 431: avg loss = 0.7522\n",
      "Student pre-training Epoch 432: avg loss = 0.7600\n",
      "Student pre-training Epoch 433: avg loss = 0.7684\n",
      "Student pre-training Epoch 434: avg loss = 0.7628\n",
      "Student pre-training Epoch 435: avg loss = 0.7559\n",
      "Student pre-training Epoch 436: avg loss = 0.7612\n",
      "Student pre-training Epoch 437: avg loss = 0.7584\n",
      "Student pre-training Epoch 438: avg loss = 0.7637\n",
      "Student pre-training Epoch 439: avg loss = 0.7456\n",
      "Student pre-training Epoch 440: avg loss = 0.7522\n",
      "Student pre-training Epoch 441: avg loss = 0.7476\n",
      "Student pre-training Epoch 442: avg loss = 0.7458\n",
      "Student pre-training Epoch 443: avg loss = 0.7529\n",
      "Student pre-training Epoch 444: avg loss = 0.7559\n",
      "Student pre-training Epoch 445: avg loss = 0.7678\n",
      "Student pre-training Epoch 446: avg loss = 0.7584\n",
      "Student pre-training Epoch 447: avg loss = 0.7520\n",
      "Student pre-training Epoch 448: avg loss = 0.7471\n",
      "Student pre-training Epoch 449: avg loss = 0.7644\n",
      "Student pre-training Epoch 450: avg loss = 0.7525\n",
      "Student pre-training Epoch 451: avg loss = 0.7547\n",
      "Student pre-training Epoch 452: avg loss = 0.7569\n",
      "Student pre-training Epoch 453: avg loss = 0.7542\n",
      "Student pre-training Epoch 454: avg loss = 0.7463\n",
      "Student pre-training Epoch 455: avg loss = 0.7556\n",
      "Student pre-training Epoch 456: avg loss = 0.7516\n",
      "Student pre-training Epoch 457: avg loss = 0.7413\n",
      "Student pre-training Epoch 458: avg loss = 0.7334\n",
      "Student pre-training Epoch 459: avg loss = 0.7421\n",
      "Student pre-training Epoch 460: avg loss = 0.7631\n",
      "Student pre-training Epoch 461: avg loss = 0.7578\n",
      "Student pre-training Epoch 462: avg loss = 0.7383\n",
      "Student pre-training Epoch 463: avg loss = 0.7423\n",
      "Student pre-training Epoch 464: avg loss = 0.7458\n",
      "Student pre-training Epoch 465: avg loss = 0.7492\n",
      "Student pre-training Epoch 466: avg loss = 0.7402\n",
      "Student pre-training Epoch 467: avg loss = 0.7312\n",
      "Student pre-training Epoch 468: avg loss = 0.7403\n",
      "Student pre-training Epoch 469: avg loss = 0.7448\n",
      "Student pre-training Epoch 470: avg loss = 0.7371\n",
      "Student pre-training Epoch 471: avg loss = 0.7504\n",
      "Student pre-training Epoch 472: avg loss = 0.7397\n",
      "Student pre-training Epoch 473: avg loss = 0.7368\n",
      "Student pre-training Epoch 474: avg loss = 0.7473\n",
      "Student pre-training Epoch 475: avg loss = 0.7463\n",
      "Student pre-training Epoch 476: avg loss = 0.7309\n",
      "Student pre-training Epoch 477: avg loss = 0.7446\n",
      "Student pre-training Epoch 478: avg loss = 0.7374\n",
      "Student pre-training Epoch 479: avg loss = 0.7330\n",
      "Student pre-training Epoch 480: avg loss = 0.7444\n",
      "Student pre-training Epoch 481: avg loss = 0.7395\n",
      "Student pre-training Epoch 482: avg loss = 0.7548\n",
      "Student pre-training Epoch 483: avg loss = 0.7260\n",
      "Student pre-training Epoch 484: avg loss = 0.7448\n",
      "Student pre-training Epoch 485: avg loss = 0.7365\n",
      "Student pre-training Epoch 486: avg loss = 0.7249\n",
      "Student pre-training Epoch 487: avg loss = 0.7255\n",
      "Student pre-training Epoch 488: avg loss = 0.7371\n",
      "Student pre-training Epoch 489: avg loss = 0.7350\n",
      "Student pre-training Epoch 490: avg loss = 0.7219\n",
      "Student pre-training Epoch 491: avg loss = 0.7349\n",
      "Student pre-training Epoch 492: avg loss = 0.7311\n",
      "Student pre-training Epoch 493: avg loss = 0.7427\n",
      "Student pre-training Epoch 494: avg loss = 0.7306\n",
      "Student pre-training Epoch 495: avg loss = 0.7270\n",
      "Student pre-training Epoch 496: avg loss = 0.7271\n",
      "Student pre-training Epoch 497: avg loss = 0.7299\n",
      "Student pre-training Epoch 498: avg loss = 0.7256\n",
      "Student pre-training Epoch 499: avg loss = 0.7212\n",
      "Student pre-training Epoch 500: avg loss = 0.7256\n",
      "Student pre-training Epoch 501: avg loss = 0.7500\n",
      "Student pre-training Epoch 502: avg loss = 0.7319\n",
      "Student pre-training Epoch 503: avg loss = 0.7348\n",
      "Student pre-training Epoch 504: avg loss = 0.7175\n",
      "Student pre-training Epoch 505: avg loss = 0.7149\n",
      "Student pre-training Epoch 506: avg loss = 0.7273\n",
      "Student pre-training Epoch 507: avg loss = 0.7224\n",
      "Student pre-training Epoch 508: avg loss = 0.7366\n",
      "Student pre-training Epoch 509: avg loss = 0.7260\n",
      "Student pre-training Epoch 510: avg loss = 0.7281\n",
      "Student pre-training Epoch 511: avg loss = 0.7287\n",
      "Student pre-training Epoch 512: avg loss = 0.7304\n",
      "Student pre-training Epoch 513: avg loss = 0.7294\n",
      "Student pre-training Epoch 514: avg loss = 0.7257\n",
      "Student pre-training Epoch 515: avg loss = 0.7275\n",
      "Student pre-training Epoch 516: avg loss = 0.7140\n",
      "Student pre-training Epoch 517: avg loss = 0.7294\n",
      "Student pre-training Epoch 518: avg loss = 0.7300\n",
      "Student pre-training Epoch 519: avg loss = 0.7273\n",
      "Student pre-training Epoch 520: avg loss = 0.7208\n",
      "Student pre-training Epoch 521: avg loss = 0.7227\n",
      "Student pre-training Epoch 522: avg loss = 0.7254\n",
      "Student pre-training Epoch 523: avg loss = 0.7130\n",
      "Student pre-training Epoch 524: avg loss = 0.7146\n",
      "Student pre-training Epoch 525: avg loss = 0.7235\n",
      "Student pre-training Epoch 526: avg loss = 0.7062\n",
      "Student pre-training Epoch 527: avg loss = 0.7117\n",
      "Student pre-training Epoch 528: avg loss = 0.7102\n",
      "Student pre-training Epoch 529: avg loss = 0.7183\n",
      "Student pre-training Epoch 530: avg loss = 0.7194\n",
      "Student pre-training Epoch 531: avg loss = 0.7148\n",
      "Student pre-training Epoch 532: avg loss = 0.7114\n",
      "Student pre-training Epoch 533: avg loss = 0.7101\n",
      "Student pre-training Epoch 534: avg loss = 0.7195\n",
      "Student pre-training Epoch 535: avg loss = 0.7202\n",
      "Student pre-training Epoch 536: avg loss = 0.7158\n",
      "Student pre-training Epoch 537: avg loss = 0.7173\n",
      "Student pre-training Epoch 538: avg loss = 0.7050\n",
      "Student pre-training Epoch 539: avg loss = 0.7031\n",
      "Student pre-training Epoch 540: avg loss = 0.7211\n",
      "Student pre-training Epoch 541: avg loss = 0.7118\n",
      "Student pre-training Epoch 542: avg loss = 0.7169\n",
      "Student pre-training Epoch 543: avg loss = 0.7112\n",
      "Student pre-training Epoch 544: avg loss = 0.7040\n",
      "Student pre-training Epoch 545: avg loss = 0.7066\n",
      "Student pre-training Epoch 546: avg loss = 0.7163\n",
      "Student pre-training Epoch 547: avg loss = 0.7091\n",
      "Student pre-training Epoch 548: avg loss = 0.7107\n",
      "Student pre-training Epoch 549: avg loss = 0.7204\n",
      "Student pre-training Epoch 550: avg loss = 0.7206\n",
      "Student pre-training Epoch 551: avg loss = 0.7059\n",
      "Student pre-training Epoch 552: avg loss = 0.7007\n",
      "Student pre-training Epoch 553: avg loss = 0.7081\n",
      "Student pre-training Epoch 554: avg loss = 0.7129\n",
      "Student pre-training Epoch 555: avg loss = 0.6919\n",
      "Student pre-training Epoch 556: avg loss = 0.7046\n",
      "Student pre-training Epoch 557: avg loss = 0.7044\n",
      "Student pre-training Epoch 558: avg loss = 0.7003\n",
      "Student pre-training Epoch 559: avg loss = 0.7057\n",
      "Student pre-training Epoch 560: avg loss = 0.7091\n",
      "Student pre-training Epoch 561: avg loss = 0.7091\n",
      "Student pre-training Epoch 562: avg loss = 0.6934\n",
      "Student pre-training Epoch 563: avg loss = 0.7089\n",
      "Student pre-training Epoch 564: avg loss = 0.6940\n",
      "Student pre-training Epoch 565: avg loss = 0.6972\n",
      "Student pre-training Epoch 566: avg loss = 0.7000\n",
      "Student pre-training Epoch 567: avg loss = 0.6982\n",
      "Student pre-training Epoch 568: avg loss = 0.6986\n",
      "Student pre-training Epoch 569: avg loss = 0.6997\n",
      "Student pre-training Epoch 570: avg loss = 0.6982\n",
      "Student pre-training Epoch 571: avg loss = 0.6955\n",
      "Student pre-training Epoch 572: avg loss = 0.6937\n",
      "Student pre-training Epoch 573: avg loss = 0.6932\n",
      "Student pre-training Epoch 574: avg loss = 0.7166\n",
      "Student pre-training Epoch 575: avg loss = 0.6905\n",
      "Student pre-training Epoch 576: avg loss = 0.7000\n",
      "Student pre-training Epoch 577: avg loss = 0.7059\n",
      "Student pre-training Epoch 578: avg loss = 0.6928\n",
      "Student pre-training Epoch 579: avg loss = 0.6878\n",
      "Student pre-training Epoch 580: avg loss = 0.7044\n",
      "Student pre-training Epoch 581: avg loss = 0.7025\n",
      "Student pre-training Epoch 582: avg loss = 0.6857\n",
      "Student pre-training Epoch 583: avg loss = 0.6822\n",
      "Student pre-training Epoch 584: avg loss = 0.6857\n",
      "Student pre-training Epoch 585: avg loss = 0.7052\n",
      "Student pre-training Epoch 586: avg loss = 0.6967\n",
      "Student pre-training Epoch 587: avg loss = 0.7025\n",
      "Student pre-training Epoch 588: avg loss = 0.6864\n",
      "Student pre-training Epoch 589: avg loss = 0.6857\n",
      "Student pre-training Epoch 590: avg loss = 0.6849\n",
      "Student pre-training Epoch 591: avg loss = 0.6957\n",
      "Student pre-training Epoch 592: avg loss = 0.6843\n",
      "Student pre-training Epoch 593: avg loss = 0.6938\n",
      "Student pre-training Epoch 594: avg loss = 0.6888\n",
      "Student pre-training Epoch 595: avg loss = 0.7116\n",
      "Student pre-training Epoch 596: avg loss = 0.6891\n",
      "Student pre-training Epoch 597: avg loss = 0.6946\n",
      "Student pre-training Epoch 598: avg loss = 0.6897\n",
      "Student pre-training Epoch 599: avg loss = 0.6816\n",
      "Student pre-training Epoch 600: avg loss = 0.6862\n",
      "Student pre-training Epoch 601: avg loss = 0.6865\n",
      "Student pre-training Epoch 602: avg loss = 0.6852\n",
      "Student pre-training Epoch 603: avg loss = 0.6801\n",
      "Student pre-training Epoch 604: avg loss = 0.6988\n",
      "Student pre-training Epoch 605: avg loss = 0.6764\n",
      "Student pre-training Epoch 606: avg loss = 0.6804\n",
      "Student pre-training Epoch 607: avg loss = 0.7035\n",
      "Student pre-training Epoch 608: avg loss = 0.6730\n",
      "Student pre-training Epoch 609: avg loss = 0.6913\n",
      "Student pre-training Epoch 610: avg loss = 0.6865\n",
      "Student pre-training Epoch 611: avg loss = 0.6952\n",
      "Student pre-training Epoch 612: avg loss = 0.6904\n",
      "Student pre-training Epoch 613: avg loss = 0.6967\n",
      "Student pre-training Epoch 614: avg loss = 0.6978\n",
      "Student pre-training Epoch 615: avg loss = 0.6907\n",
      "Student pre-training Epoch 616: avg loss = 0.6733\n",
      "Student pre-training Epoch 617: avg loss = 0.6771\n",
      "Student pre-training Epoch 618: avg loss = 0.6808\n",
      "Student pre-training Epoch 619: avg loss = 0.6857\n",
      "Student pre-training Epoch 620: avg loss = 0.6777\n",
      "Student pre-training Epoch 621: avg loss = 0.6810\n",
      "Student pre-training Epoch 622: avg loss = 0.6847\n",
      "Student pre-training Epoch 623: avg loss = 0.6852\n",
      "Student pre-training Epoch 624: avg loss = 0.6860\n",
      "Student pre-training Epoch 625: avg loss = 0.6793\n",
      "Student pre-training Epoch 626: avg loss = 0.6827\n",
      "Student pre-training Epoch 627: avg loss = 0.6782\n",
      "Student pre-training Epoch 628: avg loss = 0.6825\n",
      "Student pre-training Epoch 629: avg loss = 0.6825\n",
      "Student pre-training Epoch 630: avg loss = 0.6899\n",
      "Student pre-training Epoch 631: avg loss = 0.6859\n",
      "Student pre-training Epoch 632: avg loss = 0.6895\n",
      "Student pre-training Epoch 633: avg loss = 0.6767\n",
      "Student pre-training Epoch 634: avg loss = 0.6804\n",
      "Student pre-training Epoch 635: avg loss = 0.6774\n",
      "Student pre-training Epoch 636: avg loss = 0.6726\n",
      "Student pre-training Epoch 637: avg loss = 0.6746\n",
      "Student pre-training Epoch 638: avg loss = 0.6767\n",
      "Student pre-training Epoch 639: avg loss = 0.6612\n",
      "Student pre-training Epoch 640: avg loss = 0.6714\n",
      "Student pre-training Epoch 641: avg loss = 0.6658\n",
      "Student pre-training Epoch 642: avg loss = 0.6708\n",
      "Student pre-training Epoch 643: avg loss = 0.6751\n",
      "Student pre-training Epoch 644: avg loss = 0.6702\n",
      "Student pre-training Epoch 645: avg loss = 0.6750\n",
      "Student pre-training Epoch 646: avg loss = 0.6779\n",
      "Student pre-training Epoch 647: avg loss = 0.6642\n",
      "Student pre-training Epoch 648: avg loss = 0.6702\n",
      "Student pre-training Epoch 649: avg loss = 0.6804\n",
      "Student pre-training Epoch 650: avg loss = 0.6692\n",
      "Student pre-training Epoch 651: avg loss = 0.6600\n",
      "Student pre-training Epoch 652: avg loss = 0.6687\n",
      "Student pre-training Epoch 653: avg loss = 0.6790\n",
      "Student pre-training Epoch 654: avg loss = 0.6612\n",
      "Student pre-training Epoch 655: avg loss = 0.6769\n",
      "Student pre-training Epoch 656: avg loss = 0.6714\n",
      "Student pre-training Epoch 657: avg loss = 0.6483\n",
      "Student pre-training Epoch 658: avg loss = 0.6596\n",
      "Student pre-training Epoch 659: avg loss = 0.6715\n",
      "Student pre-training Epoch 660: avg loss = 0.6689\n",
      "Student pre-training Epoch 661: avg loss = 0.6557\n",
      "Student pre-training Epoch 662: avg loss = 0.6612\n",
      "Student pre-training Epoch 663: avg loss = 0.6699\n",
      "Student pre-training Epoch 664: avg loss = 0.6639\n",
      "Student pre-training Epoch 665: avg loss = 0.6578\n",
      "Student pre-training Epoch 666: avg loss = 0.6700\n",
      "Student pre-training Epoch 667: avg loss = 0.6682\n",
      "Student pre-training Epoch 668: avg loss = 0.6712\n",
      "Student pre-training Epoch 669: avg loss = 0.6516\n",
      "Student pre-training Epoch 670: avg loss = 0.6641\n",
      "Student pre-training Epoch 671: avg loss = 0.6453\n",
      "Student pre-training Epoch 672: avg loss = 0.6622\n",
      "Student pre-training Epoch 673: avg loss = 0.6715\n",
      "Student pre-training Epoch 674: avg loss = 0.6540\n",
      "Student pre-training Epoch 675: avg loss = 0.6644\n",
      "Student pre-training Epoch 676: avg loss = 0.6615\n",
      "Student pre-training Epoch 677: avg loss = 0.6522\n",
      "Student pre-training Epoch 678: avg loss = 0.6597\n",
      "Student pre-training Epoch 679: avg loss = 0.6604\n",
      "Student pre-training Epoch 680: avg loss = 0.6654\n",
      "Student pre-training Epoch 681: avg loss = 0.6392\n",
      "Student pre-training Epoch 682: avg loss = 0.6653\n",
      "Student pre-training Epoch 683: avg loss = 0.6645\n",
      "Student pre-training Epoch 684: avg loss = 0.6645\n",
      "Student pre-training Epoch 685: avg loss = 0.6587\n",
      "Student pre-training Epoch 686: avg loss = 0.6628\n",
      "Student pre-training Epoch 687: avg loss = 0.6619\n",
      "Student pre-training Epoch 688: avg loss = 0.6582\n",
      "Student pre-training Epoch 689: avg loss = 0.6494\n",
      "Student pre-training Epoch 690: avg loss = 0.6526\n",
      "Student pre-training Epoch 691: avg loss = 0.6546\n",
      "Student pre-training Epoch 692: avg loss = 0.6556\n",
      "Student pre-training Epoch 693: avg loss = 0.6572\n",
      "Student pre-training Epoch 694: avg loss = 0.6550\n",
      "Student pre-training Epoch 695: avg loss = 0.6588\n",
      "Student pre-training Epoch 696: avg loss = 0.6487\n",
      "Student pre-training Epoch 697: avg loss = 0.6486\n",
      "Student pre-training Epoch 698: avg loss = 0.6574\n",
      "Student pre-training Epoch 699: avg loss = 0.6554\n",
      "Student pre-training Epoch 700: avg loss = 0.6589\n",
      "Student pre-training Epoch 701: avg loss = 0.6513\n",
      "Student pre-training Epoch 702: avg loss = 0.6394\n",
      "Student pre-training Epoch 703: avg loss = 0.6351\n",
      "Student pre-training Epoch 704: avg loss = 0.6541\n",
      "Student pre-training Epoch 705: avg loss = 0.6483\n",
      "Student pre-training Epoch 706: avg loss = 0.6488\n",
      "Student pre-training Epoch 707: avg loss = 0.6586\n",
      "Student pre-training Epoch 708: avg loss = 0.6467\n",
      "Student pre-training Epoch 709: avg loss = 0.6412\n",
      "Student pre-training Epoch 710: avg loss = 0.6543\n",
      "Student pre-training Epoch 711: avg loss = 0.6426\n",
      "Student pre-training Epoch 712: avg loss = 0.6464\n",
      "Student pre-training Epoch 713: avg loss = 0.6467\n",
      "Student pre-training Epoch 714: avg loss = 0.6342\n",
      "Student pre-training Epoch 715: avg loss = 0.6444\n",
      "Student pre-training Epoch 716: avg loss = 0.6482\n",
      "Student pre-training Epoch 717: avg loss = 0.6436\n",
      "Student pre-training Epoch 718: avg loss = 0.6481\n",
      "Student pre-training Epoch 719: avg loss = 0.6376\n",
      "Student pre-training Epoch 720: avg loss = 0.6567\n",
      "Student pre-training Epoch 721: avg loss = 0.6355\n",
      "Student pre-training Epoch 722: avg loss = 0.6426\n",
      "Student pre-training Epoch 723: avg loss = 0.6385\n",
      "Student pre-training Epoch 724: avg loss = 0.6343\n",
      "Student pre-training Epoch 725: avg loss = 0.6650\n",
      "Student pre-training Epoch 726: avg loss = 0.6519\n",
      "Student pre-training Epoch 727: avg loss = 0.6537\n",
      "Student pre-training Epoch 728: avg loss = 0.6386\n",
      "Student pre-training Epoch 729: avg loss = 0.6466\n",
      "Student pre-training Epoch 730: avg loss = 0.6453\n",
      "Student pre-training Epoch 731: avg loss = 0.6395\n",
      "Student pre-training Epoch 732: avg loss = 0.6463\n",
      "Student pre-training Epoch 733: avg loss = 0.6414\n",
      "Student pre-training Epoch 734: avg loss = 0.6379\n",
      "Student pre-training Epoch 735: avg loss = 0.6330\n",
      "Student pre-training Epoch 736: avg loss = 0.6420\n",
      "Student pre-training Epoch 737: avg loss = 0.6441\n",
      "Student pre-training Epoch 738: avg loss = 0.6330\n",
      "Student pre-training Epoch 739: avg loss = 0.6360\n",
      "Student pre-training Epoch 740: avg loss = 0.6297\n",
      "Student pre-training Epoch 741: avg loss = 0.6364\n",
      "Student pre-training Epoch 742: avg loss = 0.6313\n",
      "Student pre-training Epoch 743: avg loss = 0.6412\n",
      "Student pre-training Epoch 744: avg loss = 0.6502\n",
      "Student pre-training Epoch 745: avg loss = 0.6427\n",
      "Student pre-training Epoch 746: avg loss = 0.6261\n",
      "Student pre-training Epoch 747: avg loss = 0.6291\n",
      "Student pre-training Epoch 748: avg loss = 0.6316\n",
      "Student pre-training Epoch 749: avg loss = 0.6397\n",
      "Student pre-training Epoch 750: avg loss = 0.6348\n",
      "Student pre-training Epoch 751: avg loss = 0.6315\n",
      "Student pre-training Epoch 752: avg loss = 0.6476\n",
      "Student pre-training Epoch 753: avg loss = 0.6275\n",
      "Student pre-training Epoch 754: avg loss = 0.6294\n",
      "Student pre-training Epoch 755: avg loss = 0.6354\n",
      "Student pre-training Epoch 756: avg loss = 0.6331\n",
      "Student pre-training Epoch 757: avg loss = 0.6483\n",
      "Student pre-training Epoch 758: avg loss = 0.6301\n",
      "Student pre-training Epoch 759: avg loss = 0.6359\n",
      "Student pre-training Epoch 760: avg loss = 0.6353\n",
      "Student pre-training Epoch 761: avg loss = 0.6312\n",
      "Student pre-training Epoch 762: avg loss = 0.6335\n",
      "Student pre-training Epoch 763: avg loss = 0.6299\n",
      "Student pre-training Epoch 764: avg loss = 0.6317\n",
      "Student pre-training Epoch 765: avg loss = 0.6255\n",
      "Student pre-training Epoch 766: avg loss = 0.6316\n",
      "Student pre-training Epoch 767: avg loss = 0.6395\n",
      "Student pre-training Epoch 768: avg loss = 0.6237\n",
      "Student pre-training Epoch 769: avg loss = 0.6362\n",
      "Student pre-training Epoch 770: avg loss = 0.6273\n",
      "Student pre-training Epoch 771: avg loss = 0.6293\n",
      "Student pre-training Epoch 772: avg loss = 0.6270\n",
      "Student pre-training Epoch 773: avg loss = 0.6243\n",
      "Student pre-training Epoch 774: avg loss = 0.6123\n",
      "Student pre-training Epoch 775: avg loss = 0.6263\n",
      "Student pre-training Epoch 776: avg loss = 0.6252\n",
      "Student pre-training Epoch 777: avg loss = 0.6289\n",
      "Student pre-training Epoch 778: avg loss = 0.6182\n",
      "Student pre-training Epoch 779: avg loss = 0.6315\n",
      "Student pre-training Epoch 780: avg loss = 0.6211\n",
      "Student pre-training Epoch 781: avg loss = 0.6176\n",
      "Student pre-training Epoch 782: avg loss = 0.6142\n",
      "Student pre-training Epoch 783: avg loss = 0.6202\n",
      "Student pre-training Epoch 784: avg loss = 0.6288\n",
      "Student pre-training Epoch 785: avg loss = 0.6118\n",
      "Student pre-training Epoch 786: avg loss = 0.6263\n",
      "Student pre-training Epoch 787: avg loss = 0.6233\n",
      "Student pre-training Epoch 788: avg loss = 0.6151\n",
      "Student pre-training Epoch 789: avg loss = 0.6151\n",
      "Student pre-training Epoch 790: avg loss = 0.6305\n",
      "Student pre-training Epoch 791: avg loss = 0.6343\n",
      "Student pre-training Epoch 792: avg loss = 0.6096\n",
      "Student pre-training Epoch 793: avg loss = 0.6164\n",
      "Student pre-training Epoch 794: avg loss = 0.6132\n",
      "Student pre-training Epoch 795: avg loss = 0.6207\n",
      "Student pre-training Epoch 796: avg loss = 0.6276\n",
      "Student pre-training Epoch 797: avg loss = 0.6155\n",
      "Student pre-training Epoch 798: avg loss = 0.6213\n",
      "Student pre-training Epoch 799: avg loss = 0.6269\n",
      "Student pre-training Epoch 800: avg loss = 0.6256\n",
      "Student pre-training Epoch 801: avg loss = 0.6175\n",
      "Student pre-training Epoch 802: avg loss = 0.6343\n",
      "Student pre-training Epoch 803: avg loss = 0.6181\n",
      "Student pre-training Epoch 804: avg loss = 0.6159\n",
      "Student pre-training Epoch 805: avg loss = 0.6155\n",
      "Student pre-training Epoch 806: avg loss = 0.6153\n",
      "Student pre-training Epoch 807: avg loss = 0.6164\n",
      "Student pre-training Epoch 808: avg loss = 0.6261\n",
      "Student pre-training Epoch 809: avg loss = 0.6219\n",
      "Student pre-training Epoch 810: avg loss = 0.6204\n",
      "Student pre-training Epoch 811: avg loss = 0.6151\n",
      "Student pre-training Epoch 812: avg loss = 0.6153\n",
      "Student pre-training Epoch 813: avg loss = 0.6180\n",
      "Student pre-training Epoch 814: avg loss = 0.6273\n",
      "Student pre-training Epoch 815: avg loss = 0.6133\n",
      "Student pre-training Epoch 816: avg loss = 0.6194\n",
      "Student pre-training Epoch 817: avg loss = 0.6209\n",
      "Student pre-training Epoch 818: avg loss = 0.6104\n",
      "Student pre-training Epoch 819: avg loss = 0.6178\n",
      "Student pre-training Epoch 820: avg loss = 0.6101\n",
      "Student pre-training Epoch 821: avg loss = 0.6007\n",
      "Student pre-training Epoch 822: avg loss = 0.6245\n",
      "Student pre-training Epoch 823: avg loss = 0.6106\n",
      "Student pre-training Epoch 824: avg loss = 0.6064\n",
      "Student pre-training Epoch 825: avg loss = 0.6218\n",
      "Student pre-training Epoch 826: avg loss = 0.6078\n",
      "Student pre-training Epoch 827: avg loss = 0.6232\n",
      "Student pre-training Epoch 828: avg loss = 0.6100\n",
      "Student pre-training Epoch 829: avg loss = 0.5938\n",
      "Student pre-training Epoch 830: avg loss = 0.6077\n",
      "Student pre-training Epoch 831: avg loss = 0.6117\n",
      "Student pre-training Epoch 832: avg loss = 0.5916\n",
      "Student pre-training Epoch 833: avg loss = 0.6091\n",
      "Student pre-training Epoch 834: avg loss = 0.6156\n",
      "Student pre-training Epoch 835: avg loss = 0.6206\n",
      "Student pre-training Epoch 836: avg loss = 0.6070\n",
      "Student pre-training Epoch 837: avg loss = 0.6162\n",
      "Student pre-training Epoch 838: avg loss = 0.6036\n",
      "Student pre-training Epoch 839: avg loss = 0.6041\n",
      "Student pre-training Epoch 840: avg loss = 0.5956\n",
      "Student pre-training Epoch 841: avg loss = 0.5984\n",
      "Student pre-training Epoch 842: avg loss = 0.6004\n",
      "Student pre-training Epoch 843: avg loss = 0.6135\n",
      "Student pre-training Epoch 844: avg loss = 0.6197\n",
      "Student pre-training Epoch 845: avg loss = 0.6103\n",
      "Student pre-training Epoch 846: avg loss = 0.6061\n",
      "Student pre-training Epoch 847: avg loss = 0.5995\n",
      "Student pre-training Epoch 848: avg loss = 0.5991\n",
      "Student pre-training Epoch 849: avg loss = 0.6020\n",
      "Student pre-training Epoch 850: avg loss = 0.6075\n",
      "Student pre-training Epoch 851: avg loss = 0.6200\n",
      "Student pre-training Epoch 852: avg loss = 0.5962\n",
      "Student pre-training Epoch 853: avg loss = 0.5968\n",
      "Student pre-training Epoch 854: avg loss = 0.5967\n",
      "Student pre-training Epoch 855: avg loss = 0.6150\n",
      "Student pre-training Epoch 856: avg loss = 0.5922\n",
      "Student pre-training Epoch 857: avg loss = 0.6034\n",
      "Student pre-training Epoch 858: avg loss = 0.5971\n",
      "Student pre-training Epoch 859: avg loss = 0.5876\n",
      "Student pre-training Epoch 860: avg loss = 0.5933\n",
      "Student pre-training Epoch 861: avg loss = 0.6094\n",
      "Student pre-training Epoch 862: avg loss = 0.5895\n",
      "Student pre-training Epoch 863: avg loss = 0.5931\n",
      "Student pre-training Epoch 864: avg loss = 0.5975\n",
      "Student pre-training Epoch 865: avg loss = 0.5981\n",
      "Student pre-training Epoch 866: avg loss = 0.5928\n",
      "Student pre-training Epoch 867: avg loss = 0.5910\n",
      "Student pre-training Epoch 868: avg loss = 0.5927\n",
      "Student pre-training Epoch 869: avg loss = 0.5961\n",
      "Student pre-training Epoch 870: avg loss = 0.5984\n",
      "Student pre-training Epoch 871: avg loss = 0.5997\n",
      "Student pre-training Epoch 872: avg loss = 0.5923\n",
      "Student pre-training Epoch 873: avg loss = 0.5966\n",
      "Student pre-training Epoch 874: avg loss = 0.5963\n",
      "Student pre-training Epoch 875: avg loss = 0.5809\n",
      "Student pre-training Epoch 876: avg loss = 0.5929\n",
      "Student pre-training Epoch 877: avg loss = 0.5901\n",
      "Student pre-training Epoch 878: avg loss = 0.5888\n",
      "Student pre-training Epoch 879: avg loss = 0.5943\n",
      "Student pre-training Epoch 880: avg loss = 0.5969\n",
      "Student pre-training Epoch 881: avg loss = 0.5965\n",
      "Student pre-training Epoch 882: avg loss = 0.6001\n",
      "Student pre-training Epoch 883: avg loss = 0.5956\n",
      "Student pre-training Epoch 884: avg loss = 0.6000\n",
      "Student pre-training Epoch 885: avg loss = 0.5909\n",
      "Student pre-training Epoch 886: avg loss = 0.6069\n",
      "Student pre-training Epoch 887: avg loss = 0.6022\n",
      "Student pre-training Epoch 888: avg loss = 0.5870\n",
      "Student pre-training Epoch 889: avg loss = 0.5906\n",
      "Student pre-training Epoch 890: avg loss = 0.5923\n",
      "Student pre-training Epoch 891: avg loss = 0.5746\n",
      "Student pre-training Epoch 892: avg loss = 0.5932\n",
      "Student pre-training Epoch 893: avg loss = 0.5937\n",
      "Student pre-training Epoch 894: avg loss = 0.5896\n",
      "Student pre-training Epoch 895: avg loss = 0.5884\n",
      "Student pre-training Epoch 896: avg loss = 0.5896\n",
      "Student pre-training Epoch 897: avg loss = 0.5896\n",
      "Student pre-training Epoch 898: avg loss = 0.5878\n",
      "Student pre-training Epoch 899: avg loss = 0.5892\n",
      "Student pre-training Epoch 900: avg loss = 0.5899\n",
      "Student pre-training Epoch 901: avg loss = 0.5993\n",
      "Student pre-training Epoch 902: avg loss = 0.5811\n",
      "Student pre-training Epoch 903: avg loss = 0.5869\n",
      "Student pre-training Epoch 904: avg loss = 0.5982\n",
      "Student pre-training Epoch 905: avg loss = 0.5836\n",
      "Student pre-training Epoch 906: avg loss = 0.5882\n",
      "Student pre-training Epoch 907: avg loss = 0.5852\n",
      "Student pre-training Epoch 908: avg loss = 0.5843\n",
      "Student pre-training Epoch 909: avg loss = 0.5903\n",
      "Student pre-training Epoch 910: avg loss = 0.5837\n",
      "Student pre-training Epoch 911: avg loss = 0.5855\n",
      "Student pre-training Epoch 912: avg loss = 0.5925\n",
      "Student pre-training Epoch 913: avg loss = 0.5875\n",
      "Student pre-training Epoch 914: avg loss = 0.5674\n",
      "Student pre-training Epoch 915: avg loss = 0.5742\n",
      "Student pre-training Epoch 916: avg loss = 0.5842\n",
      "Student pre-training Epoch 917: avg loss = 0.5840\n",
      "Student pre-training Epoch 918: avg loss = 0.5911\n",
      "Student pre-training Epoch 919: avg loss = 0.5829\n",
      "Student pre-training Epoch 920: avg loss = 0.5849\n",
      "Student pre-training Epoch 921: avg loss = 0.5752\n",
      "Student pre-training Epoch 922: avg loss = 0.5891\n",
      "Student pre-training Epoch 923: avg loss = 0.5740\n",
      "Student pre-training Epoch 924: avg loss = 0.5763\n",
      "Student pre-training Epoch 925: avg loss = 0.5859\n",
      "Student pre-training Epoch 926: avg loss = 0.5720\n",
      "Student pre-training Epoch 927: avg loss = 0.5743\n",
      "Student pre-training Epoch 928: avg loss = 0.5763\n",
      "Student pre-training Epoch 929: avg loss = 0.5744\n",
      "Student pre-training Epoch 930: avg loss = 0.5825\n",
      "Student pre-training Epoch 931: avg loss = 0.5755\n",
      "Student pre-training Epoch 932: avg loss = 0.5772\n",
      "Student pre-training Epoch 933: avg loss = 0.5786\n",
      "Student pre-training Epoch 934: avg loss = 0.5862\n",
      "Student pre-training Epoch 935: avg loss = 0.5975\n",
      "Student pre-training Epoch 936: avg loss = 0.5798\n",
      "Student pre-training Epoch 937: avg loss = 0.5661\n",
      "Student pre-training Epoch 938: avg loss = 0.5827\n",
      "Student pre-training Epoch 939: avg loss = 0.5734\n",
      "Student pre-training Epoch 940: avg loss = 0.5850\n",
      "Student pre-training Epoch 941: avg loss = 0.5811\n",
      "Student pre-training Epoch 942: avg loss = 0.5675\n",
      "Student pre-training Epoch 943: avg loss = 0.5751\n",
      "Student pre-training Epoch 944: avg loss = 0.5717\n",
      "Student pre-training Epoch 945: avg loss = 0.5700\n",
      "Student pre-training Epoch 946: avg loss = 0.5753\n",
      "Student pre-training Epoch 947: avg loss = 0.5678\n",
      "Student pre-training Epoch 948: avg loss = 0.5758\n",
      "Student pre-training Epoch 949: avg loss = 0.5681\n",
      "Student pre-training Epoch 950: avg loss = 0.5749\n",
      "Student pre-training Epoch 951: avg loss = 0.5776\n",
      "Student pre-training Epoch 952: avg loss = 0.5792\n",
      "Student pre-training Epoch 953: avg loss = 0.5658\n",
      "Student pre-training Epoch 954: avg loss = 0.5732\n",
      "Student pre-training Epoch 955: avg loss = 0.5683\n",
      "Student pre-training Epoch 956: avg loss = 0.5706\n",
      "Student pre-training Epoch 957: avg loss = 0.5674\n",
      "Student pre-training Epoch 958: avg loss = 0.5633\n",
      "Student pre-training Epoch 959: avg loss = 0.5675\n",
      "Student pre-training Epoch 960: avg loss = 0.5800\n",
      "Student pre-training Epoch 961: avg loss = 0.5676\n",
      "Student pre-training Epoch 962: avg loss = 0.5660\n",
      "Student pre-training Epoch 963: avg loss = 0.5690\n",
      "Student pre-training Epoch 964: avg loss = 0.5689\n",
      "Student pre-training Epoch 965: avg loss = 0.5774\n",
      "Student pre-training Epoch 966: avg loss = 0.5726\n",
      "Student pre-training Epoch 967: avg loss = 0.5678\n",
      "Student pre-training Epoch 968: avg loss = 0.5710\n",
      "Student pre-training Epoch 969: avg loss = 0.5751\n",
      "Student pre-training Epoch 970: avg loss = 0.5798\n",
      "Student pre-training Epoch 971: avg loss = 0.5701\n",
      "Student pre-training Epoch 972: avg loss = 0.5669\n",
      "Student pre-training Epoch 973: avg loss = 0.5740\n",
      "Student pre-training Epoch 974: avg loss = 0.5689\n",
      "Student pre-training Epoch 975: avg loss = 0.5610\n",
      "Student pre-training Epoch 976: avg loss = 0.5615\n",
      "Student pre-training Epoch 977: avg loss = 0.5667\n",
      "Student pre-training Epoch 978: avg loss = 0.5654\n",
      "Student pre-training Epoch 979: avg loss = 0.5631\n",
      "Student pre-training Epoch 980: avg loss = 0.5590\n",
      "Student pre-training Epoch 981: avg loss = 0.5610\n",
      "Student pre-training Epoch 982: avg loss = 0.5625\n",
      "Student pre-training Epoch 983: avg loss = 0.5633\n",
      "Student pre-training Epoch 984: avg loss = 0.5676\n",
      "Student pre-training Epoch 985: avg loss = 0.5580\n",
      "Student pre-training Epoch 986: avg loss = 0.5509\n",
      "Student pre-training Epoch 987: avg loss = 0.5630\n",
      "Student pre-training Epoch 988: avg loss = 0.5599\n",
      "Student pre-training Epoch 989: avg loss = 0.5710\n",
      "Student pre-training Epoch 990: avg loss = 0.5589\n",
      "Student pre-training Epoch 991: avg loss = 0.5547\n",
      "Student pre-training Epoch 992: avg loss = 0.5616\n",
      "Student pre-training Epoch 993: avg loss = 0.5620\n",
      "Student pre-training Epoch 994: avg loss = 0.5737\n",
      "Student pre-training Epoch 995: avg loss = 0.5567\n",
      "Student pre-training Epoch 996: avg loss = 0.5664\n",
      "Student pre-training Epoch 997: avg loss = 0.5513\n",
      "Student pre-training Epoch 998: avg loss = 0.5664\n",
      "Student pre-training Epoch 999: avg loss = 0.5638\n",
      "Student pre-training Epoch 1000: avg loss = 0.5703\n"
     ]
    }
   ],
   "source": [
    "# Prepare DataLoader for augmented data\n",
    "# Convert to PyTorch tensors\n",
    "X_aug_tensor = torch.from_numpy(augmented_X).float()\n",
    "y_activity_aug_tensor = torch.from_numpy(augmented_y_activity).long()\n",
    "\n",
    "# For transformation labels, convert to one-hot encoding (each sample is one-hot for the applied transform)\n",
    "num_transforms = len(transform_funcs)\n",
    "y_transform_aug = np.zeros((len(augmented_y_transform), num_transforms), dtype=np.float32)\n",
    "y_transform_aug[np.arange(len(augmented_y_transform)), augmented_y_transform] = 1.0\n",
    "y_transform_aug_tensor = torch.from_numpy(y_transform_aug)\n",
    "\n",
    "aug_dataset = torch.utils.data.TensorDataset(X_aug_tensor, y_activity_aug_tensor, y_transform_aug_tensor)\n",
    "aug_loader = torch.utils.data.DataLoader(aug_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "student_model.to(device)\n",
    "\n",
    "# Student model multi-task training (pre-training)\n",
    "optimizer_student = optim.Adam(student_model.parameters(), lr=0.00003, weight_decay=1e-4)\n",
    "criterion_activity = nn.CrossEntropyLoss()\n",
    "# Use BCEWithLogitsLoss for transformation heads\n",
    "criterion_transform = nn.BCEWithLogitsLoss()\n",
    "\n",
    "student_model.train()\n",
    "num_aug_epochs = 1000\n",
    "for epoch in range(1, num_aug_epochs + 1):\n",
    "    total_loss = 0.0\n",
    "    for batch_X, batch_y_act, batch_y_trans in aug_loader:\n",
    "        optimizer_student.zero_grad()\n",
    "        logits_act, logits_trans = student_model(batch_X.to(device))\n",
    "        loss_act = criterion_activity(logits_act, batch_y_act.to(device))\n",
    "        loss_trans = criterion_transform(logits_trans, batch_y_trans.to(device))\n",
    "        loss = loss_act + loss_trans\n",
    "        loss.backward()\n",
    "        optimizer_student.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(aug_loader)\n",
    "    print(f\"Student pre-training Epoch {epoch}: avg loss = {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Fine-Tuning on True Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 1: Loss=0.2753, BalAcc=0.8712, F1_wtd=0.9203\n",
      "Fine-tune Epoch 2: Loss=0.2193, BalAcc=0.8926, F1_wtd=0.9350\n",
      "Fine-tune Epoch 3: Loss=0.2096, BalAcc=0.8859, F1_wtd=0.9307\n",
      "Fine-tune Epoch 4: Loss=0.1913, BalAcc=0.8898, F1_wtd=0.9343\n",
      "Fine-tune Epoch 5: Loss=0.1736, BalAcc=0.9046, F1_wtd=0.9454\n",
      "Fine-tune Epoch 6: Loss=0.1797, BalAcc=0.8883, F1_wtd=0.9357\n",
      "Fine-tune Epoch 7: Loss=0.1687, BalAcc=0.9125, F1_wtd=0.9475\n",
      "Fine-tune Epoch 8: Loss=0.1616, BalAcc=0.9107, F1_wtd=0.9484\n",
      "Fine-tune Epoch 9: Loss=0.1476, BalAcc=0.9105, F1_wtd=0.9491\n",
      "Fine-tune Epoch 10: Loss=0.1413, BalAcc=0.9095, F1_wtd=0.9498\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "\n",
    "# --- Freeze selected layers ---\n",
    "for param in student_model.conv1.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in student_model.conv2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# --- Optimizer for only trainable parameters ---\n",
    "optimizer_ft = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, student_model.parameters()),\n",
    "    lr=0.0001,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# --- Fine-tune loader ---\n",
    "train_dataset_ft = torch.utils.data.TensorDataset(X_train_t, y_train_t)\n",
    "train_loader_ft = torch.utils.data.DataLoader(train_dataset_ft, batch_size=256, shuffle=True)\n",
    "\n",
    "# --- Fine-tuning loop ---\n",
    "student_model = student_model.to(device)\n",
    "criterion_activity = nn.CrossEntropyLoss()\n",
    "student_model.train()\n",
    "\n",
    "num_ft_epochs = 10\n",
    "for epoch in range(1, num_ft_epochs + 1):\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch_X, batch_y in train_loader_ft:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        optimizer_ft.zero_grad()\n",
    "        logits_act, _ = student_model(batch_X)\n",
    "        loss = criterion_activity(logits_act, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer_ft.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_X.size(0)\n",
    "        preds = logits_act.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader_ft.dataset)\n",
    "    train_bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "    train_f1_wtd = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f\"Fine-tune Epoch {epoch}: \"\n",
    "          f\"Loss={avg_loss:.4f}, BalAcc={train_bal_acc:.4f}, F1_wtd={train_f1_wtd:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Standard Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Downstairs       0.56      0.87      0.68        52\n",
      "     Jogging       1.00      0.64      0.78       310\n",
      "     Sitting       0.82      1.00      0.90        59\n",
      "    Standing       1.00      0.73      0.85        41\n",
      "    Upstairs       0.37      0.78      0.50        65\n",
      "     Walking       0.95      0.98      0.97       333\n",
      "\n",
      "    accuracy                           0.82       860\n",
      "   macro avg       0.78      0.83      0.78       860\n",
      "weighted avg       0.89      0.82      0.84       860\n",
      "\n",
      "Balanced Accuracy: 0.8326950278366486\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "student_model.eval()\n",
    "X_test_tensor = torch.from_numpy(test_X).float().to(device)\n",
    "with torch.no_grad():\n",
    "    logits, _ = student_model(X_test_tensor)\n",
    "    preds = logits.argmax(dim=1).to('cpu').numpy()\n",
    "\n",
    "cm = confusion_matrix(test_y_idx, preds)\n",
    "cm = (cm.T / cm.sum(axis=1)).T\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(test_y_idx, preds, target_names=unique_classes))\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy_score(test_y_idx, preds))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, xticklabels=unique_classes, yticklabels=unique_classes, cmap=\"Blues\")\n",
    "plt.title(\"Student Model Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/SelfHAR_Performance.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Linear Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear evaluation accuracy: 0.9337\n"
     ]
    }
   ],
   "source": [
    "# Modify the student model to extract features (from the conv core output)\n",
    "def extract_features(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.from_numpy(X).float().to(device)\n",
    "        # Forward pass until the features are computed:\n",
    "        X_tensor = X_tensor.transpose(1, 2)\n",
    "        x = model.relu(model.conv1(X_tensor))\n",
    "        x = model.dropout(x)\n",
    "        x = model.relu(model.conv2(x))\n",
    "        x = model.dropout(x)\n",
    "        x = model.relu(model.conv3(x))\n",
    "        x = F.max_pool1d(x, kernel_size=x.shape[2])\n",
    "        features = x.squeeze(2)\n",
    "        return features.to('cpu').numpy()\n",
    "\n",
    "train_features = extract_features(student_model, train_labeled_X)\n",
    "test_features = extract_features(student_model, test_X)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(train_features, train_labeled_y_idx)\n",
    "linear_preds = clf.predict(test_features)\n",
    "linear_acc = np.mean(linear_preds == test_y_idx)\n",
    "print(f\"Linear evaluation accuracy: {linear_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smartyEnvmnt_PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
